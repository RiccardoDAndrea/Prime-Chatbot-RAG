{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the chucking process\n",
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "\n",
    "# python PDFProcessing/PDFProccesing.py > outputs/refoutput.txt\n",
    "\n",
    "text_list = []\n",
    "\n",
    "with pdfplumber.open(\"PDF_docs/doc_0.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            text_list.append(text)\n",
    "            text = text.join(text_list)\n",
    "text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = re.split(r\"\\breferences\\b\", text, flags=re.IGNORECASE, maxsplit=1)\n",
    "\n",
    "body = parts[0].strip()  # Everything before \"References\"\n",
    "reference = parts[1].strip() if len(parts) > 1 else \"\"  # Everything after \"References\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\"PDF_docs/doc_2.pdf\")\n",
    "pdf.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(\"PDF_docs/doc_0.pdf\")\n",
    "metadata = pdf.metadata  # Extrahiere Metadaten\n",
    "if metadata and \"Title\" in metadata and metadata[\"Title\"]:\n",
    "    print( f\"Titel: {metadata['Title']}\")\n",
    "else: \n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(\"PDF_docs/doc_1.pdf\") as pdf:\n",
    "    metadata = pdf.metadata\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import ChatResponse, chat\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# # 📌 Schritt 1: PDFs laden\n",
    "# pdf_dir = \"PDF_docs/\"\n",
    "# pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]  # Nur PDFs\n",
    "\n",
    "# all_docs = []\n",
    "# for pdf_file in pdf_files:\n",
    "#     pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "#     loader = PyPDFLoader(pdf_path)\n",
    "#     docs = loader.load()\n",
    "#     all_docs.extend(docs)  # Dokumente speichern\n",
    "\n",
    "# print(f\"✅ {len(all_docs)} Dokumente geladen.\")\n",
    "\n",
    "# # 📌 Schritt 2: Embeddings erstellen\n",
    "# embeddings = OllamaEmbeddings(model=\"llama3.2:latest\")  # Korrekte Embeddings\n",
    "\n",
    "# # 📌 Schritt 3: ChromaDB persistent speichern\n",
    "# persistent_client = chromadb.PersistentClient(path=\"./chroma_langchain_db\")  # Verzeichnis für Speicherung\n",
    "# collection = persistent_client.get_or_create_collection(\"collection_name\")\n",
    "\n",
    "# 📌 Schritt 4: Dokumente zu ChromaDB hinzufügen\n",
    "# doc_texts = [doc.page_content for doc in all_docs]  # Extrahiere den Text\n",
    "# doc_ids = [f\"doc_{i}\" for i in range(len(doc_texts))]  # Einzigartige IDs\n",
    "\n",
    "# collection.add(ids=doc_ids, documents=doc_texts)  # Speichern in ChromaDB\n",
    "\n",
    "# # 📌 Schritt 5: Chroma-VectorStore mit gespeicherten Daten initialisieren\n",
    "# vector_store_from_client = Chroma(\n",
    "#     persist_directory=\"./chroma_langchain_db\",\n",
    "#     collection_name=\"collection_name\",\n",
    "#     embedding_function=embeddings,\n",
    "# )\n",
    "\n",
    "# print(\"✅ ChromaDB erfolgreich mit PDFs initialisiert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"PDF_docs/\"\n",
    "\n",
    "def save_doc(file_path):\n",
    "    # Wir laden die pdf dokumente aus dem direcotire nur die mit .pdf enden\n",
    "    pdf_files = [f for f in os.listdir(file_path) if f.endswith(\".pdf\")]  # Nur PDFs\n",
    "\n",
    "    all_docs = [] # initalisieren eine leere liste\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(file_path, pdf_file)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)  # Dokumente speichern\n",
    "        # Speichern all dokumente in die liste all_docs\n",
    "    return all_docs\n",
    "all_docs = save_doc(file_path=file_path)\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 0, 'page_label': '2207'}, page_content='T h e n e w  e ng l a n d  j o u r na l  o f m e dic i n e\\nn engl j med 366;23 nejm.org june 7, 2012 2207\\n                   anniversary article\\nTwo Hundred Years of Cancer Research\\nVincent T. DeVita, Jr., M.D., and Steven A. Rosenberg, M.D., Ph.D.\\nFrom the Yale Comprehensive Cancer \\nCenter and Smilow Cancer Hospital at \\nYale–New Haven, Yale University School \\nof Medicine, and Yale University School \\nof Public Health — all in New Haven, CT \\n(V.T.D.); the National Cancer Institute, \\nNational Institutes of Health, and the \\nUniformed Services University of the \\nHealth Sciences School of Medicine — \\nall in Bethesda, MD (S.A.R.); and George \\nWashington University School of Medi-\\ncine, Washington, DC (S.A.R.). Address \\nreprint requests to Dr. DeVita at the  \\nYale Comprehensive Cancer Center and \\nSmilow Cancer Hospital at Yale–New  \\nHaven, 333 Cedar St., PO Box 208028, \\nNew Haven, CT 06520-8028, or at vincent \\n.devita@yale.edu.\\nThis article (10.1056/NEJMra1204479) was \\npublished on May 30, 2012, at NEJM.org.\\nN Engl J Med 2012;366:2207-14.\\nCopyright © 2012 Massachusetts Medical Society.\\nI\\nn the 200 years since the New England Journal of Medicine was founded, \\ncancer has gone from a black box to a blueprint. During the first century of the \\nJournal’s publication, medical practitioners could observe tumors, weigh them, \\nand measure them but had few tools to examine the workings within the cancer \\ncell. A few astute observers were ahead of their time, including Rudolf Virchow, who \\nwith the benefit of a microscope deduced the cellular origin of cancer in 1863,1 and \\nStephen Paget, who in 1889 wisely mused about the seed-and-soil hypothesis of \\nmetastatic disease,2 a theory that is coming into its own today (Table 1). Other key \\nadvances were the discovery of a viral cause of avian cancer by Peyton Rous in 19113 \\nand the proposal by Theodor Boveri in 1914 that cancer can be triggered by chro-\\nmosomal mutations.4\\nBut the lid of the black box was not seriously pried open until 1944, when a \\nretired scientist at Rockefeller University, Oswald Avery, reported the results of his \\nbeautifully clear experiments with the pneumococcal bacillus, which showed that \\ncellular information was transmitted not by proteins but by DNA.5 His work led \\ndirectly to the important discovery of the structure of DNA by Watson and Crick in \\n1953.6 Eight years later, the genetic code was broken by Nirenberg and colleagues,7 \\nand the central dogma of biology was established; that information was transmitted \\nfrom DNA to RNA and resulted in the synthesis of proteins. Then, the first of a series \\nof totally unexpected discoveries disrupted this thinking, and we were reminded that \\nthings are not always what they seem in dealing with Mother Nature. The discovery \\nof reverse transcriptase by Temin and Mizutani8 and Baltimore9, which showed that \\ninformation could be transmitted the other way, from RNA to DNA, had a profound \\ninfluence on medicine but most particularly on cancer medicine.\\nEarly investigators discovered that DNA is a very large molecule that was difficult \\nto study in the laboratory. In 1970, Smith and Wilcox solved this problem by identify-\\ning enzymes that bacteria used defensively to cleave DNA at specific restriction \\nsites.10 These discoveries gave birth to the molecular revolution and the biotechnol-\\nogy industry. They also paved the way for the sequencing of the genome.\\nThis kind of science was expensive. The U.S. Congress partially addressed the \\nproblem by passing the National Cancer Act, which expanded the role of the Na-\\ntional Cancer Institute (NCI), the first disease-oriented agency at the National \\nInstitutes of Health (NIH). The act, which was signed into law on December 23, \\n1971, by President Richard Nixon, created a new mandate for an NIH institute: “to \\nsupport research and the application of the results of research to reduce the inci-\\ndence, morbidity and mortality from cancer.” The emphasis on the application of \\nthe results of research was new; it had not been in the mission statement of the \\nNIH. The act would quintuple the budget of the NCI by the end of the decade and \\nprovide the fuel for the revolution in molecular biology.\\nAlthough the enthusiasm in Congress for eradicating cancer was largely derived \\nfrom excitement over a few clinical advances, about 85% of these new funds went to \\nsupport basic research. At its peak in the early 1980s, the NCI accounted for 23% \\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2208'}, page_content='T h e n e w  e ng l a n d  j o u r na l  o f m e dic i n e\\nn engl j med 366;23 nejm.org june 7, 20122208\\nof the budget of the NIH, yet it supported 53% of \\nthe research in molecular biology in the United \\nStates. And the results have been explosive.\\nThe discovery of genes that drive or suppress \\ncellular growth and the complex regulation of \\nsignaling systems used by both normal cells and \\ncancer cells to communicate with each other and \\ntheir environment have brought the blueprint of \\ncancer-cell machinery into bold relief (Table 1). \\nThe association of specific abnormalities with spe-\\ncific cancers has allowed scientists to identify \\npersons who are at increased risk for common \\ncancers, such as breast and colon cancer.\\nMil e s t one s  in  C a ncer \\nT r e atmen t\\nExperiments that can be done in hours in the lab-\\noratory take months and years to replicate in the \\nclinic, so clinical advances, though plentiful, de-\\nvelop slowly. Figures 1 and 2 depict the pace of \\nchange for the past two centuries in four areas: \\ncancer treatment, chemoprevention, viruses and \\ncancer-vaccine development, and tobacco control.\\nIn the treatment of cancer, surgery was the \\nfirst tool available. In 1809, Ephraim McDowell \\nremoved an ovarian tumor without the use of \\nanesthesia, the first abdominal surgery performed \\nin the United States, and provided evidence that \\ntumor masses could be cured by surgery. The \\nfirst public use of anesthesia, as reported by \\nJohn Collins Warren in the Journal in 1846,11 and \\nthe introduction of antisepsis by Joseph Lister in \\n186712 paved the way for a cascade of surgical \\nfirsts in cancer treatment in the 19th and early \\n20th centuries. These innovative surgeons showed \\nthat any organ that was affected by cancer could \\nbe dealt with surgically.13\\nThe most profound influence on cancer sur -\\ngery occurred in 1894, when William Halsted 14 \\nintroduced radical mastectomy for breast cancer. \\nHalsted based his operation on the supposition \\nthat breast cancer spread in a centrifugal fashion \\nfrom the primary tumor to adjacent structures. He \\nrecommended en bloc resection of all surrounding \\ntissue to remove all cancer cells, even the head of \\nthe humerus if it was involved. En bloc resection \\nbecame known as “the cancer operation,” and it \\nwas applied to the removal of all other cancers, \\ndespite scant evidence supporting its use. It would \\nbe 74 years before the use of radical mastectomy \\nand en bloc resection was questioned by another \\nsurgeon, Dr. Bernard Fisher. On the basis of ex-\\nperiments in rodent tumors, Fisher proposed that \\nbreast cancer had early access to the bloodstream \\nand lymphatic tissues. Lymph-node involvement, \\nhe hypothesized, was merely an indication of gen-\\neralized spread of disease. Radical mastectomy \\nwas both too much and too little: too much for \\nsmall tumors and too little for large tumors that \\nhad already metastasized. In a series of clinical \\ntrials conducted by what is now called the Na-\\ntional Surgical Adjuvant Breast and Bowel Project \\nTable 1. Singular Discoveries and Major Events in the Cancer Field and Changing \\nRelative Survival Rates for Patients with Cancer in the United States, 1863–2006.*\\nYear Discovery or Event\\nRelative \\nSurvival Rate\\n1863 Cellular origin of cancer (Virchow)\\n1889 Seed-and-soil hypothesis (Paget)\\n1914 Chromosomal mutations in cancer (Boveri)\\n1937 Founding of NCI\\n1944 Transmission of cellular information by DNA (Avery)\\n1950 Availability of cancer drugs through Cancer \\nChemotherapy National Service Center\\n1953 Report on structure of DNA 35%\\n1961 Breaking of the genetic code\\n1970 Reverse transcriptase\\n1971 Restriction enzymes\\nPassage of National Cancer Act\\n1975 Hybridomas and monoclonal antibodies 50%\\nTracking of cancer statistics by SEER program\\n1976 Cellular origin of retroviral oncogenes\\n1979 Epidermal growth factor and receptor\\n1981 Suppression of tumor growth by p53\\n1984 G proteins and cell signaling\\n1986 Retinoblastoma gene\\n1990 First decrease in cancer incidence and mortality\\n1991 Association between mutation in APC gene  \\nand colorectal cancer\\n1994 Genetic cancer syndromes\\nAssociation between BRCA1 and breast cancer\\n2000 Sequencing of the human genome\\n2002 Epigenetics in cancer\\nMicroRNAs in cancer\\n2005 First decrease in total number of deaths from cancer 68%\\n2006 Tumor stromal interaction\\n* Data are from the National Cancer Institute (NCI) Survival, Epidemiology,  \\nand End Results (SEER) program. APC denotes adenomatous polyposis coli.\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 2, 'page_label': '2209'}, page_content='Two Hundred Years of Cancer Research\\nn engl j med 366;23 nejm.org june 7, 2012 2209\\n(NSABP), which Fisher led, he clearly showed that \\nradical en bloc removal of tissue did nothing \\nmore than could be accomplished by removing the \\ntumor mass itself, if surgery was supplemented by \\nchemotherapy, radiation therapy, or both. Fisher \\nalso showed that less radical surgery plus chemo-\\ntherapy or radiation therapy accomplished the \\ngoal with much less morbidity. These studies15-25\\nrevolutionized the treatment of breast cancer. \\nSince then, most other surgical procedures have \\nbeen tailored to the availability of other treat-\\nments, and cancer surgery has become more \\neffective, with less morbidity. In the first half of \\nthe 20th century, however, surgery was the only \\noption, and a minority of patients could be cured \\nby surgical removal of their tumors alone.\\nChemotherapy/uni0020or\\nsystemic/uni0020therapy\\nRadiation/uni0020therapy\\nSurgery\\nDiscovery of radium,\\n1898\\nTransplantable\\nrodent tumors,\\n1912\\nHead and neck cancer\\ncured by fractionated\\nradiotherapy,\\n1928\\nFolic acid antagonists used in leukemia,\\n1948\\nFisher hypothesis,\\n1968\\nGamma-knife radiosurgery,\\n1968\\nMultileaf collimator,\\n1980\\nIntensity-modulated\\nradiation therapy,\\n1988\\nAdjuvant chemotherapy\\nfor breast cancer,\\n1974\\nCure for testicular cancer,\\n1976\\nTargeting of aromatase enzyme,\\n1977\\nProof of principle:\\ntargeted therapy\\nwith imatinib for CML,\\n1996\\nFirst effective cancer\\nimmunotherapy with interleukin-2,\\n1985\\nLinear accelerator at Stanford,\\n1961\\nMethotrexate used in choriocarcinoma,\\n1957\\nProof of principle:\\ndrug cures for Hodgkin’s disease\\nand childhood leukemia,\\n1967\\nDiscovery of estrogen receptor,\\n1961\\nFirst monoclonal\\nantibody approved,\\n1997\\nDevelopment of\\nkinase inhibitors,\\n2005\\nBreast-conserving\\nsurgery,\\n2002\\nProton beam at Berkeley,\\n1954\\nCobalt teletherapy,\\n1950\\nDiscovery that prostate cancer\\nis hormone-dependent,\\n1945\\nNitrogen mustard used\\nin lymphomas,\\n1943\\nDiscovery of\\nroentgen rays,\\n1895\\nHalsted hypothesis\\nleads to radical\\nmastectomy and\\nen bloc resection,\\n1894\\nOophorectomy for\\nadvanced breast cancer,\\n1906\\nBreast-cancer\\nmortality\\nbegins to fall,\\n1991\\nIncidence\\nper 100,000\\n(age-adjusted)\\nMortality\\nper 100,000\\n(age-adjusted)\\n1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\\n400\\n200\\n511\\n215\\n473\\n178\\n444\\n151\\n5/18/2012\\nAUTHOR/uni0020PLEASE/uni0020NOTE:\\nFigure has been redrawn and type has been reset\\nPlease check carefully\\nAuthor\\nFig #\\nTitle\\nDE\\nME\\nArtist\\nCOLOR/uni0020/uni0020FIGURE\\nDraft 5\\nTwo Hundred Years of\\nCancer Research\\n1\\nPrince\\nWilliams\\nDeVi_ra1204479\\nBaden\\nFigure 1. Timeline of Pivotal Events in Cancer Treatment.\\nCML denotes chronic myeloid leukemia.\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 3, 'page_label': '2210'}, page_content='T h e n e w  e ng l a n d  j o u r na l  o f m e dic i n e\\nn engl j med 366;23 nejm.org june 7, 20122210\\nThe era of radiation treatment began in 1895, \\nwhen Roentgen reported on his discovery of \\nx-rays,26 and accelerated in 1898 with the discov-\\nery of radium by Pierre and Marie Curie. 27 In \\n1928, it was shown that head and neck cancers \\ncould be cured by fractionated radiation treat-\\nments, a milestone in the field.28 The modern era \\nof radiation therapy began in 1950 with the intro-\\nduction of cobalt teletherapy. Since then, aided \\nby advances in computing, the field has been \\ndriven by advances in technology that have al-\\nlowed the therapeutic radiologist to deliver beam \\nenergy precisely to the tumor and to spare the \\nnormal tissue in the path of the radiation beam. \\nIncidence\\nper 100,000\\n(age-adjusted)\\nMortality\\nper 100,000\\n(age-adjusted)\\n1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\\n400\\n200\\n511\\n215\\n473\\n178\\n444\\n151\\nViruses/uni0020and/uni0020cancer\\nChemoprevention\\nTobacco/uni0020and/uni0020cancer\\nHypothesis that tobacco\\nis linked to lung cancer,\\n1912\\nTamoxifen discovered,\\n1967\\nHepatitis B discovered,\\n1967\\nTobacco advertising on radio and\\ntelevision banned in U.S.,\\n1970\\nFirst vaccine against hepatitis B,\\n1974\\nHepatitis linked to hepatoma,\\n1974\\nLink discovered between\\nHPV and cervical cancer,\\n1976\\nVaccine prevents\\nhepatitis and hepatoma,\\n1981\\nHPV vaccine developed,\\n1985\\nTamoxifen\\nprevention trials,\\n1989\\nFinasteride reduces\\nprostate-cancer\\nincidence,\\n2003\\nAspirin prevents\\ncolon cancer,\\n2003\\nProof of principle:\\nchemoprevention\\nworks,\\n1990\\nBCG prevents bladder cancer,\\n1991\\nAntiestrogen drugs\\nprevent DCIS,\\n1995\\nTamoxifen reduces\\nbreast-cancer incidence,\\n1998\\nFDA approves HPV\\nvaccine to prevent\\ncervical cancer,\\n2000\\nLung-cancer incidence\\nand mortality begin to fall,\\n1990–1991\\nExperimental evidence\\nlinks lung cancer to smoking,\\n1950\\nHPV discovered,\\n1907\\nSurgeon General’s report\\non risks of smoking,\\n1964\\nWarning labels on\\ncigarette packages,\\n1965\\n5/21/2012\\nAUTHOR/uni0020PLEASE/uni0020NOTE:\\nFigure has been redrawn and type has been reset\\nPlease check carefully\\nAuthor\\nFig #\\nTitle\\nDE\\nME\\nArtist\\nCOLOR/uni0020/uni0020FIGURE\\nDraft 6\\nTwo Hundred Years of\\nCancer Research\\n2\\nPrince\\nWilliams\\nDeVi_ra1204479\\nBaden\\nFigure 2. Timeline of Pivotal Events in Cancer Prevention.\\nBCG denotes bacille Calmette–Guérin, DCIS ductal carcinoma in situ, FDA Food and Drug Administration, and HPV human papillomavirus.\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 4, 'page_label': '2211'}, page_content='Two Hundred Years of Cancer Research\\nn engl j med 366;23 nejm.org june 7, 2012 2211\\nLike surgery, radiation therapy has become more \\neffective, with less morbidity, and can be used in \\ncombination with other treatments.\\nBy the 1950s, it had become apparent that no \\nmatter how complete the resection or how good \\nthe radiation therapy or how high the dose deliv-\\nered, cure rates after surgery, radiation therapy, or \\nthe two combined had flattened out. Only about a \\nthird of all cancers could be cured by the use of \\nthese two treatment approaches, alone or together.\\nIt was Paul Ehrlich at the turn of the 20th \\ncentury who first made a concerted effort to de-\\nvelop chemicals to cure cancer. He coined the word \\n“chemotherapy.” After animal models of trans-\\nplantable tumors were developed in the early 20th \\ncentury,29 researchers devoted the first half of the \\ncentury to establishing screening systems that \\nwould reliably predict antitumor activity in hu-\\nmans on the basis of data from murine models. \\nHowever, these efforts were largely unsuccessful. \\nPart of the problem was the limited capability for \\ntesting new agents in humans. Two events pro-\\nvided optimism about the future of anticancer \\ndrugs: the use of nitrogen mustard in lymphomas \\nat Yale in 194330 and Farber’s report in 1948 that \\nfolic acid antagonists could induce temporary re-\\nmission in childhood leukemia.31 In 1955, these \\ndiscoveries led to a national screening effort to \\ndevelop and test anticancer drugs. Then the use \\nof cancer chemotherapy, although shrouded in \\ncontroversy, began in earnest. Missing was proof \\nof principle, already established for surgery and \\nradiation therapy, that drugs could cure any can-\\ncer. Major advances came in the mid-1960s with \\nfirm evidence that childhood leukemia32 and ad-\\nvanced Hodgkin’s disease in adults33,34 could be \\ncured by combination chemotherapy.\\nProof of cure by chemotherapy had a permis-\\nsive effect on the use of drugs as an adjuvant to \\nsurgery and radiation therapy. Doctors started to \\nbe willing to consider using chemotherapy. In the \\nmid-1970s, two landmark studies of adjuvant che-\\nmotherapy in breast cancer were published: one \\nfrom the NSABP, which tested a single drug and \\nwas reported by Fisher and colleagues in 1975,15 \\nand one from Italy, which tested a drug combi -\\nnation and was reported by Bonadonna et al. in \\n1976.35 The latter study evaluated a combination \\nregimen (cyclophosphamide, methotrexate, and \\nfluorouracil) developed by the NCI but was per-\\nformed under contract with the Milan Cancer In-\\nstitute, despite large populations of patients with \\noperable breast cancer in the United States, be -\\ncause no major U.S. center was willing to test \\ncombination chemotherapy as an adjuvant. The \\nresults of both studies were positive, and the race \\nwas on. By 1991, thanks to the availability of mul-\\ntiple effective chemotherapeutic agents and hor-\\nmone treatments, improved diagnostic tools for \\nearly diagnosis, and intelligently designed clinical \\ntrials, the rate of death from breast cancer began \\nto fall, a trend that has continued.36 Early diagno-\\nsis and lumpectomy coupled with systemic thera-\\npy have greatly reduced the morbidity associated \\nwith breast-cancer treatment, with good cosmetic \\neffects. Such advances have fulfilled the mandate \\nof the war on cancer “to support research . . . \\nto reduce the incidence, morbidity and mortality \\nfrom cancer.”\\nThe success of adjuvant treatment of breast \\ncancer, in turn, had a permissive effect on the use \\nof drugs in the postoperative treatment of other \\nmajor cancers, such as colorectal cancer. As a \\nconsequence of early diagnosis, prevention, and \\nadjuvant treatment, the rate of death from colorec-\\ntal cancer has fallen by 40% during the past four \\ndecades.36\\nAnother paradigmatic change in cancer treat-\\nment occurred in 2006, when Druker et al. 37 \\nshowed the efficacy of a drug (imatinib) that tar-\\ngeted the unique molecular abnormality in chron-\\nic myeloid leukemia. This work provided proof of \\nprinciple that treatments targeting specific mo-\\nlecular abnormalities that are unique to certain \\ncancers could convert them into manageable \\nchronic illnesses. Since then, chemotherapy has \\nbecome targeted therapy, and the literature has \\nbeen dominated by the search for drugs to in-\\nhibit unique molecular targets, with recent suc-\\ncess in the treatment of some very difficult-to-\\ntreat tumors, such as melanoma38 and lung \\ncancer.39\\nUntil recently, cancer treatment was a three-\\nlegged stool sitting on a base of surgery, radiation \\ntherapy, and chemotherapy. In the past 25 years, \\nimmunotherapy has been added as an important \\ncomponent of cancer treatment.\\nAntibodies were first described in the 1880s \\nand dominated studies of immunology for almost \\n100 years but had little effect on cancer treatment. \\nIn 1975, Köhler and Milstein developed methods \\nfor producing antibodies by fusing cultured my-\\neloma cells with normal B cells from immunized \\nmice.40 The availability of large amounts of an -\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 5, 'page_label': '2212'}, page_content='T h e n e w  e ng l a n d  j o u r na l  o f m e dic i n e\\nn engl j med 366;23 nejm.org june 7, 20122212\\ntibodies with a single specificity led to the suc -\\ncessful development of therapeutic antibodies for \\ncancer, starting with approval by the Food and \\nDrug Administration (FDA) of rituximab for the \\ntreatment of B-cell lymphomas in 1997 41 and fol-\\nlowed by the approval of many other antibodies, \\nmost of which act by inhibiting growth factor \\nreceptors on the surface of cancer cells.\\nIn the early 1960s, it became clear that cellular \\nrather than humoral immunity played a major role \\nin the immune destruction of experimental can-\\ncers, although the inability to manipulate T cells \\noutside the body severely hampered studies of \\ntumor immunity. The description of T-cell growth \\nfactor (subsequently called interleukin-2) in 1976 \\nwas a seminal discovery that stimulated extensive \\nstudies of the cellular immune reaction to experi-\\nmental and human cancers.42 The durable regres-\\nsion of metastatic melanoma and renal cancers in \\nhumans after the administration of interleukin-2, \\ndescribed in 1985, represented the first clear dem-\\nonstration that immune manipulations could cause \\nthe regression of invasive metastatic disease.43 \\nInterleukin-2 was approved for the treatment of \\nmetastatic renal cancer in 1992 and for meta-\\nstatic melanoma in 1998. The subsequent devel-\\nopment of immunomodulatory agents such as \\nipilimu mab,44 the development of cell-transfer \\ntherapies,45,46 and the use of genetically engi -\\nneered lymphocytes to treat cancer 47 have provided \\nadditional evidence of the ability of immuno -\\ntherapy to mediate cancer regression. With the \\nincreasing use of these agents, the cancer-treat-\\nment platform sits firmly on four legs.\\nC a ncer  Pr e v en t ion\\nNo matter how easy cancer treatment may be -\\ncome, it is preferable to prevent cancer. But pre-\\nvention has been an elusive goal. Figure 2 illus-\\ntrates three notable pathways to success, with \\ndiscoveries of the connection between viruses and \\ncancer, methods of chemoprevention, and the role \\nof tobacco in cancer. When the cause of cancer is \\nknown, its prevention becomes a problem in mod-\\nifying human behavior. Nicotine is one of the most \\naddicting substances known, and exposure to to-\\nbacco smoke is by far the best known and most \\nfrequent cause of cancer, causing an estimated \\n40% of all deaths from cancer. It was suggested \\nas early as 1912 that smoking might be related to \\nlung cancer,48 with the epidemiologic evidence \\nbecoming solid in the 1950s. These findings led \\nto the Surgeon General’s report on smoking and \\ncancer that was issued in 1964,49 the use of warn-\\ning labels on cigarette packages in 1965, and a \\nban on tobacco advertising in 1970. These and \\nother aggressive, well-publicized public health \\nmeasures, which were strongly pursued by the \\nAmerican Cancer Society with support from the \\nNCI, have led to a steady reduction in the rate of \\nsmoking, which has decreased to half the 1950 \\nlevel in the United States. It takes time for the \\ndeleterious effects of the thousands of carcino-\\ngenic chemicals in tobacco to dissipate, and it \\nwas not until 1990 that the incidence of lung can-\\ncer in men began to decline, followed by a de -\\ncline in lung-cancer mortality beginning in 1991.\\nTo date, the historic goal of creating a can -\\ncer vaccine has been realized only for cancers \\nthat are caused by viral infections. Even when \\nthe causal virus has been identified, the elapsed \\ntime from discovery to prevention has been \\nlong. The human papillomavirus was discovered \\nin 1907, but it was not linked to cervical cancer \\nuntil 1976,50 and a vaccine to prevent infection \\nby the virus in young girls was not approved by \\nthe FDA until 2000. Hepatitis B virus was dis-\\ncovered in 1967 and was linked to liver cancer \\nin 1974. In 1984, it was shown that both hepa-\\ntitis B and liver cancer could be prevented by \\nvaccination against hepatitis B.51 Since then, in \\nsome parts of the world, vaccination of new -\\nborns against the hepatitis B virus has become \\nroutine. Since it is estimated that 20% of all \\ncancers are caused in some way by viruses, \\nfurther development of vaccines holds much \\npromise.\\nThe use of chemicals to prevent cancer (che-\\nmoprevention) can be effective.52 Antiestrogens \\ncan prevent ductal carcinoma in situ and reduce \\nthe incidence of breast cancer, finasteride can \\nprevent prostate cancer, and plain old aspirin can \\nprevent colorectal cancer. However, this approach \\nis not widely used because large numbers of other-\\nwise normal persons would need to be exposed to \\npotentially toxic materials in order to prevent \\nsome cancers.\\nSurv i va l Now a nd in the Fu t ur e\\nTable 1 shows the changes in relative cancer sur-\\nvival rates related to events in science, and Fig -\\nures 1 and 2 show changes in cancer incidence \\nand mortality, with notations in the charts when \\nthe rates of death from a specific cancer began to \\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 6, 'page_label': '2213'}, page_content='Two Hundred Years of Cancer Research\\nn engl j med 366;23 nejm.org june 7, 2012 2213\\nfall. Soon after the development of successful \\ntreatments in the 1970s, disease-specific death \\nrates began to fall dramatically for childhood leu-\\nkemia and Hodgkin’s disease. The incidence of \\nthese diseases was too low to affect overall rates \\nof death from cancer. Overall rates began to de-\\ncline soon after the introduction of better early \\ndiagnosis and preventive measures and effective \\nadjuvant treatment of common cancers, such as \\ncancer of the breast and colon. The 5-year relative \\nsurvival rate for all cancers, which was 38% in the \\nlate 1960s, just before the passage of the National \\nCancer Act, is now 68%. Straight-line projections \\nindicate that the survival rate will rise to 80% by \\n2015.53,54 Overall rates of death from cancer, which \\nbegan to decline in 1990 in the United States, \\nhave decreased by 24% overall since then.53,54 \\nStraight-line projections to the year 2015 indicate \\nthat the overall absolute reduction in cancer mor-\\ntality will be about 38 percentage points.\\nHowever, these projections are almost certainly \\nunderestimates, since they are based on the as -\\nsumption that there will be little change in the \\nmanagement of cancer between now and 2015. \\nMost of the current declines are the result of the \\nwidespread implementation of old technology for \\ndiagnosis, prevention, and treatment, stimulated \\nby funds provided by the war on cancer. However, \\nthe biggest payoff from that investment — the \\nclinical application of the fruits of the extraordi-\\nnary molecular revolution initiated by the National \\nCancer Act — is yet to come and cannot be mea-\\nsured with the use of current statistics.\\nThe  F u t ur e\\nThe sequencing of the human genome in 2000 has \\nhad a profound effect on all of medicine. The cost \\nof sequencing is reminiscent of Moore’s law, with \\nthe cost halving every 2 years. It is not difficult \\nto foresee a time when a person’s individual ge-\\nnome can be sequenced for as little as $100, put-\\nting genetic studies in the realm of a routine labo-\\nratory test. Starter companies with this aim already \\nexist.\\nSecond- and third-generation deep sequenc -\\ning is revealing the complexity of the cancer \\nblueprint and no doubt will reveal networks not \\nyet imagined. Nonetheless, we are clearly fac-\\ning a future in which patients with cancer or \\nthose at increased risk will have their genome \\nsequenced as a matter of routine, with com-\\nparisons between the premalignant tissue and \\nthe malignant tissue. Detected abnormalities \\nwill become targets of relatively simple drug \\ntherapies, and if the effects mirror what we \\nhave seen in recent years with targeted therapy, \\nthe ability to prevent or treat cancers in the \\nfuture will be impressive. The economic and \\nsocial consequences of converting cancer into a \\ncurable or chronic disease will be both gratify-\\ning and daunting. This overview of 200 years of \\nthe cancer field provides support for the prin-\\nciple of the value of patience and investment in \\nresearch.\\nDisclosure forms provided by the authors are available with \\nthe full text of this article at NEJM.org.\\nReferences\\n1. Virchow R. Cellular pathology as \\nbased upon physiological and pathologi -\\ncal histology. Philadelphia: J.B. Lippin -\\ncott, 1863.\\n2. Paget S. The distribution of secondary \\ngrowths in cancer of the breast. Lancet \\n1889;1:571-3.\\n3. Rous P. A transmissible avian neo -\\nplasm (sarcoma of the common fowl).  \\nJ Exp Med 1910;12:696-705.\\n4. Boveri T. Zur Frage der Entwicklung \\nmaligner Tumoren. Jena, Germany: Gustav \\nFischer-Verlag, 1914.\\n5. Avery OT, Macleod CM, McCarty M. \\nStudies of the chemical nature of the sub-\\nstance inducing transformation of pneu -\\nmococcal types: induction of transforma -\\ntion by a desoxyribonucleic acid fraction \\nisolated from pneumococcus type III.  \\nJ Exp Med 1944;79:137-58.\\n6. Watson JD, Crick FHC. Molecular \\nstructure of nucleic acids: structure for \\ndeoxyribose nucleic acid. Nature 1953; \\n171:737-8.\\n7. Nirenberg MW, Matthaei JH. The de -\\npendence of cell-free protein synthesis in \\nE. coli upon naturally occurring or syn-\\nthetic polyribonucleotides. Proc Natl Acad \\nSci U S A 1961;47:1588-602.8. Temin HM, Mizutani S. RNA-depen -\\ndent DNA polymerase in virions of rous \\nsarcoma virus. Nature 1970;226:1211-3. \\n[Erratum, Nature 1970;227:102.]9. Baltimore D. RNA-dependent DNA \\npolymerase in virions of RNA tumour vi -\\nruses. Nature 1970;226:1209-11.\\n10. Smith HO, Wilcox KW. A restriction \\nenzyme from Hemophilus influenzae. I. \\nPurification and general properties. J Mol \\nBiol 1970;51:379-91.11. Warren JC. Inhalation of ethereal va -\\npor for the prevention of pain in surgical \\noperations. Boston Med Surg J 1846;35: \\n375-9.12. Lister J. On the antiseptic principle in \\nthe practice of surgery. Br Med J 1867;2: \\n246-8.\\n13. Whipple AO, Parsons WB, Mullins \\nCR. Treatment of carcinoma of the am -\\npulla of Vater. Ann Surg 1935;102:763-79.\\n14. Halsted WS. The results of operations \\nfor the cure of cancer of the breast per -\\nformed at the Johns Hopkins Hospital \\nfrom June, 1889, to January, 1894. Ann \\nSurg 1894;20:497-555.15. Fisher B, Carbone P, Economou SG, et \\nal. L-phenylalanine mustard (L-PAM) in \\nthe management of primary breast can-\\ncer. N Engl J Med 1975;292:110-22.\\n16. Fisher B, Redmund C, Brown A. Treat-\\nment of primary breast cancer with che -\\nmotherapy and tamoxifen. N Engl J Med \\n1981;305:1-6.\\n17. Fisher B, Bauer M, Margolese R. Five-\\nyear results of a randomized clinical trial \\ncomparing total mastectomy and segmen-\\ntal mastectomy with or without radiation \\nin the treatment of breast cancer. N Engl \\nJ Med 1985;312:665-73.18. Fisher B, Redmond C, Fisher ER. Ten-\\nyear results of a randomized clinical trial \\ncomparing radical mastectomy and total \\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign CS4 (6.0.6)', 'creationdate': '2012-05-30T17:31:53-04:00', 'moddate': '2025-02-11T09:45:26-08:00', 'trapped': '/False', 'subject': 'N Engl J Med 2012.366:2207-2214', 'title': 'Two Hundred Years of Cancer Research', 'source': 'PDF_docs/doc_0.pdf', 'total_pages': 8, 'page': 7, 'page_label': '2214'}, page_content='n engl j med 366;23 nejm.org june 7, 20122214\\nTwo Hundred Years of Cancer Research\\nmastectomy with or without radiation.  \\nN Engl J Med 1985;312:674-81.\\n19. Fisher B, Redmond C, Dimitrov NV, et \\nal. A randomized clinical trial evaluating \\nsequential methotrexate and fluorouracil \\nin the treatment of patients with node-\\nnegative breast cancer who have estrogen-\\nreceptor-negative tumors. N Engl J Med \\n1989;320:473-8.\\n20. Fisher B, Costantino J, Redmond C, et \\nal. A randomized clinical trial evaluating \\ntamoxifen in the treatment of patients \\nwith node-negative breast cancer who have \\nestrogen-receptor–positive tumors. N Engl \\nJ Med 1989;320:479-84.\\n21. Fisher B, Redmond C, Poisson R, et al. \\nEight-year results of a randomized clini -\\ncal trial comparing total mastectomy and \\nlumpectomy with or without irradiation in \\nthe treatment of breast cancer. N Engl J \\nMed 1989;320:822-8. [Erratum, N Engl J \\nMed 1994;330:1467.]\\n22. Fisher B, Costantino J, Redmond C, et \\nal. Lumpectomy compared with lumpec -\\ntomy and radiation therapy for the treat -\\nment of intraductal breast cancer. N Engl \\nJ Med 1993;328:1581-6.\\n23. Fisher B, Anderson S, Redmond CK, \\nWolmark N, Wickerham L, Cronin WM. \\nReanalysis and results after 12 years of \\nfollow-up in a randomized clinical trial \\ncomparing total mastectomy with lumpec-\\ntomy with or without irradiation in the \\ntreatment of breast cancer. N Engl J Med \\n1995;333:1456-61.\\n24. Fisher B, Anderson S, Bryant J, et al. \\nTwenty-year follow-up of a randomized \\ntrial comparing total mastectomy, lumpec-\\ntomy, and lumpectomy plus irradiation for \\nthe treatment of invasive breast cancer.  \\nN Engl J Med 2002;347:1233-41.\\n25. Fisher B, Anderson SJ. Local therapy \\nand survival in breast cancer. N Engl J \\nMed 2007;357:1051-2.\\n26. Roentgen K. On a new kind of rays. \\nStanton A, trans. Nature 1896;53:274.\\n27. Curie P, Curie M, Bémont G. On a \\nnew, strongly radioactive substance con -\\ntained in pitchblende. CR (East Lansing, \\nMich) 1898;127:1215-7.\\n28. Coutard H. Roentgen therapy of epi -\\ntheliomas of the tonsillar region, hypo -\\npharynx, and larynx from 1920 to 1926. \\nAJR Am J Roentgenol 1932;28:313-31.\\n29. Clowes GHA, Baeslack FW. Further \\nevidence of immunity against cancer in \\nmice after spontaneous recovery: sixth an-\\nnual report. Albany: New York State Can-\\ncer Laboratory, 1904-5.\\n30. Goodman LS, Wintrobe MM, \\nDameshek W, et al. Nitrogen mustard ther-\\napy: use of methyl-bis(beta-chloroethyl) \\namine hydrochloride and tris (beta-chlo -\\nroethyl) amine hydrochloride for Hodg -\\nkin’s disease, lymphosarcoma, leukemia, \\nand certain allied and miscellaneous dis-\\norders. JAMA 1946;105:475-6.\\n31. Farber S, Diamond LK, Mercer RD, \\nSylvester RF Jr, Wolff JA. Temporary remis-\\nsions in acute leukemia in children pro -\\nduced by folic acid antagonist, 4-amino- \\npteroyl-glutamic acid (aminopterin). N Engl \\nJ Med 1948;238:787-93.\\n32. Frei E III, Karon M, Levin RH, et al. \\nThe effectiveness of combinations of anti-\\nleukemic agents in inducing and main -\\ntaining remission in children with acute \\nleukemia. Blood 1965;26:642-56.\\n33. DeVita VT, Moxley JH, Brace K, Frei E \\nIII. Intensive combination chemotherapy \\nand X-irradiation in the treatment of \\nHodgkin’s disease. Proc Am Assoc Cancer \\nRes 1965;6:15.\\n34. DeVita VT Jr, Serpick AA, Carbone PP. \\nCombination chemotherapy in the treat -\\nment of advanced Hodgkin’s disease. Ann \\nIntern Med 1970;73:881-95.\\n35. Bonadonna G, Brusamolino E, Vala -\\ngussa P, et al. Combination chemotherapy \\nas an adjuvant treatment in operable breast \\ncancer. N Engl J Med 1976;294:405-10.\\n36. Brawley O. A strategic approach to the \\ncontrol of cancer. National Press Founda-\\ntion (http://nationalpress.org/images/ \\nuploads/programs/10cancer_brawley.ppt).\\n37. Druker BJ, Guilhot F, O’Brien SG, et \\nal. Five-year follow-up of patients receiv -\\ning imatinib for chronic myeloid leuke -\\nmia. N Engl J Med 2006;355:2408-17.\\n38. Chapman PB, Hauschild A, Robert C, \\net al. Improved survival with vemurafenib \\nin melanoma with BRAF V600E mutation. \\nN Engl J Med 2011;364:2507-16.\\n39. Kwak EL, Bang Y-J, Camidge DR, et al. \\nAnaplastic lymphoma kinase inhibition \\nin non–small-cell lung cancer. N Engl J \\nMed 2010;363:1693-703. [Erratum, N Engl \\nJ Med 2011;364:588.]\\n40. Köhler G, Milstein C. Continuous cul-\\ntures of fused cells secreting antibody of \\npredefined specificity. Nature 1975;256: \\n495-7.\\n41. Maloney DG, Grillo-López AJ, White \\nCA, et al. IDEC-C2B8 (rituximab) anti-CD20 \\nmonoclonal antibody therapy in patients \\nwith relapsed low-grade non-Hodgkin’s \\nlymphoma. Blood 1997;90:2188-95.\\n42. Morgan DA, Ruscetti FW, Gallo RG. \\nSelective in vitro growth of T lymphocytes \\nfrom normal bone marrows. Science \\n1976;193:1007-8.\\n43. Rosenberg SA, Lotze MT, Muul LM, et \\nal. Observations on the systemic adminis-\\ntration of autologous lymphokine-activated \\nkiller cells and recombinant interleukin-2 \\nto patients with metastatic cancer. N Engl \\nJ Med 1985;313:1485-92.\\n44. Hodi FS, O’Day SJ, McDermott DF, et \\nal. Improved survival with ipilimumab in \\npatients with metastatic melanoma. N Engl \\nJ Med 2010;363:711-23. [Erratum, N Engl \\nJ Med 2010;363:1290.]\\n45. Rosenberg SA, Packard BS, Aebersold \\nPM, et al. Use of tumor infiltrating lym -\\nphocytes and interleukin-2 in the immu -\\nnotherapy of patients with metastatic mela-\\nnoma: preliminary report. N Engl J Med \\n1988;319:1676-80.\\n46. Dudley ME, Wunderlich JR, Robbins \\nPF, et al. Cancer regression and autoim -\\nmunity in patients after clonal repopula -\\ntion with antitumor lymphocytes. Science \\n2002;298:850-4.\\n47. Morgan RA, Dudley ME, Wunderlich \\nJR, et al. Cancer regression in patients af-\\nter transfer of genetically engineered lym-\\nphocytes. Science 2006;314:126-9.\\n48. Adler L. Primary malignant growth of \\nthe lungs and bronchi. London: Longmans-\\nGreen, 1912.\\n49. Smoking and health: report of the Ad-\\nvisory Committee to the Surgeon General \\nof the Public Health Service. Washington, \\nDC: Department of Health, Education, and \\nWelfare, Public Health Service, 1964. \\n(Publication no. (PHS) 1103.)\\n50. zur Hausen H. Condylomata acumi -\\nnata and human genital cancer. Cancer Res \\n1976;36:794.\\n51. Wong VC, Ip HM, Reesink HW, et al. \\nPrevention of the HBsAg carrier state in \\nnewborn infants of mothers who are \\nchronic carriers of HBsAg and HBeAg by \\nadministration of hepatitis-B vaccine and \\nhepatitis-B immunoglobulin: double-blind \\nrandomised placebo-controlled study. Lan-\\ncet 1984;1:921-6.\\n52. Hong WK, Lippman SM, Itri LM, Karp \\nDD. Prevention of second primary tumors \\nwith isotretinoin in squamous-cell carci-\\nnoma of the head and neck. N Engl J Med \\n1990;323:795-801.\\n53. Byers TE. Trends in cancer mortality. \\nIn: DeVita VT Jr, Lawrence TS, Rosenberg \\nSA, DePinho RA, Weinberg RA, eds. DeVita, \\nHellman, and Rosenberg’s cancer: princi-\\nples & practices of oncology. 9th ed. Phila-\\ndelphia: Wolters Kluwer Health/Lippincott \\nWilliams & Wilkins, 2011:261-8.\\n54. Byers T, Barrera E, Fontham ET, et al. \\nA midpoint assessment of the American \\nCancer Society challenge goal to halve the \\nU.S. cancer mortality rates between the \\nyears 1990 and 2015. Cancer 2006;107: \\n396-405.\\nCopyright © 2012 Massachusetts Medical Society.\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 0, 'page_label': '1'}, page_content='Journal of Machine Learning Research 26 (2025) 1-31 Submitted 5/24; Published 1/25\\nRandom ReLU Neural Networks as Non-Gaussian Processes\\nRahul Parhi rahul@ucsd.edu\\nDepartment of Electrical and Computer Engineering\\nUniversity of California, San Diego\\nLa Jolla, CA 92093, USA\\nPakshal Bohra pakshalbohra@gmail.com\\nAyoub El Biari ayoubelbiari@gmail.com\\nMehrsa Pourya mehrsa.pourya@epfl.ch\\nMichael Unser michael.unser@epfl.ch\\nBiomedical Imaging Group\\n´Ecole polytechnique f´ ed´ erale de Lausanne\\nCH-1015 Lausanne, Switzerland\\nEditor: Mohammad Emtiyaz Khan\\nAbstract\\nWe consider a large class of shallow neural networks with randomly initialized parameters\\nand rectiﬁed linear unit activation functions. We prove that these random neural networks\\nare well-deﬁned non-Gaussian processes. As a by-product, we demonstrate that these\\nnetworks are solutions to stochastic diﬀerential equations driven by impulsive white noise\\n(combinations of random Dirac measures). These processes are parameterized by the law of\\nthe weights and biases as well as the density of activation thresholds in each bounded region\\nof the input domain. We prove that these processes are isotropic and wide-sense self-similar\\nwith Hurst exponent 3 /2. We also derive a remarkably simple closed-form expression for\\ntheir autocovariance function. Our results are fundamentally diﬀerent from prior work in\\nthat we consider a non-asymptotic viewpoint: The number of neurons in each bounded\\nregion of the input domain (i.e., the width) is itself a random variable with a Poisson law\\nwith mean proportional to the density parameter. Finally, we show that, under suitable\\nhypotheses, as the expected width tends to inﬁnity, these processes can converge in law not\\nonly to Gaussian processes, but also to non-Gaussian processes depending on the law of\\nthe weights. Our asymptotic results provide a new take on several classical results (wide\\nnetworks converge to Gaussian processes) as well as some new ones (wide networks can\\nconverge to non-Gaussian processes).\\nKeywords: Gaussian processes, non-Gaussian processes, random initialization, random\\nneural networks, stochastic processes.\\n1. Introduction\\nA shallow (single-hidden-layer) neural network is a function of the form\\nx↦→\\nN∑\\nk=1\\nvkσ(wT\\nkx−bk), x∈Rd, (1)\\nc⃝2025 Rahul Parhi, Pakshal Bohra, Ayoub El Biari, Mehrsa Pourya, and Michael Unser.\\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided\\nat http://jmlr.org/papers/v26/24-0737.html.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 1, 'page_label': '2'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwhere σ : R →R is the activation function, N is the width of the network, and, for\\nk = 1,...,N , vk ∈R and wk ∈Rd \\\\{0}are the weights and bk ∈R are the biases of the\\nnetwork. It is well-known that, as N →∞, several such networks with i.i.d. random weights\\nand biases are equivalent to a Gaussian process (Neal, 1996). This result was extended to\\ndeep neural networks with i.i.d. random parameters by Lee et al. (2018). This correspondence\\nenables exact Bayesian inference for regression using wide neural networks (Williams, 1996;\\nLee et al., 2018).\\nMotivated by the tight link between wide neural networks and stochastic processes, we\\nstudy properties of shallow rectiﬁed linear unit (ReLU) neural networks with randomly\\ninitialized parameters, henceforth referred to as random (ReLU) neural networks . We study\\nPoisson-type random functions of the form\\nsReLU(x) =\\n∑\\nk∈Z\\nvk\\n[\\nReLU(wT\\nkx−bk) + cT\\nkx+ c0,k\\n]\\n, x∈Rd, (2)\\nwhere ReLU(t) := t+ = max{0,t}, the vk are drawn i.i.d. with respect to the law PV and\\nthe (wk,bk) are drawn such that\\n1. the activation thresholds 1 are mutually independent;\\n2. in expectation, the number of thresholds that intersect a ﬁnite volume in Rd is a\\nconstant (proportional to the product of a parameter λ> 0 and a property related to\\nthe geometry of the volume); and\\n3. for every ﬁnite volume in Rd, the thresholds are i.i.d. uniformly in the volume.\\nThe randomness that generates the ( wk,bk) motivates the denomination Poisson as\\nit mimics the randomness in the jumps found in a unit interval of a compound Poisson\\nprocess (Daley and Vere-Jones, 2007). The parameter λ >0 plays the role of the rate\\nparameter of a compound Poisson process and controls the density of activation thresholds\\nin each ﬁnite volume. The correction terms ( x↦→cT\\nkx+ c0,k)k∈Z that appear in the sum are\\naﬃne functions that ensure that the sum in (2) converges almost surely. This is equivalent to\\nimposing boundary conditions on sReLU. These boundary conditions are crucial in proving\\nthat, under suitable hypotheses on PV, sReLU is a well-deﬁned stochastic process. This is\\none of the primary technical contributions of this paper. Similar correction terms/boundary\\nconditions appear in the deﬁnition of fractional Brownian motion (Mandelbrot and Van Ness,\\n1968) and L´ evy processes (Sato, 1999; Jacob and Schilling, 2001).\\nBy restricting our attention to compact subsets Ω ⊂Rd, say, to the unit ball Bd\\n1 = {x∈\\nRd : ∥x∥2 ≤1}, we have that (see Section 4.1) the process (2) is realized by a random\\nPoisson sum of the form\\nsReLU\\n⏐⏐\\nBd\\n1\\n(x) = wT\\n0 x+ b0 +\\nNλ∑\\nk=1\\nvkReLU(wT\\nkx−bk), (3)\\nwhere the width Nλ is a Poisson random variable with mean λS, where S is proportional to\\nthe surface area of Bd\\n1, and wT\\n0 x+ b0 is an aﬃne function. Thus, the form in the right-hand\\n1. The activation thresholdof the neuron x ↦→ReLU(wTx−b) is the hyperplane Hw,b = {x ∈Rd: wTx = b}.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 2, 'page_label': '3'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nside of (3) is a ﬁnite-width neural network with random parameters (including the width).\\nThe aﬃne function x ↦→wT\\n0 x+ b0 is a skip connection in neural network parlance. As\\nλ→∞, we have that the expected value of the width satisﬁes E[Nλ] →∞. Therefore, this\\nlimiting scenario corresponds to the asymptotic (i.e., inﬁnite-width) regime.\\n1.1 Contributions\\nThe purpose of this paper is to study the properties of random neural networks as in (2)\\nand (3) for the class of admissible laws PV (in the sense of Deﬁnition 5) which, for example,\\nincludes the Gaussian law. As these networks are completely speciﬁed by the law PV and\\nthe rate parameter λ> 0, we let\\nsReLU(·) ∼RP(λ; PV) (4)\\ndenote that sReLU is generated according to the randomness described above, where RP\\nstands for ReLU process. The main contributions of this paper are outlined below.\\nRandom ReLU Networks as Stochastic Processes In Section 4, we prove that\\nsReLU is a well-deﬁned stochastic process. In doing so, we derive the so-called characteristic\\nfunctional2 of the process, which provides us with a complete characterization of its statistical\\ndistribution. Further, we show that sReLU is the unique continuous piecewise linear (CPwL)\\nsolution to the stochastic diﬀerential equation (SDE)\\nTReLU sL= w s.t. ∂ ms(0) = 0,|m|≤ 1, (5)\\nwhere L= denotes equality in law and TReLU = K R ∆ is the whitening operator for ReLU neu-\\nrons. The driving term wof the SDE is an impulsive white noise process which is constructed\\nfrom combinations of random Dirac measures. The boundary conditions ∂ms(0) = 0,\\n|m|≤ 1, are crucial in guaranteeing the existence of solutions to this SDE. In the form\\nof the whitening operator, K is the ﬁltering operator of computed tomography, R is the\\nRadon transform, and ∆ is the Laplacian (see Section 3 for a precise deﬁnition of these\\noperators). The operator TReLU was proposed by Ongie et al. (2020) to study the capacity\\nof bounded-norm inﬁnite-width ReLU networks.\\nProperties of Random ReLU Networks In Section 5, we derive the ﬁrst- and second-\\norder statistics of sReLU. Speciﬁcally, we present a remarkably simple closed-form expression\\nfor its autocovariance function. With the help of these statistics and the characteristic\\nfunctional, we show that sReLU is a non-Gaussian process. We then show that sReLU is\\nisotropic and wide-sense self-similar with Hurst exponent H = 3/2.\\nAsymptotic Results In Section 6, we show that in the inﬁnite-width regime ( λ→∞),\\nsReLU converges in law to a Gaussian process when PV is a Gaussian law with a variance\\nthat is inversely proportional to λ. On the other hand, when PV is a symmetric α-stable\\n(SαS) law with α∈(1,2) and scaling parameter proportional to λ−1/α, sReLU converges in\\nlaw to a non-Gaussian process.\\n2. The characteristic functional of a stochastic process is analogous to the characteristic function of a random\\nvariable. See Section 2 for a detailed discussion.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 3, 'page_label': '4'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\n1.2 Related Work\\nThere is a large body of work that investigates the connections between neural networks with\\nrandom initialization and stochastic processes. Early work in this direction is due to Neal\\n(1996) who proved that wide limits of shallow neural networks with bounded activation\\nfunctions are Gaussian processes when the ( wk,bk) are drawn i.i.d. with respect to any law\\nand the vk are drawn i.i.d. with respect to a law that has zero mean and ﬁnite variance.\\nMore recently, it has been argued by many authors, with varying degrees of mathematical\\nrigor, that deep neural networks with i.i.d. random initialization are Gaussian processes in\\nwide limits (Lee et al., 2018; Matthews et al., 2018; Garriga-Alonso et al., 2019; Novak et al.,\\n2019; Yang, 2019; Dyer and Gur-Ari, 2020; Hanin, 2023).\\nAnother line of work that is closely related to our setting is that of Yaida (2020), who\\nstudies the stochastic processes realized by ﬁnite-width random neural networks and shows\\nthat such processes are non-Gaussian. The results of this paper are complementary to\\nthat of Yaida (2020) in that our ﬁnite-width networks as in (3) also correspond to non-\\nGaussian processes. However, our work is fundamentally diﬀerent as we use the framework\\nof generalized stochastic processes (see Section 2). This allows us to derive the characteristic\\nfunctional of the random neural network, which provides a complete description of its\\nstatistical distribution (i.e., the law of the process). The characteristic functional also allows\\nus to easily study the limiting processes as the expected width E[Nλ] →∞, which Yaida\\n(2020) does not investigate.\\nIn particular, we derive a novel and remarkably simple closed-form expression of the\\nautocovariance function of the ReLU processes. Another important distinction of our\\nasymptotic results compared to prior work on wide networks is that, in the asymptotic\\nregime (λ→∞), the neural networks as in (2) and (3) can converge not only to Gaussian\\nprocesses, but also to non-Gaussian processes, depending on the speciﬁc choice of PV. This\\ntype of result was alluded to by Neal (1996) in the case of S αS initialization, although\\ntheoretical arguments were not carried out. Thus, this paper is the ﬁrst, to the best of our\\nknowledge, to carry out a rigorous investigation of the convergence of wide networks to\\nnon-Gaussian processes.\\n2. Generalized Stochastic Processes\\nThe mathematical framework used in this paper is based on the theory of generalized\\nstochastic processes (Itˆ o, 1954; Gelfand, 1955; Gelfand and Vilenkin, 1964; Itˆ o, 1984) as\\nopposed to the more common “time-series” approach to studying stochastic processes. In\\nthis section, we present the relevant background on generalized stochastic processes. We also\\nrefer the reader to the book of Unser and Tafti (2014) for further background. While this\\ntheory relies on some rather heavy concepts from functional analysis, it allows for elegant\\narguments to investigate the properties of the stochastic processes realized by the random\\nneural networks in (2) and (3).\\nThroughout this paper, we ﬁx a complete probability space (Ω,F,P). Before we introduce\\nthis theory, we ﬁrst recall some results from classical probability theory. A real-valued random\\nvector Xis a measurable function from the probability space (Ω,F,P) to (Rd,B(Rd)), where\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 4, 'page_label': '5'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nB(Rd) denotes the Borel σ-algebra on Rd. The law of X is the pushforward measure\\nPX(A) := (X♯P)(A) := P\\n(\\nX−1(A)\\n)\\n= P({ω∈Ω : X(ω) ∈A}) = P(X∈A), (6)\\nfor all A∈B(Rd). Consequently, the characteristic function of X is the (conjugate) Fourier\\ntransform of PX, given by\\nˆPX(ξ) = E[eiXTξ], ξ∈Rd, (7)\\nwhere i2 = −1.\\nGeneralized stochastic processes are random variables that take values in the (continuous)\\ndual of a nuclear space. In the remainder of this section, let Ndenote a nuclear space and\\nN′denote its dual. If u∈N′and ϕ∈N, we let ⟨u,ϕ⟩N′×N denote the the duality pairing\\nof u and ϕ (i.e., the evaluation of u at ϕ). A prototypical example of a nuclear space is the\\nSchwartz space S(Rd) of smooth and rapidly decreasing test functions. Its dual S′(Rd) is\\nthe space of tempered generalized functions. 3 In order to discuss random variables that take\\nvalues in the dual of a nuclear space, we must equip that space with a σ-algebra.\\nDeﬁnition 1 The cylindrical σ-algebra on N′, denoted by Bc(N′), is the σ-algebra generated\\nby cylinders of the form {u∈N′: (⟨u,ϕ1⟩N′×N,..., ⟨u,ϕN⟩N′×N) ∈A}, where N ∈N\\\\{0},\\nϕ1,...,ϕ N ∈N, and A∈B(RN).\\nWe remark that when N is not only nuclear, but also Fr´ echet, such as S(Rd), the\\ncylindrical σ-algebra Bc(N′) coincides with the Borel σ-algebra B(N′) (see Fernique, 1967;\\nItˆ o, 1984).\\nDeﬁnition 2 A generalized stochastic process is a measurable mapping\\ns: (Ω,F,P) →(N′,Bc(N′)). (8)\\nThe law of s is then the probability measure Ps := s♯P which is deﬁned on Bc(N′). The\\ncharacteristic functional4 of s is the (conjugate) Fourier transform of Ps, given by\\nˆPs(ϕ) = E[ei⟨s,ϕ⟩N′×N], ϕ ∈N. (9)\\nObserve that this deﬁnition recovers the classical characteristic function for random\\nvectors that take values in Rd. Indeed, Rd is a nuclear space whose dual is Rd. Furthermore,\\nfor any ( x,ξ) ∈Rd ×Rd, we have that ⟨x,ξ⟩Rd×Rd = xTξ. The characteristic functional\\nof a generalized stochastic process contains all statistical information of the process in\\nthe same way that the characteristic function of a classical random variable contains all\\nstatistical information of that random variable. Analogous to the ﬁnite-dimensional case,\\nthe Bochner–Minlos theorem (see Minlos (1959)) says that a functional ˆP : N→ C is the\\ncharacteristic functional of a generalized stochastic process if and only if ˆP is continuous,\\npositive deﬁnite, and satisﬁes ˆP(0) = 1.\\nThe attractive feature of the framework of generalized stochastic processes is that it\\ncovers not only classical stochastic processes, but also processes that do not admit a pointwise\\ninterpretation such as white noise processes. For example, a generalized Gaussian process is\\ndeﬁned as follows.\\n3. This space is often referred to as the space of tempered distributions. We adopt the nomenclature of\\ntempered generalized functions in this paper so as to not cause confusion with probability distributions.\\n4. The characteristic functional of a generalized stochastic process was introduced by Kolmogorov (1935).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 5, 'page_label': '6'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nDeﬁnition 3 A generalized stochastic process s that takes values in N′ is said to be\\nGaussian if its characteristic functional is of the form\\nˆPs(ϕ) = exp\\n(\\niµs(ϕ) −1\\n2Σs(ϕ,ϕ)\\n)\\n, (10)\\nwhere ϕ∈N, µs : N→ R is the mean functional of the process, given by\\nµs(ϕ) = E[⟨s,ϕ⟩N′×N], (11)\\nand Σs : N×N→ R is the covariance functional of the process, given by\\nΣs(ϕ1,ϕ2) = E[(⟨s,ϕ1⟩N′×N−µs(ϕ1))(⟨s,ϕ2⟩N′×N−µs(ϕ2))]. (12)\\nThe above deﬁnition is backwards compatible with classical Gaussian processes that are\\nspace-indexed, as shown by Duttweiler and Kailath (1973), yet it also includes Gaussian\\nwhite noise (Hida and Ikeda, 1967).\\nWith this machinery in hand, the primary technical contributions of this paper are (i) to\\nprove that, for any λ> 0 and any admissible PV (in the sense of Deﬁnition 5), the random\\nneural network sReLU ∼RP(λ; PV) is a generalized stochastic process that takes values in\\nS′(Rd), and (ii) to provide an explicit form of its (non-Gaussian) characteristic functional\\n(Section 4). With the help of the latter, we then derive various properties of the stochastic\\nprocess in the non-asymptotic regime (Section 5) and also study its asymptotic ( λ→∞)\\nbehavior (Section 6) for various PV.\\n3. The Radon Transform and Related Operators\\nOur characterization of random ReLU neural networks as stochastic processes hinges on\\nthe whitening operator that appears in the SDE (5). This operator is based on the Radon\\ntransform. In this section we introduce the relevant background on the Radon transform\\nand related operators. We refer the reader to the books of Ramm and Katsevich (1996) and\\nHelgason (2011) for an in depth treatment of the Radon transform. The Radon transform\\nof ϕ∈L1(Rd) is given by\\nR{ϕ}(u,t) =\\n∫\\nuTx=t\\nϕ(x) dx, (u,t) ∈Sd−1 ×R, (13)\\nwhere dx denotes the integration against the ( d−1)-dimensional Lebesgue measure on\\nthe hyperplane {x∈Rd : uTx= t}and Sd−1 = {x∈Rd : ∥x∥2 = 1}denotes the unit\\nsphere in Rd. Observe that the Radon transform of ϕ is a even since (u,t) and (−u,−t)\\nparametrize the same hyperplane. The adjoint operator, or dual Radon transform, applied\\nto φ∈L∞(Sd−1 ×R) is given by\\nR∗{φ}(x) =\\n∫\\nSd−1\\nφ(u,uTx) du, x∈R, (14)\\nwhere dudenotes integration against the surface measure of Sd−1.\\nLet S(Sd−1 ×R) denote the Schwartz space of smooth and rapidly decreasing functions\\non Sd−1 ×R. The range of the Radon transform on S(Rd), deﬁned by SR := R\\n(\\nS(Rd)\\n)\\n, is\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 6, 'page_label': '7'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\na closed subspace of S(Sd−1 ×R) (Helgason, 2011, p. 60). Therefore, since S(Sd−1 ×R) is\\nnuclear, SR is also nuclear. The next proposition summarizes the continuity and invertibility\\nof the Radon transform.\\nProposition 4 (Ludwig 1966; Gelfand et al. 1966; Helgason 2011) The operator R\\ncontinuously maps S(Rd) into S(Sd−1 ×R). Moreover,\\nR∗K R = 1\\n2(2π)d−1 (−∆)\\nd−1\\n2 R∗R = 1\\n2(2π)d−1 R∗R(−∆)\\nd−1\\n2 = Id (15)\\non S(Rd). The underlying operators 5 are the Laplacian ∆ = ∑d\\nn=1 ∂2\\nxn and the ﬁltering\\noperator K = 1\\n2(2π)d−1 (−∂2\\nt)\\nd−1\\n2 . Furthermore, R : S(Rd) →SR is a homeomorphism with\\ninverse R−1 = R∗K : SR →S(Rd).\\n4. Random ReLU Neural Networks as Stochastic Processes\\nIn this section, we will prove that, for any λ >0 and admissible PV, the random neural\\nnetwork s ∼RP(λ; PV) is a well-deﬁned stochastic process and derive its characteristic\\nfunctional on S(Rd). The admissibility conditions in Deﬁnition 5 are rather mild and most\\nchoices of PV (e.g., Gaussian, SαS for 1 <α ≤2, uniform, etc.) satisfy these hypotheses.\\nDeﬁnition 5 We say that the probability measure PV is admissible if\\n1. it is a L´ evy measure, i.e., it satisﬁes PV({0}) = 0 and\\n∫\\nR min{1,v2}dPV(v) < ∞,\\nand\\n2. it has a ﬁrst absolute moment, i.e., if V ∼PV, then E[|V|] <∞.\\nGiven a ReLU neuron x↦→ReLU(wTx−b) with w∈Rd \\\\{0}and b∈R, we observe\\nthat, thanks to the homogeneity of the ReLU,\\nReLU(wTx−b) = ∥w∥2 ReLU( ˜wTx−˜b), (16)\\nwhere ˜w= w/∥w∥2 and ˜b = b/∥w∥2. Therefore, the space of functions representable by\\nshallow ReLU neural networks with input weights constrained to be unit norm is the same\\nas the space of functions representable by shallow ReLU neural networks without constraints\\non the weights (Parhi and Nowak, 2023b; Shenouda et al., 2024). To that end, we focus on\\nneurons of the form ReLU( wTx−b) with (w,b) ∈Sd−1 ×R.\\nAn important property of the operator TReLU = K R ∆ is that it “whitens” ReLU\\nneurons. This result was implicitly proven by Ongie et al. (2020, Example 1), explicitly\\nproven by Parhi and Nowak (2021, Lemma 17), and then further investigated by, e.g.,\\nBartolucci et al. (2023, Lemma 5.6) and Unser (2023, Corollary 11). The whitening property\\nis summarized in the following proposition.\\nProposition 6 For any ReLU neuron\\nr(w,b)(x) = ReLU(wTx−b) (17)\\n5. Non-integer powers of ( −∆) and (−∂2\\nt) are understood in the Fourier domain.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 7, 'page_label': '8'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwith (w,b) ∈Sd−1 ×R, we have that\\nTReLU r(w,b) = δe\\n(w,b), (18)\\nwhere δe\\nz = (δz+ δ−z)/2 denotes the even symmetrization of the Dirac measure δz supported\\nat z∈Sd−1 ×R.\\nThe equality in (18) is understood in Me(Sd−1 ×R), the subspace of even ﬁnite (Radon)\\nmeasures on Sd−1 ×R. The arguments of the proof are based on duality. Indeed, observe\\nthat the adjoint of TReLU takes the form T∗\\nReLU = ∆ R∗K (since ∆ and K are self-adjoint).\\nFurthermore, from Proposition 4 combined with the fact that ∆ : S(Rd) → S(Rd) is\\ncontinuous, we see that T∗\\nReLU : SR →S(Rd) is continuous. Therefore, by duality, TReLU :\\nS′(Rd) →S ′\\nR is continuous. Since r(w,b) ∈S ′(Rd), we have that TReLU r(w,b) is indeed\\nwell-deﬁned. Finally, Me(Sd−1 ×R) is continuously embedded in S′\\nR and so any ﬁnite\\nmeasure in the range of K R can be concretely identiﬁed to have even symmetries (see Unser,\\n2023; Parhi and Unser, 2024, for a detailed discussion). These symmetries are evidenced\\nby the fact that the Radon transform of a “classical” function is necessarily even from the\\nintegral form in (13).\\nProposition 6 motivates us to study Radon-domain impulsive white noises that are\\nrealized by Poisson-type random measures of the form\\nwPoi =\\n∑\\nk∈Z\\nvkδe\\n(wk,bk), (19)\\nwhere vk\\ni.i.d.∼ PV for some admissible PV (in the sense of Deﬁnition 5) and the collection of\\nrandom variables ((wk,bk))k∈Z is a (homogeneous) Poisson point process 6 on Sd−1 ×R with\\nrate parameter λ> 0. This point process satisﬁes the following properties.\\n1. The ( wk,bk) are mutually independent.\\n2. For any measurable subset Π ⊂Sd−1 ×R, if we deﬁne the random variable\\nNΠ = |{(wk,bk) : ( wk,bk) ∈Π}|, (20)\\nthen\\nP(NΠ = n) = (λ|Π|)n\\nn! e−λ|Π|, (21)\\nwhere |Π|denotes the d-dimensional Hausdorﬀ measure of Π. That is to say, NΠ is a\\nPoisson random variable with mean λ|Π|.\\n3. For any measurable subset B ⊂Sd−1 ×R,\\nP((wk,bk) ∈B|(wk,bk) ∈Π) = |B∩Π|\\n|Π| . (22)\\nThat is to say, if a point lies in Π, then its location will be uniformly distributed on Π.\\n6. For a general treatment of point processes, we refer the reader to the book of Daley and Vere-Jones\\n(2007).\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 8, 'page_label': '9'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nNext, if we suppose that there exists a “suitable” right-inverse T†\\nReLU of TReLU that\\nsatisﬁes TReLU T†\\nReLU = Id on S′\\nR, then, intuitively, we could “invert” the result of Propo-\\nsition 6 to ﬁnd that T†\\nReLU{wPoi}is precisely a random ReLU neural network generated\\nin (2). It turns out that such a family of right-inverses exist. These inverses were ﬁrst\\nproposed by Parhi and Nowak (2021, Lemma 21) in order to prove representer theorems\\nfor neural networks. Some further properties of these operator were identiﬁed by Parhi and\\nNowak (2022) and Unser (2023). We summarize the properties from Parhi and Nowak (2021,\\nLemma 21) and Unser (2023, Theorem 13) that are required for our investigation in the\\nnext proposition.\\nProposition 7 For any ε> 0, there exists an operator T†ε\\nReLU deﬁned on S′\\nR such that, for\\nany w∈S′\\nR,\\nTReLU T†ε\\nReLU w= w, (23)[\\n(∂mgε\\nd) ∗T†ε\\nReLU{w}\\n]\\n(0) = 0,|m|≤ 1, (24)\\nwhere gε\\nd : Rd →R is the multivariate Gaussian probability density function with mean 0 and\\ncovariance matrix diag(ε,...,ε ). The restriction of T†ε\\nReLU to the subspace Me(Sd−1 ×R) ⊂\\nS′\\nR continuously maps Me(Sd−1 ×R) to S′(Rd). This mapping is realized by the integral\\noperator\\nT†ε\\nReLU\\n⏐⏐⏐\\nMe(Sd−1×R)\\n{µ}(x) =\\n∫\\nSd−1×R\\nkε\\nx(u,t) dµ(u,t) (25)\\nwhose kernel is given by\\nkε\\nx(u,t) = ReLU(uTx−t) −(uTx−t)\\n2 −\\n(\\ngε\\n1 ∗|·|\\n2\\n)\\n(t) + (uTx)\\n(\\ngε\\n1 ∗sgn\\n2\\n)\\n(t)\\n= ReLU(uTx−t) + uε\\n0\\nTx+ tε\\n0, (26)\\nwhere sgn is the signum function. Furthermore, there exists a universal constant C >0 such\\nthat\\n|kε\\nx(u,t)|≤ C(1 + ∥x∥2) for all (u,t) ∈Sd−1 ×R. (27)\\nRemark 8 The purpose of introducing the ε-indexed right-inverse operators is for a molliﬁ-\\ncation argument. We will eventually consider the limit ε→0 (see the proof of Theorem 9 in\\nAppendix A).\\nWith this inverse operator, we observe that, if wPoi is an impulsive Poisson noise with\\nrate λ> 0 and weights drawn i.i.d. according to PV (as in (19)), then, for any ε> 0,\\nT†ε\\nReLU{wPoi}= T†ε\\nReLU\\n{∑\\nk∈Z\\nvkδe\\n(wk,bk)\\n}\\n=\\n∑\\nk∈Z\\nvkT†ε\\nReLU\\n{\\nδe\\n(wk,bk)\\n}\\n=\\n∑\\nk∈Z\\nvk\\n[\\nReLU(wT\\nk(·) −bk) + cεT\\nk (·) + cε\\n0,k\\n]\\n, (28)\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 9, 'page_label': '10'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwhere the second line is justiﬁed due to the uniform bound in (27), and the third line follows\\nfrom (25). Therefore, T†ε\\nReLU{wPoi}is a random neural network as in (2) that satisﬁes the\\nboundary conditions in (24). We write\\nsε\\nReLU ∼RPε(λ; PV) (29)\\nto denote that sε\\nReLU is such a random neural network. Furthermore, we let\\nsReLU ∼RP(λ; PV), (30)\\nas introduced in Section 1, correspond to a random ReLU neural network that satisﬁes the\\nthe limiting boundary conditions as ε→0. That is to say, ∂msReLU(0) = 0, |m|≤ 1, with\\nthe convention that the value of a piecewise constant function at a jump is the middle value.\\nIn the next theorem, we prove that these random neural networks are well-deﬁned stochas-\\ntic process that take values in S′(Rd) and provide a complete statistical characterization\\nthrough their characteristic functional.\\nTheorem 9 For any ε> 0, λ> 0, and admissible PV (in the sense of Deﬁnition 5), the\\nrandom neural network sε\\nReLU ∼RPε(λ; PV) is a measurable mapping\\nsε\\nReLU : (Ω,F,P) →(S′(Rd),B(S′(Rd)) (31)\\nwith characteristic functional given by\\nˆPsε\\nReLU(ϕ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nR\\n∫\\nSd−1\\n(\\neivT†ε∗\\nReLU{ϕ}(u,t) −1\\n)\\ndudtdPV(v)\\n)\\n, ϕ ∈S(Rd), (32)\\nwhere dudenotes integration against the surface measure on Sd−1 and\\nT†ε∗\\nReLU : ϕ↦→\\n∫\\nRd\\nkε\\nx(·)ϕ(x) dx (33)\\nis the adjoint 7 of T†ε\\nReLU. Furthermore, sε\\nReLU is the unique CPwL solution to the SDE\\nTReLU sL= wPoi s.t. [(∂mgε\\nd) ∗s](0) = 0,|m|≤ 1, (34)\\namong all tempered weak solutions, 8 where wPoi is an impulsive Poisson noise with rate λ\\nand weights drawn i.i.d. according to PV (as in (19)). All other tempered weak solutions to\\nthe SDE take the form sε\\nReLU + h, where h is a harmonic polynomial of degree ≥2.9\\nFinally, in the limiting scenario ( ε → 0), we have that sReLU ∼ RP(λ; PV) is a\\nmeasurable mapping (Ω,F,P) →(S′(Rd),B(S′(Rd)) with characteristic functional given by\\nˆPsReLU(ϕ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nR\\n∫\\nSd−1\\n(\\neivT†∗\\nReLU{ϕ}(u,t) −1\\n)\\ndudtdPV(v)\\n)\\n, ϕ ∈S(Rd), (35)\\n7. Observe that T †ε∗\\nReLU is well-deﬁned on S(Rd) thanks to (27).\\n8. A tempered weak solution to the SDE is any random tempered generalized function s⋆ ∈S′(Rd) that\\nsatisﬁes (34). Such a solution is referred to as “tempered” as it lies in S′(Rd) and “weak” since the action\\nof TReLU on s⋆ is understood by duality.\\n9. A harmonic polynomial h is a polynomial deﬁned on Rd such that ∆h= 0 on all of Rd.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 10, 'page_label': '11'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nwhere T†∗\\nReLU is the limiting operator as ε→0 whose kernel is kx := limε→0 kε\\nx (pointwise\\nlimit). This random neural network is the unique CPwL solution to the SDE\\nTReLU sL= wPoi s.t. ∂ ms(0) = 0,|m|≤ 1. (36)\\nWhile the proof of the theorem is rather technical, the main ingredients can be divided\\ninto two steps. The ﬁrst is to prove that wPoi is a well-deﬁned stochastic process that\\ntakes values in S′\\nR. The second is to invoke the computation in (28) which linearly and\\ncontinuously transforms wPoi into a random ReLU neural network. This transformation\\nallows us to derive the characteristic functional of sReLU in terms of the characteristic\\nfunctional of wPoi. The proof appears in Appendix A.\\n4.1 Restrictions to Compact Domains\\nRecall from (3) that, for any λ> 0 and admissible PV (in the sense of Deﬁnition 5), the\\nrestriction of the random neural network sReLU ∼RP(λ; PV) to a compact domain, say, the\\nunit ball Bd\\n1 is a random Poisson sum of the form\\nsReLU\\n⏐⏐\\nBd\\n1\\n(x) = wT\\n0 x+ b0 +\\nNλ∑\\nk=1\\nvkReLU(wT\\nkx−bk), (37)\\nwhere the width Nλ is a Poisson random variable. The reader can quickly check that the\\nactivation thresholds that intersect Bd\\n1 correspond to Poisson points that lie in Sd−1 ×[−1,1].\\nThus, the number of neurons Nλ is a Poisson random variable with mean λ|Sd−1 ×[−1,1]|,\\nwhich is λ multiplied by twice the surface area of the ( d−1)-sphere. For general compact\\ndomains Ω ⊂Rd, following Parhi and Nowak (2023a, Section IV), we deﬁne\\nZΩ := {(w,b) ∈Sd−1 ×R : {x: wTx= b}∩Ω ̸= ∅}. (38)\\nThen, the restriction sReLU\\n⏐⏐\\nΩ is a random neural network whose width Nλ,Ω is a Poisson\\nrandom variable with mean λ|ZΩ|. As λ→∞, we see that E[Nλ,Ω] →∞. Therefore, the\\nasymptotic setting (λ→∞) corresponds to the inﬁnite-width regime.\\n5. Properties of Random ReLU Neural Networks\\nThe characteristic functional (35) allows us to derive the ﬁrst- and second-order statistics\\nof sReLU as well as infer some of its other properties such as isotropy and wide-sense\\nself-similarity. We summarize these properties in Theorem 10.\\nTheorem 10 For λ >0 and admissible PV (in the sense of Deﬁnition 5), let sReLU ∼\\nRP(λ; PV). Then, the following statements hold.\\n1. The mean of sReLU is given by\\nE[sReLU(x)] = λE[V]\\n∫\\nR\\n∫\\nSd−1\\nkx(u,t) dudt, (39)\\nwhere kx is deﬁned in Theorem 9.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\n2. If PV has a ﬁnite second moment, then the autocovariance of sReLU is given by\\nCsReLU(x,y) = E[(sReLU(x) −E[sReLU(x)])(sReLU(y) −E[sReLU(y)])]\\n= AλE[V2]\\n(\\n∥x−y∥3\\n2 −∥x∥3\\n2 −∥y∥3\\n2 + 3xTy(∥x∥2 + ∥y∥2)\\n)\\n, (40)\\nwhere A= Γ(−3/2)\\n2d+3πd/2Γ((d+3)/2) and Γ(·) is Euler’s gamma function.\\n3. The process sReLU is isotropic, i.e., it has the same probability law as its rotated version\\nsReLU(UT·), where U is any (d×d) rotation matrix.\\n4. If PV has zero mean and a ﬁnite second moment, then sReLU is wide-sense self-similar\\nwith Hurst exponent H = 3/2, i.e., it has the same second-order moments as its scaled\\nand renormalized version aHsReLU(·/a) with a> 0.\\n5. The process sReLU is non-Gaussian.\\nThe proof of Theorem 10 can be found in Appendix B. We mention that the expression\\nof the autocovariance in (40) is remarkably simple. This is in contrast to prior works that\\neither (i) do not provide a closed-form expression (Lee et al., 2018; Yaida, 2020; Hanin,\\n2023), or (ii) provide a closed-form expression, but do not consider the ReLU activation\\nfunction (Williams, 1996). Furthermore, other than the work of Yaida (2020), these prior\\nworks only consider the inﬁnite-width regime.\\n6. Asymptotic Results\\nIn the literature, there has been a lot of work on studying the wide limits of random neural\\nnetworks. Here, we present an asymptotic result for random ReLU neural networks with\\ni.i.d. weights drawn from an S αS law. The proof appears in Appendix C.\\nTheorem 11 For n ∈N, let sn\\nReLU ∼RP (λ = n; PV) with PV being a symmetric α-\\nstable law with scale parameter bn(−1/α),10 where α∈(1,2] and b∈R+, that is, ˆPV(ξ) =\\nexp\\n(\\n−|bξ|α\\nn\\n)\\n. Then, we have\\nsn\\nReLU\\nL−−−→\\nn→∞\\ns∞\\nReLU, (41)\\nwhere s∞\\nReLU is a well-deﬁned generalized stochastic process that takes values in S′(Rd) and\\nhas the characteristic functional\\nˆPs∞\\nReLU(ϕ) = exp\\n(\\n−|b|α∥T†∗\\nReLU{ϕ}∥α\\nLα\\n)\\n, ϕ ∈S(Rd). (42)\\nWhen α= 2, the SαS law is the Gaussian law. In this case, we can deduce that s∞\\nReLU\\nis indeed a Gaussian process (see Appendix C). On the other hand, for α∈(1,2), we can\\nreadily see that s∞\\nReLU is non-Gaussian. Therefore, we have rigorously shown that wide limits\\nof random neural networks are not necessarily Gaussian processes.\\n10. Similar to Neal (1996); Lee et al. (2018), the scale parameter inversely depends on the expected width of\\nthe network.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 12, 'page_label': '13'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\n(a) λ= 1\\n (b) λ= 10\\n (c) λ= 100\\n (d) λ= 1000\\nFigure 1: PV is Gaussian.\\n(a) λ= 1\\n (b) λ= 10\\n (c) λ= 100\\n (d) λ= 1000\\nFigure 2: PV is symmetric (α= 1.25)-stable.\\nWe illustrate these observations numerically in Figures 1 and 2, where we generated\\nrandom neural networks with PV being Gaussian ( α = 2) and non-Gaussian ( α = 1.25),\\nrespectively. There, we plot a top-down view of realizations of random neural networks for\\nλ∈{1,10,100,1000}where we color the linear regions with the magnitude of the gradient\\nof the function. Figure 1(d) looks like a two-dimensional Gaussian process, while Figure 2(d)\\nremains to look CPwL (non-Gaussian). Discussion on how we generated the random neural\\nnetworks numerically along with some additional ﬁgures appear in Appendix D.\\n7. Conclusion\\nWe have investigated the statistical properties of random ReLU neural networks. We proved\\nthat these networks are well-deﬁned non-Gaussian processes in the non-asymptotic regime.\\nWe showed that these processes are isotropic and wide-sense self-similar with Hurst exponent\\n3/2. Remarkably, the autocovariances of these processes have simple closed-form expressions.\\nFinally, we showed that, under suitable hypotheses, as the expected width tends to inﬁnity,\\nthese processes can converge in law not only to Gaussian processes, but also to non-Gaussian\\nprocesses depending on the law of the weights. These asymptotic results recover the classical\\nobservation that wide networks converge to Gaussian processes as well as prove that wide\\nnetworks can converge to non-Gaussian processes. Although the presented investigation\\nonly considered shallow random ReLU neural networks, an important direction of future\\nwork would be to generalize our exact characterizations to deeper networks. To that end,\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 13, 'page_label': '14'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nthe techniques developed by Zavatone-Veth and Pehlevan (2021) could provide a starting\\npoint for that investigation.\\nAcknowledgments\\nThe authors would like to thank the anonymous reviewers and the action editor for their\\ncareful reading of the manuscript. This work was supported in part by the Swiss National\\nScience Foundation under Grant 200020 219356 / 1 and in part by the European Research\\nCouncil (ERC Project FunLearn) under Grant 101020573.\\nAppendix A. Proof of Theorem 9\\nAs preparation before the proof of Theorem 9, we collect and prove some intermediary\\nresults. To begin, we shall ﬁrst prove that wPoi is a well-deﬁned stochastic process taking\\nvalues in S′\\nR. Recall that wPoi is an impulsive white noise that is realized by a Poisson-type\\nrandom measure of the form\\nwPoi =\\n∑\\nk∈Z\\nvkδe\\n(wk,bk), (43)\\nwhere vk\\ni.i.d.∼ PV for some admissible PV (in the sense of Deﬁnition 5) and the collection of\\nrandom variables ((wk,bk))k∈Z is a (homogeneous) Poisson point process on Sd−1 ×R with\\nrate parameter λ> 0. This point process satisﬁes the following properties.\\n1. The ( wk,bk) are mutually independent.\\n2. For any measurable subset Π ⊂Sd−1 ×R, if we deﬁne the random variable\\nNΠ = |{(wk,bk) : ( wk,bk) ∈Π}|, (44)\\nthen\\nP(NΠ = n) = (λ|Π|)n\\nn! e−λ|Π|, (45)\\nwhere |Π|denotes the d-dimensional Hausdorﬀ measure of Π. That is to say, NΠ is a\\nPoisson random variable with mean λ|Π|.\\n3. For any measurable subset B ⊂Sd−1 ×R,\\nP((wk,bk) ∈B|(wk,bk) ∈Π) = |B∩Π|\\n|Π| . (46)\\nThat is to say, if a point lies in Π, then its location will be uniformly distributed on Π.\\nLemma 12 The random measure wPoi can be viewed as a measurable mapping\\nwPoi : (Ω,F,P) →(S′\\nR,B(S′\\nR)) (47)\\nwith characteristic functional given by\\nˆPwPoi(ψ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nR\\n∫\\nSd−1\\n(\\neivψ(u,t) −1\\n)\\ndudtdPV(v)\\n)\\n, (48)\\nwhere dudenotes integration against the surface measure on Sd−1.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 14, 'page_label': '15'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nProof Let D(Rd) ⊂S (Rd) denote the space of inﬁnitely diﬀerentiable and compactly\\nsupported functions on Rd. Let DR := R\\n(\\nD(Rd)\\n)\\ndenote the range of the Radon transform\\non D(Rd). We now summarize the properties of DR that are relevant for our problem (cf.,\\nLudwig, 1966). First, DR is a closed subspace of D(Sd−1 ×R), the nuclear space of inﬁnitely\\ndiﬀerentiable and compactly supported functions on Sd−1 ×R and is therefore nuclear.\\nFurthermore, DR is dense in SR, which implies that S′\\nR is continuously embedded in D′\\nR.\\nIn particular, DR is the subspace of compactly supported functions in SR.\\nNext, we shall prove that wPoi can be viewed as a measurable mapping\\nwPoi : (Ω,F,P) →(D′\\nR,Bc(D′\\nR)) (49)\\nby computing its characteristic functional ˆQwPoi on DR.11 Let ψ∈DR and let\\nNψ = |{(wk,bk) : ( wk,bk) ∈supp ψ}|. (50)\\nWe have, by deﬁnition, that\\n⟨wPoi,ψ⟩D′\\nR ×DR =\\nNψ∑\\nk=1\\nv′\\nkψ(w′\\nk,b′\\nk), (51)\\nwhere we use an appropriate relabeling of {vk,wk,bk : ( wk,bk) ∈supp ψ}. Therefore,\\nˆQwPoi(ψ) = E[e\\ni⟨wPoi,ψ⟩D′\\nR ×DR]\\n= E[ei ∑Nψ\\nk=1 v′\\nkψ(w′\\nk,b′\\nk)]\\n= E\\n\\uf8ee\\n\\uf8f0E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\neiv′\\nkψ(w′\\nk,b′\\nk)\\n⏐⏐⏐⏐⏐⏐\\nNψ\\n\\uf8f9\\n\\uf8fb\\n\\uf8f9\\n\\uf8fb\\n= E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\nE\\n[\\neiv′\\nkψ(w′\\nk,b′\\nk)\\n⏐⏐⏐Nψ\\n]\\n\\uf8f9\\n\\uf8fb (52)\\n= E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\nE\\n[\\neiv′\\nkψ(w′\\nk,b′\\nk)\\n]\\n\\uf8f9\\n\\uf8fb\\n= E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\nE\\n[\\nE\\n[\\neiv′\\nkψ(w′\\nk,b′\\nk)\\n⏐⏐⏐vk\\n]]\\n\\uf8f9\\n\\uf8fb\\n= E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\nE\\n[ 1\\n|supp ψ|\\n∫\\nsupp ψ\\neiv′\\nkψ(w,b) d(w,b)\\n]\\uf8f9\\n\\uf8fb (53)\\n= E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\n1\\n|supp ψ|\\n∫\\nR\\n∫\\nsupp ψ\\neivψ(w,b) d(w,b) dPV(v)\\n\\uf8f9\\n\\uf8fb, (54)\\n11. Note that DR is not Fr´ echet so we use the cylindricalσ-algebra as opposed to the Borel σ-algebra.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 15, 'page_label': '16'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwhere (52) holds by the mutual independence of the (wk,bk) and (53) holds since the random\\nvariables\\n(w′\\nk,b′\\nk) |(w′\\nk,b′\\nk) ∈supp ψ (55)\\nare uniformly distributed on supp ψ. Next, deﬁne the auxiliary functional\\nM(ψ) =\\n∫\\nR\\n∫\\nsupp ψ\\neivψ(w,b) d(w,b) dPV(v). (56)\\nWe have that\\nˆQwPoi(ψ) = E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\nM(ψ)\\n|supp ψ|\\n\\uf8f9\\n\\uf8fb\\n= E\\n[( M(ψ)\\n|supp ψ|\\n)Nψ\\n]\\n=\\n∞∑\\nn=0\\n( M(ψ)\\n|supp ψ|\\n)n(λ|supp ψ|)n\\nn! e−λ|supp ψ| (57)\\n= e−λ|supp ψ|\\n∞∑\\nn=0\\n(λM(ψ))n\\nn!\\n= e−λ|supp ψ|eλM(ψ) (58)\\n= exp(λ(M(ψ) −|supp ψ|) (59)\\n= exp\\n(\\nλ\\n∫\\nR\\n∫\\nSd−1×R\\n(\\neivψ(z) −1\\n)\\ndzdPV(v)\\n)\\n, (60)\\nwhere (57) holds since Nψ is a Poisson random variable with mean λ|supp ψ|, (58) holds by\\nthe Taylor series expansion of t↦→et, (59) holds since |supp ψ|=\\n∫\\nsupp ψ1 dz, and (60) holds\\nsince z↦→eivψ(z) −1 vanishes outside supp ψ. At this point, we remark that, since PV is a\\nL´ evy measure (Deﬁnition 5), it is well-known that the form of (60) is continuous, positive\\ndeﬁnite, and satisﬁes ˆQwPoi(0) = 1 (see, e.g., Gelfand and Vilenkin, 1964, Theorem 2, p. 275).\\nThis implies that wPoi is indeed a generalized stochastic process that takes values in D′\\nR.\\nTo prove the lemma, it remains to extend the domain of ˆQwPoi to SR. To that end, let\\nˆPwPoi(ψ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nSd−1×R\\n(\\neivψ(z) −1\\n)\\ndzdPV(v)\\n)\\n, ψ ∈SR. (61)\\nWe now invoke an adaption of Fageot et al. (2014, Theorem 3) which investigates impulsive\\nwhite noise deﬁned on Rd as a special case. Their theorem implies that, thanks to the\\nadmissibility conditions on PV (Deﬁnition 5), the probability measures QwPoi and PwPoi are\\ncompatible on B(S′\\nR) = Bc(S′\\nR) ⊂Bc(D′\\nR) in the sense that\\nQwPoi(B) = PwPoi(B), for all B ∈B(S′\\nR) (62)\\nand QwPoi(D′\\nR \\\\S′\\nR) = 0, which proves the lemma.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 16, 'page_label': '17'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nLet S∆(Rd) := ∆\\n(\\nS(Rd)\\n)\\ndenote the range of the Laplacian operator on S(Rd). This is a\\nclosed subspace of S(Rd). Observe that its dual S′\\n∆(Rd) can be identiﬁed with the quotient\\nspace S′(Rd)/N∆, where\\nN∆ = {f ∈S′(Rd) : ∆ f = 0 ⇔⟨f,φ⟩S′(Rd)×S(Rd) = 0 for all φ∈S∆(Rd)}. (63)\\nis the null space of the Laplacian operator. It is well-known that N∆ is inﬁnite-dimensional\\nand that its members are necessarily polynomials, the so-called harmonic polynomials .\\nTherefore, the members of S′\\n∆(Rd) are actually equivalence classes of the form\\n[f] = {f + h : h∈N∆}∈S ′\\n∆(Rd), (64)\\nwhere f ∈S′(Rd). With this notation, we now prove Theorem 9.\\nProof [Proof of Theorem 9] Recall that TReLU = K R ∆ and so T∗\\nReLU = ∆ R∗K. Observe\\nthat, by Proposition 4,\\nT∗\\nReLU : SR →S∆(Rd) (65)\\nis a continuous bijection, where we equip the closed subspaces SR ⊂S (Sd−1 ×R) and\\nS∆(Rd) ⊂S(Rd) with the subspace topology from their respective parent Fr´ echet spaces.\\nBy the open mapping theorem for Fr´ echet spaces (see, e.g., Rudin, 1991, Theorem 2.11),\\nthere exists a continuous inverse operator\\nT∗−\\nReLU : S∆(Rd) →SR (66)\\nwith the properties that that T∗\\nReLU T∗−\\nReLU = Id on S∆(Rd) and T∗−\\nReLU T∗\\nReLU = Id on SR.\\nTherefore, by duality, we have the continuous bijections\\nTReLU : S′\\n∆(Rd) →SR\\nT−\\nReLU : S′\\nR →S′\\n∆(Rd), (67)\\nwhere we recall that S′\\n∆(Rd) ∼= S′(Rd)/N∆.\\nNext, we note that the operator\\nT†ε∗\\nReLU : ϕ↦→\\n∫\\nRd\\nkε\\nx(·)ϕ(x) dx (68)\\nspeciﬁed in (33) continuously maps S∆(Rd) → SR (cf., Parhi and Unser, 2025, Equa-\\ntion (A.3)). Observe that, by Proposition 7, its extension by duality T†ε\\nReLU : S′\\nR →S′\\n∆(Rd)\\ncoincides with T−\\nReLU. In particular, T†ε\\nReLU imposes the boundary conditions from (24) on\\nthe aﬃne component of the harmonic polynomials in the equivalence classes in S′\\n∆(Rd). Said\\ndiﬀerently, the range space T†ε\\nReLU\\n(\\nS′\\nR\\n)\\nis the closed subspace of S′\\n∆(Rd) whose equivalence\\nclass members [s] ∈S′\\n∆(Rd) additionally satisfy\\n[(∂mgε\\nd) ∗s0](0) = 0,|m|≤ 1 (69)\\nfor all s0 ∈[s]. Therefore, we can rewrite the SDE (34) as\\nsL= T†ε\\nReLU wPoi, (70)\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 17, 'page_label': '18'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwhere the equality is understood in S′\\n∆(Rd), i.e.,\\n⟨s,φ⟩S′\\n∆(Rd)×S∆(Rd)\\nL= ⟨T†ε\\nReLU wPoi,φ⟩S′\\n∆(Rd)×S∆(Rd) = ⟨wPoi,T†ε∗\\nReLU φ⟩S′\\nR×SR, (71)\\nfor all φ ∈S∆(Rd). The above equality implies that the characteristic functional of any\\nsolution s to (70) (and, subsequently, the original SDE (34)) takes the form\\nˆPs(φ) = ˆPT†ε\\nReLU wPoi\\n(φ) = ˆPwPoi(T†ε∗\\nReLU φ). (72)\\nThis characteristic functional is well-deﬁned for any φ∈S∆(Rd) since T†ε∗\\nReLU φ∈SR, which\\nensures that the right-hand side is well-deﬁned by Lemma 12.\\nSince sε\\nReLU := T†ε\\nReLU wPoi via the computation in (28), we see that sε\\nReLU is one member\\nin an equivalence class in S′(Rd)/N∆. In particular, this implies that sReLU ∈S′(Rd) and\\nthat the equivalence class [sε\\nReLU] = {sε\\nReLU +h: h∈N∆}is a well-deﬁned stochastic process\\nthat takes values in S′\\n∆(Rd) ∼= S′(Rd)/N∆ whose characteristic functional on S∆ is given\\nby (72). Equivalently stated, the full set of tempered weak solutions of the SDE (34) has\\nmembers that necessarily take the form sε\\nReLU + h, where h∈N∆ is a harmonic polynomial\\nof degree ≥2 (since boundary conditions of the SDE, imposed by T†ε\\nReLU, force the aﬃne\\ncomponent of all solutions to be the same). Consequently, from these boundary conditions,\\nwe readily see that the only CPwL solution to the SDE is sε\\nReLU.\\nTo complete the proof we need to derive the form of the characteristic functional of\\nsε\\nReLU on the larger space S(Rd) ⊃S∆(Rd). For any ϕ∈S(Rd), we have that\\n⟨sε\\nReLU,ϕ⟩S′(Rd)×S(Rd)\\nL= ⟨T†ε\\nReLU wPoi,ϕ⟩S′(Rd)×S(Rd) (73)\\nFrom the expression of the kernel ( u,t) ↦→kε\\nx(u,t) in (26) we see that (i) it is continuous\\nin the variables ( u,t) ∈Sd−1 ×R and (ii) it decays faster than any polynomial in the\\nt-variable. Therefore, for every ϕ∈S(Rd), the function T†ε∗\\nReLU{ϕ}is a continuous function\\nin (u,t) ∈Sd−1 ×R that decays faster than any polynomial in the t-variable. In particular,\\nthis ensures that, for any 1 ≤p≤∞, the map\\nT†ε∗\\nReLU : S(Rd) →Lp(Sd−1 ×R) (74)\\nis continuous.\\nThe right-hand side of (73) is, by deﬁnition, the integration of T†ε∗\\nReLU{ϕ}against the\\nlocally ﬁnite Radon measure wPoi, i.e., for any ϕ∈S(Rd) we have that\\n⟨sε\\nReLU,ϕ⟩S′(Rd)×S(Rd)\\nL=\\n∫\\nSd−1×R\\nT†ε∗\\nReLU{ϕ}d\\n(∑\\nk∈Z\\nvkδe\\n(wk,bk)\\n)\\n=\\n∑\\nk∈Z\\nvk\\n∫\\nSd−1×R\\nT†ε∗\\nReLU{ϕ}dδe\\n(wk,bk)\\n=\\n∑\\nk∈Z\\nvkT†ε∗\\nReLU{ϕ}(wk,bk), (75)\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 18, 'page_label': '19'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nwhere interchanging of the integral and sum in the second line is well-deﬁned due to the\\nregularity of T†ε∗\\nReLU{ϕ}and the third line uses the fact that the range of T†ε∗\\nReLU on S(Rd) is\\na space of even functions. This proves that\\nˆPsε\\nReLU(ϕ)\\n= ˆPwPoi(T†ε∗\\nReLU ϕ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nR\\n∫\\nSd−1\\n(\\neivT†ε∗\\nReLU{ϕ}(u,t) −1\\n)\\ndudtdPV(v)\\n)\\n, (76)\\nfor all ϕ ∈ S(Rd), where the last equality comes from Lemma 12. We shall now ver-\\nify that ˆPsε\\nReLU is a valid characteristic functional on S(Rd). This then implies that\\nsε\\nReLU : (Ω ,F,P) →(S′(Rd),B(S′(Rd)) is a measurable mapping and therefore a well-\\ndeﬁned stochastic process.\\nObserve that the second admissibility condition on PV (Item 2 in Deﬁnition 5) states\\nthat PV has a ﬁnite absolute moment. This is a suﬃcient condition to ensure that this\\ncharacteristic functional (76) is well-deﬁned for every ϕ∈S(Rd). Indeed, we have that\\nΨ(ξ) := λ\\n∫\\nR\\n(\\neivξ −1\\n)\\ndPV(v) ≤λ|ξ|E[|V|], (77)\\nwhere V ∼PV (cf., Unser et al., 2014, p. 1952). Therefore,\\nˆPsε\\nReLU(ϕ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nR\\n∫\\nSd−1\\n(\\neivT†ε∗\\nReLU{ϕ}(u,t) −1\\n)\\ndudtdPV(v)\\n)\\n= exp\\n(∫\\nSd−1×R\\nΨ\\n(\\nT†ε∗\\nReLU{ϕ}\\n)\\ndz\\n)\\n≤exp\\n(\\nC∥T†ε∗\\nReLU{ϕ}∥L1\\n)\\n<∞, (78)\\nfor any ϕ ∈S(Rd), where C = λE[|V|] < ∞, where in the last line we used (74) with\\np= 1. Since T†ε∗\\nReLU : S(Rd) →L1(Sd−1 ×R) linearly and continuously, Proposition 3.1 of\\nFageot and Unser (2019) then guarantees that ˆPsε\\nReLU is continuous, positive deﬁnite, and\\nsatisﬁes ˆPsε\\nReLU(0) = 1. Therefore, the Bochner–Minlos theorem ensures that ˆPsε\\nReLU is the\\ncharacteristic functional of the well-deﬁned stochastic process sε\\nReLU.\\nIn the limiting scenario of ε →0, we see that the random neural network sReLU ∼\\nRP(λ; PV) is a measurable mapping (Ω ,F,P) →(S′(Rd),B(S′(Rd)) whose characteristic\\nfunctional is\\nˆPsReLU(ϕ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nR\\n∫\\nSd−1\\n(\\neivT†∗\\nReLU{ϕ}(u,t) −1\\n)\\ndudtdPV(v)\\n)\\n, ϕ ∈S(Rd), (79)\\nwhere we observe that this limiting characteristic functional remains to be valid in the sense\\nof the Bochner–Minlos theorem since the property that, for any 1 ≤p≤∞, the map\\nT†∗\\nReLU : S(Rd) →Lp(Sd−1 ×R) (80)\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 19, 'page_label': '20'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nis continuous, remains to be true since kx = limε→0 kε\\nx (pointwise limt) is compactly sup-\\nported. Consequently, sReLU is the unique CPwL solution to SDE (36). 12\\nAppendix B. Proof of Theorem 10\\nProof\\n1. Thanks to the moment generating properties of the characteristic functional (Gelfand\\nand Vilenkin, 1964), the mean functional of sReLU can be obtained as\\nµsReLU(ϕ) = E[⟨sReLU,ϕ⟩S′(Rd)×S(Rd)] = (−i) d\\ndξ\\nˆPsReLU(ξϕ)\\n⏐⏐⏐\\nξ=0\\n, (81)\\nwhere ϕ ∈S(Rd). First, observe that the characteristic functional of sReLU can be\\nwritten as\\nˆPsReLU(ϕ) = exp\\n(∫\\nSd−1×R\\nΨ\\n(\\nT†∗\\nReLU{ϕ}(z)\\n)\\ndz\\n)\\n(82)\\nwith Ψ deﬁned as in (77). Here, note that we have\\nΨ′(x) = iλ\\n∫\\nR\\nveivxdPV(v). (83)\\nLet us denote h(z) = T†∗\\nReLU{ϕ}(z). By applying the chain rule, we can write\\nd\\ndξ\\nˆPsReLU(ξϕ) = exp\\n(∫\\nR×Sd−1\\nΨ(ξh(z)) dz\\n)\\n·\\n∫\\nR×Sd−1\\nΨ′(ξh(z))h(z) dz. (84)\\nOn setting ξ= 0, we get\\nd\\ndξ\\nˆPsReLU(ξϕ)\\n⏐⏐⏐\\nξ=0\\n= Ψ′(0)\\n∫\\nR×Sd−1\\nh(z) dz (85)\\nas Ψ(0) = 0. Therefore, the mean functional is\\nµsReLU(ϕ) = (−i) d\\ndξ\\nˆPsReLU(ξϕ)\\n⏐⏐⏐\\nξ=0\\n= λE[V]\\n∫\\nSd−1×R\\nT†∗\\nReLU{ϕ}(z) dz\\n= λE[V]\\n∫\\nRd\\n∫\\nR\\n∫\\nSd−1\\nkx(u,t)ϕ(x) dudtdx. (86)\\nNext, we establish a link between the mean functional of sReLU and the quantity\\nE[sReLU(x)]. Since sReLU has a pointwise interpretation, we have\\n⟨sReLU,ϕ⟩S′(Rd)×S(Rd) =\\n∫\\nRd\\nsReLU(x)ϕ(x) dx. (87)\\n12. The molliﬁer argument is necessary in order to make sense of the boundary conditions (36) for elements\\nof S′(Rd) that are not regular enough for the derivatives to exist.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 20, 'page_label': '21'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nConsequently, the mean functional can also be computed as\\nµsReLU(ϕ) = E[⟨sReLU,ϕ⟩S′(Rd)×S(Rd)] = E\\n[∫\\nRd\\nsReLU(x)ϕ(x) dx\\n]\\n=\\n∫\\nRd\\nE[sReLU(x)]ϕ(x) dx, (88)\\nwhere exchanging the expectation and the integral is justiﬁed by the Fubini–Tonelli\\ntheorem since the integrand in (86) is absolutely integrable by (80) with p= 1. On\\ncomparing (88) with (86), we see that\\nE[sReLU(x)] = λE[V]\\n∫\\nR\\n∫\\nSd−1\\nkx(u,t) dudt. (89)\\n2. The covariance functional of sReLU is given by\\nΣsReLU(ϕ1,ϕ2)\\n= E\\n[(\\n⟨sReLU,ϕ1⟩S′(Rd)×S(Rd) −µsReLU(ϕ1)\\n)(\\n⟨sReLU,ϕ2⟩S′(Rd)×S(Rd) −µsReLU(ϕ2)\\n)]\\n= RsReLU(ϕ1,ϕ2) −µsReLU(ϕ1)µsReLU(ϕ2), (90)\\nwhere ϕ1,ϕ2 ∈S(Rd) and\\nRsReLU(ϕ1,ϕ2) = E\\n[\\n⟨sReLU,ϕ1⟩S′(Rd)×S(Rd) ⟨sReLU,ϕ2⟩S′(Rd)×S(Rd)\\n]\\n(91)\\nis the correlation functional of sReLU. This quantity can be computed from its\\ncharacteristic functional (cf., Gelfand and Vilenkin, 1964) as\\nRsReLU(ϕ1,ϕ2) = − d2\\ndξ1dξ2\\nˆPsReLU(ξ1ϕ1 + ξ2ϕ2)\\n⏐⏐⏐\\nξ1=0,ξ2=0\\n. (92)\\nLet us ﬁrst deﬁne the quantity f(ξ1,ξ2) as\\nf(ξ1,ξ2) =\\n∫\\nSd−1×R\\nΨ\\n(\\nT†∗\\nReLU{ξ1ϕ1 + ξ2ϕ2}(z)\\n)\\ndz\\n=\\n∫\\nSd−1×R\\nΨ\\n(\\nξ1 T†∗\\nReLU{ϕ1}(z) + ξ2 T†∗\\nReLU{ϕ2}(z)\\n)\\ndz. (93)\\nFurther, let us denote h1(z) = T†∗\\nReLU{ϕ1}(z) and h2(z) = T†∗\\nReLU{ϕ2}(z). By applying\\nthe chain rule twice, we write\\nd2\\ndξ1dξ2\\nˆPsReLU(ξ1ϕ1 + ξ2ϕ2)\\n= exp(f(ξ1,ξ2))\\n( d2\\ndξ1dξ2\\nf(ξ1,ξ2) + d\\ndξ1\\nf(ξ1,ξ2) d\\ndξ2\\nf(ξ1,ξ2)\\n)\\n,\\n(94)\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 21, 'page_label': '22'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwhere\\nd\\ndξk\\nf(ξ1,ξ2) =\\n∫\\nSd−1×R\\nΨ′(ξ1h1(z) + ξ2h2(z))hk(z) dz (95)\\nand\\nd2\\ndξ1dξ2\\nf(ξ1,ξ2) =\\n∫\\nSd−1×R\\nΨ′′(ξ1h1(z) + ξ2h2(z))h1(z)h2(z) dz. (96)\\nOn setting ξ1 = 0 and ξ2 = 0, we get\\nd2\\ndξ1dξ2\\nˆPsReLU(ξ1ϕ1 + ξ2ϕ2)\\n⏐⏐⏐\\nξ1=0,ξ2=0\\n= Ψ ′′(0)\\n∫\\nSd−1×R\\nh1(z)h2(z) dz\\n+\\n(\\nΨ′(0)\\n∫\\nSd−1×R\\nh1(z)dz\\n)(\\nΨ′(0)\\n∫\\nSd−1×R\\nh2(z) dz\\n)\\n.\\n(97)\\nNote that we have\\nΨ′′(x) = −λ\\n∫\\nR\\nv2eivxdPV(v). (98)\\nThus, the correlation functional is of the form\\nRsReLU(ϕ1,ϕ2) = − d2\\ndξ1dξ2\\nˆPsReLU(ξ1ϕ1 + ξ2ϕ2)\\n⏐⏐⏐\\nξ1=0,ξ2=0\\n= λE[V2]\\n(∫\\nSd−1×R\\nT†∗\\nReLU{ϕ1}(z) T†∗\\nReLU{ϕ2}(z) dz\\n)\\n+ µsReLU(ϕ1)µsReLU(ϕ2). (99)\\nConsequently, the covariance functional is given by\\nΣsReLU(ϕ1,ϕ2)\\n= λE[V2]\\n∫\\nSd−1×R\\nT†∗\\nReLU{ϕ1}(z) T†∗\\nReLU{ϕ2}(z) dz\\n= λE[V2]\\n∫\\nRd\\n∫\\nRd\\n∫\\nR\\n∫\\nSd−1\\nkx(u,t)ky(u,t)ϕ1(x)ϕ2(y) dudtdxdy. (100)\\nNext, we derive the connection between the covariance functional of sReLU and the\\nautocovariance E[(sReLU(x) −E[sReLU(x)])(sReLU(y) −E[sReLU(y)])]. Since sReLU\\nhas a pointwise interpretation, the covariance functional can also be computed as\\nΣsReLU(ϕ1,ϕ2)\\n= E\\n[(\\n⟨sReLU,ϕ1⟩S′(Rd)×S(Rd) −µsReLU(ϕ1)\\n)(\\n⟨sReLU,ϕ2⟩S′(Rd)×S(Rd) −µsReLU(ϕ2)\\n)]\\n= E\\n[(∫\\nRd\\n(sReLU(x) −E[sReLU(x)])ϕ1(x) dx\\n)(∫\\nRd\\n(sReLU(y) −E[sReLU(y)])ϕ2(y) dy\\n)]\\n=\\n∫\\nRd\\n∫\\nRd\\nE[(sReLU(x) −E[sReLU(x)])(sReLU(y) −E[sReLU(y)])]ϕ1(x)ϕ2(y) dxdy,\\n(101)\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 22, 'page_label': '23'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nwhere exchanging the expectation and the integral is justiﬁed by the Fubini–Tonelli\\ntheorem since the integrand in (100) is absolutely integrable from (80) with p= 2. If\\nwe compare (101) with (100), we see that\\nCsReLU(x,y) = E[(sReLU(x) −E[sReLU(x)])(sReLU(y) −E[sReLU(y)])]\\n= λE[V2]\\n∫\\nR\\n∫\\nSd−1\\nkx(u,t)ky(u,t) dudt. (102)\\nTo simplify the double integral in (102), we ﬁrst observe that, by deﬁnition,\\n(x,y) ↦→\\n∫\\nR\\n∫\\nSd−1\\nkx(u,t)ky(u,t) dudt, (x,y) ∈Rd ×Rd, (103)\\nis the (Schwartz) kernel of the operator T†\\nReLU T†∗\\nReLU. Next, we note that the right-\\ninverse operator can be equivalently speciﬁed as the composition of operators T†\\nReLU =\\n(Id −P)∆−1 R∗(cf. Unser, 2023, Equation (57)), where ∆ −1 is the Riesz potential of\\norder 2, i.e., it is the Fourier multiplier\\n(−∆)−γ\\n2 fˆ (ω) = ∥ω∥−γ\\n2 ˆf(ω), ω∈Rd, (104)\\nwith γ = 2, and P is the projection onto the space of aﬃne functions adapted to the\\nboundary conditions of the SDE (36). Concretely,\\nP{f}=\\nd∑\\nn=0\\n⟨φn,f⟩pn, (105)\\nwhere p0(x) = 1 and pn(x) = xn, n= 1,...,d is a basis for the space of aﬃne function\\non Rd and φ0 = δ (Dirac distribution) and φn = −δ′\\nn := −∂xnδ, n= 1,...,d , is the\\nlinear functional that evaluates the partial derivative in the nth component at 0, i.e.,\\n⟨φn,f⟩= ∂xnf(0), n= 1,...,d . Consequently, the adjoint projector is given by\\nP∗{f}=\\nd∑\\nn=0\\n⟨pn,f⟩φn. (106)\\nWith this notation, we have that\\nT†\\nReLU T†∗\\nReLU = (Id −P)∆−1 R∗R ∆−1(Id −P∗)\\n= (Id −P)∆−1(−∆)−d−1\\n2 ∆−1(Id −P∗)\\n= (Id −P)(−∆)−d+3\\n2 (Id −P∗), (107)\\nwhere the second line follows from Proposition 4. The (Schwartz) kernel of the operator\\n(generalized impulse response) can be identiﬁed with ( x,y) ↦→T†\\nReLU T†∗\\nReLU{δ(·−\\ny)}(x). We have that\\n(Id −P∗){δ(·−y)}= δ(·−y) −\\nd∑\\nk=0\\n⟨pk,δ(·−y)⟩φn = δ(·−y) −δ+\\nd∑\\nn=1\\ny1δ′\\nn, (108)\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 23, 'page_label': '24'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwhere we used the property that the shifted Dirac distribution is the sampling functional.\\nNext,\\n(−∆)−d+3\\n2 (Id −P∗){δ(·−y)}(x) = A\\n(\\n∥x−y∥3\\n2 −∥x∥3\\n2 +\\nd∑\\nn=1\\ny1(3xn∥x∥2)\\n)\\n= A\\n(\\n∥x−y∥3\\n2 −∥x∥3\\n2 + 3xTy∥x∥2\\n)\\n, (109)\\nwhere A = Γ(−3/2)\\n2d+3πd/2Γ((d+3)/2) and we used the fact that x ↦→A∥x∥3\\n2 is the radially\\nsymmetric Green’s function of ( −∆)\\nd+3\\n2 (Gelfand and Shilov, 1964). Finally,\\n(Id −P)(−∆)−d+3\\n2 (Id −P∗){δ(·−y)}(x)\\n= A\\n(\\n∥x−y∥3\\n2 −∥x∥3\\n2 + 3xTy∥x∥2\\n)\\n−A\\n(\\n∥y∥3\\n2 −\\nd∑\\nn=1\\n3yn∥y∥2xn\\n)\\n= A\\n(\\n∥x−y∥3\\n2 −∥x∥3\\n2 −∥y∥3\\n2 + 3xTy(∥x∥2 + ∥y∥2)\\n)\\n. (110)\\nPutting everything together, we ﬁnd that the autocovariance takes the form\\nCsReLU(x,y) = λAE[V2]\\n(\\n∥x−y∥3\\n2 −∥x∥3\\n2 −∥y∥3\\n2 + 3xTy(∥x∥2 + ∥y∥2)\\n)\\n. (111)\\n3. In order to show that sReLU is isotropic, we will show that its characteristic functional\\nsatisﬁes\\nˆPsReLU(ϕ) = ˆPsReLU(ϕ(U·)) (112)\\nfor any ϕ∈S(Rd) and any (d×d) rotation matrix U. First, we note that the kernel\\nof T†\\nReLU can be written as\\nkx(u,t) = ReLU(uTx−t) −(uTx−t)\\n2 −|t|\\n2 + (uTx)sgn(t)\\n2\\n= ReLU(uTx−t) + (uTx)h1(t) + h2(t), (113)\\nwhere h1(t) = sgn(t)−1\\n2 and h2(t) = t−|t|\\n2 . Let U be a ( d×d) rotation matrix. Then,\\nwe have\\nT†∗\\nReLU{ϕ(U·)}(u,t) =\\n∫\\nRd\\nkx(u,t)ϕ(Ux) dx\\n=\\n∫\\nRd\\nkUT ˜x(u,t)ϕ(˜x) d˜x (114)\\n=\\n∫\\nRd\\nk˜x(Uu,t)ϕ(˜x) d˜x (115)\\n= T†∗\\nReLU{ϕ}(Uu,t). (116)\\nThe transition from (114) to (115) is possible because\\nkUT ˜x(u,t) = ReLU(uTUT ˜x−t) + (uTUT ˜x)h1(t) + h2(t)\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 24, 'page_label': '25'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\n= ReLU((Uu)T ˜x−t) + ((Uu)T ˜x)h1(t) + h2(t)\\n= k˜x(Uu,t).\\nFrom Theorem 9, the characteristic functional of sReLU is given by\\nˆPsReLU(ϕ) = exp\\n(∫\\nR\\n∫\\nSd−1\\nΨ\\n(\\nT†∗\\nReLU{ϕ}(u,t)\\n)\\ndudt\\n)\\n(117)\\nwith Ψ deﬁned as in (77). Thus, based on (116) and (117), we can write\\nˆPsReLU(ϕ(U·)) = exp\\n(∫\\nR\\n∫\\nSd−1\\nΨ\\n(\\nT†∗\\nReLU{ϕ(U·)}(u,t)\\n)\\ndudt\\n)\\n= exp\\n(∫\\nR\\n∫\\nSd−1\\nΨ\\n(\\nT†∗\\nReLU{ϕ}(Uu,t)\\n)\\ndudt\\n)\\n= exp\\n(∫\\nR\\n∫\\nSd−1\\nΨ\\n(\\nT†∗\\nReLU{ϕ}(˜u,t)\\n)\\nd˜udt\\n)\\n= ˆPsReLU(ϕ). (118)\\n4. In order to show that sReLU (when PV has zero mean and a ﬁnite second moment) is\\nwide-sense self-similar with Hurst exponent H = 3/2, we will show that for a> 0,\\na2HE[sReLU(x/a)sReLU(y/a)] = E[sReLU(x)sReLU(y)]. (119)\\nSince PV has zero mean, based on (39), we have that E[sReLU(x)] = 0. Thus, using\\n(40), we immediately see that\\nE[sReLU(x/a)sReLU(y/a)] = a−3E[sReLU(x)sReLU(y)]. (120)\\n5. From the mean and covariance functionals in (86) and (100), respectively, and the\\nform of the characteristic functional (35), we deduce from Deﬁnition 3 that sReLU is\\nnon-Gaussian, even when PV has a ﬁnite second moment\\nAppendix C. Asymptotic Results\\nTo prove Theorem 11, we rely on a generalized version of the L´ evy continuity theorem from\\nBierm´ e et al. (2018, Theorem 2.3), which we state below.\\nTheorem 13 (Generalized L´ evy continuity theorem)Let (sn)n∈N be a sequence of\\ngeneralized stochastic processes that take values in S′(Rd) with characteristic functionals\\n(ˆPsn)n∈N. If ˆPsn converges pointwise to a functional ˆQ : S(Rd) →C that is continuous at\\n0, then there exists a generalized stochastic process s such that its characteristic functional\\nsatisﬁes ˆPs = ˆQ and sn\\nL−−−→\\nn→∞\\ns.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 25, 'page_label': '26'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nProof [Proof of Theorem 11] By Theorem 13, we need to show that\\n1. for every ϕ∈S(Rd), the sequence\\n(\\nˆPsn\\nReLU(ϕ)\\n)\\nn∈N\\nconverges to\\nˆPs∞\\nReLU(ϕ) := exp\\n(\\n−|b|α∥T†∗\\nReLU{ϕ}∥α\\nLα\\n)\\n(121)\\nand\\n2. the functional ˆPs∞\\nReLU is continuous on S(Rd).\\nWe ﬁrst show that, for every ϕ∈S(Rd),\\nlim\\nn→∞\\nˆPsn\\nReLU(ϕ) = ˆPs∞\\nReLU(ϕ). (122)\\nOur derivation is inspired from the proof of Lemma 2 of Fageot et al. (2020). Since sn\\nReLU\\nis a bona ﬁde generalized stochastic process that takes values in S′(Rd), the functional\\nˆPsn\\nReLU(ϕ) is well-deﬁned for ϕ ∈S(Rd). On the other hand, we observe that ˆPs∞\\nReLU(ϕ)\\nis also well-deﬁned for ϕ ∈S (Rd) due to (80). Next, we prove the convergence. The\\ncharacteristic functional of sn\\nReLU is\\nˆPsn\\nReLU(ϕ) = exp\\n(∫\\nR\\n∫\\nSd−1\\nΨn\\n(\\nT†∗\\nReLU{ϕ}(u,t)\\n)\\ndudt\\n)\\n, (123)\\nwhere\\nΨn(ξ) := n\\n(\\ne−|bξ|α\\nn −1\\n)\\n. (124)\\nFor a ﬁxed z∈Sd−1 ×R, we have that\\nΨn(φ(z)) = n\\n(\\ne−|bφ(z)|α\\nn −1\\n)\\n−−−→\\nn→∞\\n−|bφ(z)|α, (125)\\nwhere φ= T†∗\\nReLU{ϕ}. Thus, we need to show that\\n∫\\nSd−1×R\\nΨn(φ(z)) dz−−−→\\nn→∞\\n∫\\nSd−1×R\\n−|bφ(z)|αdz. (126)\\nFrom p. 1058 in Fageot et al. (2020), we have that\\n|Ψn(φ(z))|≤\\n√\\n2|bφ(z)|α. (127)\\nThe function z↦→|bφ(z)|α is in L1(Sd−1 ×R) due to (80). Thus, we can apply the Lebesgue\\ndominated convergence theorem to show that (126), and consequently (122), holds. Finally,\\nthe continuity of ˆPs∞\\nReLU(ϕ) on S(Rd) follows from the fact that the operator T†∗\\nReLU continu-\\nously maps S(Rd) to Lp(Sd−1 ×R) for p∈[1,2] (cf., Equation (80)).\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 26, 'page_label': '27'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nGaussianity of s∞\\nReLU When α= 2, the characteristic functional of s∞\\nReLU can be written\\nas\\nˆPs∞\\nReLU(ϕ) = exp\\n(∫\\nR\\n∫\\nSd−1\\nΨ∞\\n(\\nT†∗\\nReLU{ϕ}(u,t)\\n)\\ndudt\\n)\\n, (128)\\nwhere ϕ∈S(Rd) and Ψ∞(ξ) = −|bξ|2 for ξ∈R. Using the moment generating properties\\nof the characteristic functional (as in Appendix B), we get that the mean functional is\\nµs∞\\nReLU(ϕ) = 0, ϕ ∈S(Rd), (129)\\nas Ψ′\\n∞(0) = 0, and the covariance functional is\\nΣs∞\\nReLU(ϕ1,ϕ2) = 2|b|2\\n∫\\nSd−1×R\\nT†∗\\nReLU{ϕ1}(z) T†∗\\nReLU{ϕ2}(z) dz, ϕ 1,ϕ2 ∈S(Rd), (130)\\nas Ψ′′\\n∞(0) = −2|b|2. Thus, from (128)–(130) and Deﬁnition 3, we see that s∞\\nReLU is a Gaussian\\nprocess when α= 2.\\nAppendix D. Discussion of the Numerical Examples\\nWe generated realizations of the random neural networks by taking advantage of the property\\nthat Poisson points are uniformly distributed in each ﬁnite volume (cf., Equation (22))\\ncombined with the fact that the width of a random neural network observed on a compact\\ndomain is a Poisson random variable with mean proportional to the rate parameter λ\\nmultiplied by a property related to the geometry of the domain (cf., Section 4.1). In\\nparticular, the random neural network realizations in Figures 1 and 2 were plotted on the\\ncompact domain Ω = [−1,+1]d and were generated according to the following procedure.\\n1. Generate a Poisson random variable Nλ,Ω with mean λ|ZΩ|, where ZΩ was deﬁned in\\n(38).\\n2. Generate Nλ,Ω points i.i.d. uniformly on the ﬁnite volume ZΩ ⊂Sd−1 ×R, which we\\ndenote by {(wk,bk)}Nλ,Ω\\nk=1 .\\n3. Generate Nλ,Ω i.i.d. random variables according to the law PV, which we denote by\\n{vk}Nλ,Ω\\nk=1 .\\n4. Construct the random neural network\\nsnumeric\\nReLU (x) =\\nNλ,Ω∑\\nk=1\\nvk\\n[\\nReLU(wT\\nkx−bk) + cT\\nkx+ c0,k\\n]\\n(131)\\naccording to the computation in (28) with ε→0.\\nThe resulting random neural network snumeric\\nReLU is, up to an aﬃne function, a realization\\nof RP(λ; PV). Finally, in order to highlight the linear regions of the generated networks,\\nwe color the top-down plots in Figures 1 and 2 according to the magnitude of the gradient\\nof snumeric\\nReLU . As the color map choice is arbitrary, the resulting plots are thus realizations\\nof RP(λ; PV) (since the magnitude of the gradient of an aﬃne function is a constant, and\\ntherefore simply shifts the color map). We include some additional plots of the random\\nneural networks in Figures 3 and 4. These ﬁgures are surface plots of the random neural\\nnetworks in Figures 1 and 2, respectively.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 27, 'page_label': '28'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\n(a) λ= 1\\n (b) λ= 10\\n (c) λ= 100\\n (d) λ= 1000\\nFigure 3: PV is Gaussian.\\n(a) λ= 1\\n (b) λ= 10\\n (c) λ= 100\\n (d) λ= 1000\\nFigure 4: PV is symmetric (α= 1.25)-stable.\\nReferences\\nFrancesca Bartolucci, Ernesto De Vito, Lorenzo Rosasco, and Stefano Vigogna. Understand-\\ning neural networks with reproducing kernel Banach spaces. Applied and Computational\\nHarmonic Analysis, 62:194–236, 2023.\\nHermine Bierm´ e, Olivier Durieu, and Yizao Wang. Generalized random ﬁelds and L´ evy’s\\ncontinuity theorem on the space of tempered distributions. Communications on Stochastic\\nAnalysis, 12(4):4, 2018.\\nDaryl J. Daley and David Vere-Jones. An Introduction to the Theory of Point Processes:\\nVolume II: General Theory and Structure . Probability and Its Applications. Springer New\\nYork, 2007.\\nDonald L. Duttweiler and Thomas Kailath. RKHS approach to detection and estimation\\nproblems–IV: Non-Gaussian detection. IEEE Transactions on Information Theory , 19(1):\\n19–28, 1973.\\nEthan Dyer and Guy Gur-Ari. Asymptotics of wide networks from Feynman diagrams. In\\nInternational Conference on Learning Representations, 2020.\\nJulien Fageot and Michael Unser. Scaling limits of solutions of linear stochastic diﬀerential\\nequations driven by L´ evy white noises.Journal of Theoretical Probability, 32(3):1166–1189,\\n2019.\\nJulien Fageot, Arash Amini, and Michael Unser. On the continuity of characteristic\\nfunctionals and sparse stochastic modeling. Journal of Fourier Analysis and Applications ,\\n20:1179–1211, 2014.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 28, 'page_label': '29'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nJulien Fageot, Virginie Uhlmann, and Michael Unser. Gaussian and sparse processes are\\nlimits of generalized Poisson processes. Applied and Computational Harmonic Analysis ,\\n48(3):1045–1065, 2020.\\nXavier Fernique. Processus lin´ eaires, processus g´ en´ eralis´ es.Annales de l’institut Fourier , 17\\n(1):1–92, 1967.\\nAdri` a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolu-\\ntional networks as shallow Gaussian processes. In International Conference on Learning\\nRepresentations, 2019.\\nIzrail M. Gelfand. Generalized random processes. Dokl. Akad. Nauk SSSR (N.S.) , 100:\\n853–856, 1955.\\nIzrail M. Gelfand and Georgiy E. Shilov. Generalized functions. Vol. I: Properties and\\noperations. Academic Press, 1964.\\nIzrail M. Gelfand and Naum Ya. Vilenkin. Generalized functions, Vol. 4: Applications of\\nharmonic analysis. Academic Press, 1964.\\nIzrail M. Gelfand, Mark I. Graev, and Naum Ya. Vilenkin. Generalized functions. Vol. 5:\\nIntegral geometry and representation theory. Academic Press, 1966.\\nBoris Hanin. Random neural networks in the inﬁnite width limit as Gaussian processes. The\\nAnnals of Applied Probability, 33(6A):4798–4819, 2023.\\nSigurdur Helgason. Integral Geometry and Radon Transforms. Springer New York, 2011.\\nTakeyuki Hida and Nobuyuki Ikeda. Analysis on Hilbert space with reproducing kernel\\narising from multiple Wiener integral. In Proc. Fifth Berkeley Sympos. Math. Statist. and\\nProbability, pages 117–143. Univ. California Press, Berkeley, CA, 1967.\\nKiyosi Itˆ o. Stationary random distributions.Memoirs of the College of Science. University\\nof Kyoto. Series A. Mathematics , 28:209–223, 1954.\\nKiyosi Itˆ o.Foundations of stochastic diﬀerential equations in inﬁnite dimensional spaces ,\\nvolume 47. SIAM, 1984.\\nNiels Jacob and Ren´ e L. Schilling.L´ evy-Type Processes and Pseudodiﬀerential Operators,\\npages 139–168. Birkh¨ auser Boston, Boston, MA, 2001. ISBN 978-1-4612-0197-7.\\nAndrei N. Kolmogorov. La transformation de Laplace dans les espaces lin´ eaires. CR Acad.\\nSci. Paris, 200:1717–1718, 1935.\\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeﬀrey Pennington,\\nand Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. In International\\nConference on Learning Representations, 2018.\\nDonald Ludwig. The Radon transform on Euclidean space. Communications on Pure and\\nApplied Mathematics, 19:49–81, 1966.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 29, 'page_label': '30'}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nBenoit B. Mandelbrot and John W. Van Ness. Fractional Brownian motions, fractional\\nnoises and applications. SIAM Review, 10(4):422–437, 1968.\\nAlexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin\\nGhahramani. Gaussian process behaviour in wide deep neural networks. In International\\nConference on Learning Representations, 2018.\\nRobert A. Minlos. Generalized random processes and their extension in measure. Trudy\\nMoskovskogo Matematicheskogo Obshchestva, 8:497–518, 1959.\\nRadford M. Neal. Bayesian Learning for Neural Networks . Lecture Notes in Statistics.\\nSpringer New York, 1996.\\nRoman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolaﬁa,\\nJeﬀrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks\\nwith many channels are Gaussian processes. In International Conference on Learning\\nRepresentations, 2019.\\nGreg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view\\nof bounded norm inﬁnite width ReLU nets: The multivariate case. In International\\nConference on Learning Representations, 2020.\\nRahul Parhi and Robert D. Nowak. Banach space representer theorems for neural networks\\nand ridge splines. Journal of Machine Learning Research , 22(43):1–40, 2021.\\nRahul Parhi and Robert D. Nowak. What kinds of functions do deep neural networks learn?\\nInsights from variational spline theory. SIAM Journal on Mathematics of Data Science , 4\\n(2):464–489, 2022.\\nRahul Parhi and Robert D. Nowak. Near-minimax optimal estimation with shallow ReLU\\nneural networks. IEEE Transactions on Information Theory , 69(2):1125–1140, 2023a.\\nRahul Parhi and Robert D. Nowak. Deep learning meets sparse regularization: A signal\\nprocessing perspective. IEEE Signal Processing Magazine , 40(6):63–74, 2023b.\\nRahul Parhi and Michael Unser. Distributional extension and invertibility of the k-plane\\ntransform and its dual. SIAM Journal on Mathematical Analysis , 56(4):4662–4686, 2024.\\nRahul Parhi and Michael Unser. Function-space optimality of neural architectures with\\nmultivariate nonlinearities. SIAM Journal on Mathematics of Data Science , 7(1):110–135,\\n2025.\\nAlexander G. Ramm and Alexander I. Katsevich. The Radon transform and local tomography.\\nCRC Press, Boca Raton, FL, 1996.\\nWalter Rudin. Functional analysis. International Series in Pure and Applied Mathematics.\\nMcGraw-Hill, Inc., New York, second edition, 1991.\\nKen-Iti Sato. L´ evy Processes and Inﬁnitely Divisible Distributions. Cambridge Studies in\\nAdvanced Mathematics. Cambridge University Press, 1999.\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.20', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-21T03:17:38+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-21T03:17:38+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) kpathsea version 6.3.1', 'source': 'PDF_docs/doc_3.pdf', 'total_pages': 31, 'page': 30, 'page_label': '31'}, page_content='Random ReLU Neural Networks as Non-Gaussian Processes\\nJoseph Shenouda, Rahul Parhi, Kangwook Lee, and Robert D. Nowak. Variation spaces for\\nmulti-output neural networks: Insights on multi-task learning and network compression.\\nJournal of Machine Learning Research , 25(231):1–40, 2024.\\nMichael Unser. Ridges, neural networks, and the Radon transform. Journal of Machine\\nLearning Research, 24(37):1–33, 2023.\\nMichael Unser and Pouya D. Tafti. An introduction to sparse stochastic processes. Cambridge\\nUniversity Press, 2014.\\nMichael Unser, Pouya D. Tafti, and Qiyu Sun. A uniﬁed formulation of Gaussian versus\\nsparse stochastic processes—Part I: Continuous-domain theory. IEEE Transactions on\\nInformation Theory, 60(3):1945–1962, 2014.\\nChristopher Williams. Computing with inﬁnite networks. Advances in Neural Information\\nProcessing Systems, 9, 1996.\\nSho Yaida. Non-Gaussian processes and neural networks at ﬁnite widths. In Mathematical\\nand Scientiﬁc Machine Learning , pages 165–192. PMLR, 2020.\\nGreg Yang. Tensor programs I: Wide feedforward or recurrent neural networks of any\\narchitecture are Gaussian processes. Advances in Neural Information Processing Systems ,\\n32, 2019.\\nJacob Zavatone-Veth and Cengiz Pehlevan. Exact marginal prior distributions of ﬁnite\\nBayesian neural networks. Advances in Neural Information Processing Systems , 34:\\n3364–3375, 2021.\\n31'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows; modified using iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': 'D:20010309153645', 'moddate': '2025-02-18T06:49:22-08:00', 'subject': 'N Engl J Med 1997.336:1569-1574', 'author': 'Bailar', 'title': 'Cancer Undefeated', 'source': 'PDF_docs/doc_2.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='CANCER UNDEFEATED\\n \\nVolume 336 Number 22\\n \\n/H11554\\n \\n1569\\n \\nSpecial Article\\n \\nCANCER UNDEFEATED\\n \\nJ\\n \\nOHN\\n \\n C. B\\n \\nAILAR\\n \\n III, M.D., P\\n \\nH\\n \\n.D., \\n \\nAND\\n \\n H\\n \\nEATHER\\n \\n L. G\\n \\nORNIK\\n \\n, M.H.S.\\n \\nA\\n \\nBSTRACT\\n \\nBackground\\n \\nDespite decades of basic and clinical\\nresearch and trials of promising new therapies, can-\\ncer remains a major cause of morbidity and mortal-\\nity. We assessed overall progress against cancer in\\nthe United States from 1970 through 1994 by analyz-\\ning changes in age-adjusted mortality rates.\\n \\nMethods\\n \\nWe obtained from the National Center for\\nHealth Statistics data on all deaths from cancer and\\nfrom cancer at specific sites, as well as on deaths due\\nto cancer according to age, race, and sex, for the years\\n1970 through 1994. We computed age-specific mor-\\ntality rates and adjusted them to the age distribution\\nof the U.S. population in 1990.\\n \\nResults\\n \\nAge-adjusted mortality due to cancer in\\n1994 (200.9 per 100,000 population) was 6.0 percent\\nhigher than the rate in 1970 (189.6 per 100,000). After\\ndecades of steady increases, the age-adjusted mor-\\ntality due to all malignant neoplasms plateaued, then\\ndecreased by 1.0 percent from 1991 to 1994. The de-\\ncline in mortality due to cancer was greatest among\\nblack males and among persons under 55 years of\\nage. Mortality among white males 55 or older has\\nalso declined recently. These trends reflect a combi-\\nnation of changes in death rates from specific types\\nof cancer, with important declines due to reduced\\ncigarette smoking and improved screening and a\\nmixture of increases and decreases in the incidence\\nof types of cancer not closely related to tobacco use.\\n \\nConclusions\\n \\nThe war against cancer is far from\\nover. Observed changes in mortality due to cancer\\nprimarily reflect changing incidence or early detec-\\ntion. The effect of new treatments for cancer on mor-\\ntality has been largely disappointing. The most prom-\\nising approach to the control of cancer is a national\\ncommitment to prevention, with a concomitant rebal-\\nancing of the focus and funding of research. (N Engl\\nJ Med 1997;336:1569-74.)\\n \\n©1997, Massachusetts Medical Society.\\n \\nFrom the Department of Health Studies, University of Chicago, 5841\\nS. Maryland Ave., MC 2007, Chicago, IL 60637-1470, where reprint re-\\nquests should be addressed to Dr. Bailar.\\n \\nN 1986, when one of us reported on trends in\\nthe incidence of cancer in the United States\\nfrom 1950 through 1982,\\n \\n1\\n \\n it was clear that\\nsome 40 years of cancer research, centered pri-\\nmarily on treatment, had failed to reverse a long,\\nslow increase in mortality. Here we update that\\nanalysis through 1994. Our evaluation begins with\\n1970, both to provide some overlap with the previ-\\nous article and because passage of the National Can-\\nI\\n \\ncer Act of 1971 marked a critical increase in the\\nmagnitude and vigor of the nation’s efforts in cancer\\nresearch.\\n \\n2\\n \\nThe 1986 report and follow-up articles\\n \\n1,3-5\\n \\n were\\ncriticized,\\n \\n6-8\\n \\n primarily on the grounds that research\\nalready completed had not yet been incorporated\\ninto practice and that new research findings were on\\nthe way. Critics also argued that data for all cancers\\ncombined are not meaningful and that the study of\\nage-adjusted mortality rates is not appropriate when\\nthe rates in different age groups exhibit different\\ntrends, as they do for cancer.\\nThe Senate asked the National Cancer Institute to\\nconvene a committee to consider how to measure\\nprogress against cancer, and it published its report in\\n1990.\\n \\n9\\n \\n The committee recommended that progress\\nbe assessed in three general areas: direct measures\\n(mortality, incidence, and survival, including the\\nquality of life), portents of change (such as reductions\\nin tobacco use), and advances in knowledge that may\\nhave an effect in the future.\\n \\n9\\n \\n Direct measures were\\ntaken to be central to the assessment of progress.\\nThe most basic measure of progress against cancer\\nis age-adjusted mortality. The use of rates removes\\nthe effect of changes in the overall size of the pop-\\nulation. Adjustment for age further removes the ef-\\nfect of changes in the age distribution of the popu-\\nlation, and with it the effect of changing mortality\\nfrom causes other than cancer. The use of mortality\\nas the chief measure of progress against cancer, rath-\\ner than incidence or survival, focuses attention on\\nthe outcome that is most reliably reported and is of\\ngreatest concern to the public: death. The use of\\nrates for all types of cancer combined, though diffi-\\ncult to interpret in biologic terms, usefully supple-\\nments site-specific rates because it prevents selective\\nreporting of data to support particular views and min-\\nimizes the effects of changes in the diagnosis and re-\\nporting of specific types of cancer.\\nBriefly summarized, the reason for not focusing\\non the reported incidence of cancer is that the scope\\nand precision of diagnostic information, practices in\\nscreening and early detection, and criteria for re-\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 18, 2025. For personal use only. \\n No other uses without permission. Copyright © 1997 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows; modified using iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': 'D:20010309153645', 'moddate': '2025-02-18T06:49:22-08:00', 'subject': 'N Engl J Med 1997.336:1569-1574', 'author': 'Bailar', 'title': 'Cancer Undefeated', 'source': 'PDF_docs/doc_2.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='1570\\n \\n/H11554\\n \\nMay 29, 1997\\n \\nThe New England Journal of Medicine\\n \\nporting cancer have changed so much over time that\\ntrends in incidence are not reliable.\\n \\n1\\n \\n For example,\\nthe development and vigorous commercial promo-\\ntion of the test for prostate-specific antigen occurred\\nat the same time as a doubling of the reported inci-\\ndence of cancer of the prostate between 1974 and\\n1990 (from 65.6 per 100,000 population to 131.8\\nper 100,000),\\n \\n10\\n \\n without visibly affecting mortality.\\nFew knowledgeable observers believe that either\\nthe true frequency or the lethality of the disease has\\nchanged much. A similar but smaller trend has af-\\nfected rates of breast cancer, and there are reasons\\nfor concern about the incidence of other cancers.\\n \\n1\\n \\nTrends in survival rates are also suspect, because\\nthey are based on the same series of patients as inci-\\ndence rates, and any inflation of incidence due to the\\ninclusion of less malignant or nonmalignant diseases\\ncreates a spurious increase in case survival rates.\\n \\nMETHODS\\n \\nSources of Data\\n \\nNumbers of deaths according to year, age, race, sex, and cancer\\nsite were obtained from the National Center for Health Statis-\\ntics.\\n \\n11\\n \\n Population data came from the Bureau of the Census\\n \\n \\n \\nand\\nthe National Center for Health Statistics\\n \\n12,13\\n \\n (and Rosenberg H,\\nMortality Statistics Branch: personal communication). Other data\\nwere obtained from the National Cancer Institute.\\n \\n10\\n \\nAge-specific mortality rates, the building blocks of age-adjusted\\nrates, are simple ratios of numbers of deaths to the size of the pop-\\nulation. The numerators are the numbers of deaths from a specific\\ncancer or group of cancers among people in specific age ranges\\nand, often, with specific demographic characteristics. The denom-\\ninator is the corresponding U.S. population, as estimated by the\\nBureau of the Census. Data adjusted for age by the “direct” meth-\\nod (which we use throughout) are weighted sums of these age-\\nspecific rates, with the weights determined by reference to some\\nfixed population, such as the total U.S. population in the 1990\\ncensus.\\n \\n14\\n \\n For example, age adjustment of rates for each of the years\\nfrom 1984 through 1994 to the 1990 standard entails the estima-\\ntion of mortality as if the actual population in each of those years\\nhad the same age distribution as the 1990 U.S. population.\\nIf we want to examine recent changes in overall mortality due\\nto cancer, the most appropriate reference population for adjust-\\nment is one that falls within, or very close to, the period of study.\\nBecause we are focusing largely on events in recent years, we have\\nused the U.S. population as reflected in the 1990 census.\\nWhen trends in different age groups diverge, the choice of a\\nreference population can make a substantial difference in estimat-\\ned trends. For example, the population of the United States was\\nmuch younger in 1940 than in 1990, and hence the use of the\\n1940 population as the reference group gives greater weight to\\nmortality rates among younger persons, which have been declin-\\ning, whereas rates in older persons have been increasing. There-\\nfore, the 1940 standard gives an unduly favorable picture of re-\\ncent trends in mortality due to cancer; rates adjusted to the 1970\\nstandard lie between those adjusted to 1940 and those adjusted to\\n1990. Data presented at a recent press conference by the Depart-\\nment of Health and Human Services and the American Cancer\\nSociety, and in a related publication, reported rates that were ad-\\njusted to the 1970 and 1940 populations.\\n \\n15,16\\n \\nRESULTS\\n \\nTable 1 shows age-adjusted death rates for all ma-\\nlignant neoplasms, year by year, since 1986. For the\\nU.S. population as a whole, the long-sustained annu-\\nal increase in mortality due to cancer ceased in about\\n1991. Between 1991, when the highest rate was re-\\nported, and 1994, the most recent year for which\\ndata are available, mortality decreased by 1.0 percent\\n(from 203.0 to 200.9 per 100,000 population). This\\ndrop may well portend larger improvements to come.\\nEven if rates turn upward again, the decline will sure-\\nly resume within the next few years as a result of re-\\nductions in smoking over recent decades.\\nFor historical perspective, U.S. cancer mortality\\nrates, age adjusted to 1970 by the National Cancer\\nInstitute, increased by an estimated 0.3 percent an-\\nnually from 1975 through 1993, as compared with\\nan increase of 0.1 percent per year from 1950 through\\n1975.\\n \\n10\\n \\n This accelerated increase in mortality due to\\ncancer occurred despite the enlarged scope of cancer\\nresearch since 1971.\\nFigure 1 presents trends in mortality from all ma-\\nlignant neoplasms since 1970, according to race and\\nsex. After decades of rather steady increases in each\\ndemographic group, mortality rates plateaued or de-\\nclined slightly in the 1990s, most notably in the\\nblack male population, among whom the recent\\ndownward trend follows years of rapidly increasing\\nmortality.\\nFigure 2 shows trends since 1970 for males and\\nfemales in two broad age groups. The population\\nunder 55 years of age is much larger than the older\\npopulation, whereas rates of mortality due to cancer\\nare much higher among older people than in the\\nyounger age group. As a result, the smaller percent-\\nage increase in mortality observed in the smaller,\\nolder group represents more deaths than the larger\\npercentage decrease in the younger group. The in-\\nterplay of these factors determines the population-\\nwide rate, which has changed much more slowly than\\nrates within these two broad age groups.\\nAmong older persons, both men and women,\\nmortality due to cancer increased by 15 to 20 per-\\ncent between 1970 and 1994, with a recent decline\\namong older men. During the same period, mortal-\\nity due to cancer among people younger than 55 de-\\ncreased by about 25 percent for both sexes. The\\nclose parallels between the rates for males and fe-\\nmales in each age category seem coincidental, since\\nthe rates for the two sexes reflect distinct patterns of\\ncancer sites.\\nWe turn now to some specific forms of cancer.\\nMortality due to breast cancer has increased by ap-\\nproximately 10 percent since 1970 among women\\n55 years of age or older, with a recent plateau, but\\nhas decreased by almost 25 percent among younger\\nwomen (Fig. 3). The recent and substantial increase\\nin the use of mammography among women over 50,\\nfor whom annual examination is known to be effec-\\ntive, has not prevented this increase. These data sug-\\ngest that a true increase in incidence may have been\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 18, 2025. For personal use only. \\n No other uses without permission. Copyright © 1997 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows; modified using iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': 'D:20010309153645', 'moddate': '2025-02-18T06:49:22-08:00', 'subject': 'N Engl J Med 1997.336:1569-1574', 'author': 'Bailar', 'title': 'Cancer Undefeated', 'source': 'PDF_docs/doc_2.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='CANCER UNDEFEATED\\n \\nVolume 336 Number 22\\n \\n/H11554\\n \\n1571\\n \\nonly partially offset by the effectiveness of screening.\\nAlthough mammography before the age of 50 is\\ncontroversial, these data suggest that declines in\\nmortality were well established before mammogra-\\nphy became widely used. Overall, the decrease among\\nyounger women and the increase among older wom-\\nen have left population-wide mortality almost un-\\nchanged.\\nFor lung cancer, death rates for women 55 or old-\\ner have increased to almost four times the 1970 rate,\\nwhereas rates among males younger than 55 have\\ndecreased slightly (Fig. 4). Rates for older men and\\nyounger women have risen since 1970, but with\\nsome recent downturn. These trends reflect delayed\\neffects of changes in smoking habits that occurred\\ndecades ago.\\nFigure 5 shows trends in mortality for addition-\\nal types of cancer from 1970 through 1993. Age-\\nadjusted rates for several important types of cancer\\ndeclined steadily. The decrease in cancer of the stom-\\nach, observed worldwide over many decades, is not\\nwell understood, but it is largely or entirely a result\\nof decreasing incidence rather than earlier detection\\nor improved therapy. The sharp decline for cancer of\\nthe cervix is also not fully explained but reflects a\\ncombination of reduced incidence and improvements\\nin the detection of premalignant lesions by means of\\nthe Papanicolaou smear and their subsequent remov-\\nal; earlier detection of invasive cervical neoplasms\\nmay also be important. Deaths from cancer of the\\nuterus (including uterine neoplasms not specified as\\nof the cervix) are primarily due to endometrial can-\\ncers, but they include a small proportion of deaths\\nfrom cervical cancer reported as nonspecific cancer\\nof the uterus and a few malignant myometrial neo-\\nplasms. Here, too, there has been a sustained de-\\n \\n*The rates shown are numbers of deaths from all\\nmalignant neoplasms per 100,000 population. Rates\\nhave been adjusted for age, with standardization to\\nthe age distribution of the U.S. resident population\\nin 1990.\\n \\nT\\n \\nABLE\\n \\n 1.\\n \\n R\\n \\nECENT\\n \\n T\\n \\nRENDS\\n \\n \\n \\nIN\\n \\n M\\n \\nORTALITY\\n \\n \\nD\\n \\nUE\\n \\n \\n \\nTO\\n \\n C\\n \\nANCER\\n \\n \\n \\nIN\\n \\n \\n \\nTHE\\n \\n U\\n \\nNITED\\n \\n S\\n \\nTATES\\n \\n.*\\n \\nY\\n \\nEAR\\n \\n T\\n \\nOTAL\\n \\nM\\n \\nALES\\n \\nF\\n \\nEMALES\\n \\ndeaths/100,000\\n \\n1986 199.0 256.4 161.3\\n1987 199.2 256.7 161.3\\n1988 199.8 256.8 162.3\\n1989 201.6 258.4 164.1\\n1990 202.4 259.6 164.6\\n1991 203.0 259.3 165.7\\n1992 201.8 256.7 165.3\\n1993 202.1 256.5 165.7\\n1994 200.9 253.2 165.7\\n \\nFigure 1.\\n \\n Mortality from All Malignant Neoplasms, 1970\\nthrough 1994, in the Total U.S. Population and According to\\nRace and Sex.\\nThe rates have been age-adjusted to the U.S. resident popula-\\ntion of 1990.\\n150\\n350\\n1970 1990\\n250\\n1975 1980 1985\\nDeaths per 100,000\\nBlack males\\nWhite males\\nBlack females\\nWhite females\\nTotal\\n \\nFigure 2.\\n \\n Mortality from All Malignant Neoplasms, 1970 through\\n1994, in the Total U.S. Population as a Percentage of the Rate\\nin 1970, According to Age and Sex.\\nThe rates have been age-adjusted to the U.S. resident popula-\\ntion of 1990.\\n70\\n120\\n1970 1990\\n100\\n110\\n80\\n90\\n1975 1980 1985\\nPercentage of 1970\\nMortality Rate\\nFemales /H1109155 yr\\nMales /H1109155 yr\\nMales 0 – 54 yr\\nFemales 0 – 54 yr\\n \\nFigure 3.\\n \\n Mortality from Breast Cancer, 1970 through 1993, in\\nthe Total U.S. Female Population as a Percentage of the Rate in\\n1970, According to Age.\\nThe rates have been age-adjusted to the U.S. female resident\\npopulation of 1990.\\n75\\n115\\n1970 1990\\n95\\n105\\n85\\n1975 1980 1985\\nPercentage of 1970\\nMortality Rate\\nFemales /H1109155 yr\\nAll females\\nFemales 0 – 54 yr\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 18, 2025. For personal use only. \\n No other uses without permission. Copyright © 1997 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows; modified using iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': 'D:20010309153645', 'moddate': '2025-02-18T06:49:22-08:00', 'subject': 'N Engl J Med 1997.336:1569-1574', 'author': 'Bailar', 'title': 'Cancer Undefeated', 'source': 'PDF_docs/doc_2.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='1572\\n \\n/H11554\\n \\nMay 29, 1997\\n \\nThe New England Journal of Medicine\\n \\ncline, though not as great as for cervical cancer, and\\nat least a part of this improvement is due to earlier\\ndetection.\\nMortality from leukemia (all types and in all age\\ngroups) has also decreased. Deaths from colorectal\\ncancer (including anal cancer) decreased substan-\\ntially for reasons that are not entirely clear, but they\\nmay include earlier detection as well as a reduction\\nin incidence.\\n \\n10\\n \\n Improved treatment has contributed\\nlittle.\\nSmall increases have been reported for malignant\\nbrain tumors and malignant melanoma, shown here\\nsince 1979, when the National Center for Health\\nStatistics introduced a new format for reporting mor-\\ntality data.\\n \\n11\\n \\n Mortality from lymphomas and other\\nlymphoid neoplasms (including Hodgkin’s disease,\\nnon-Hodgkin’s lymphoma, and multiple myeloma)\\nincreased by 17.3 percent from 1970 to 1993, de-\\nspite reductions in mortality from Hodgkin’s disease\\nalone.\\n \\n10\\n \\nTrends in mortality due to cancer among children\\nrequire special comment. Death rates for each major\\ncategory of childhood cancer have declined by about\\n50 percent since the 1970s (data not shown). The\\ndecline is continuing, and the percentage drop in\\nthe most recent 10-year period is slightly greater\\nthan that for the previous 10 years. To put this find-\\ning in perspective, however, cancer acc ounted for\\nonly 1699 deaths among children under 15 years of\\nage in the United States in 1993, among a total of\\n529,904 deaths due to cancer in all age groups.\\n \\n11\\n \\nEven the complete elimination of deaths due to child-\\nhood cancer would have little effect on the national\\ndeath toll.\\n \\nDISCUSSION\\n \\nIt is worth reviewing probable reasons for these\\nchanges in mortality due to cancer. Some declines\\nare clearly a result of reduced incidence or earlier de-\\ntection (cancer of the cervix, other cancers of the\\nuterus, and cancers of the colon, rectum, and stom-\\nach). Similarly, recent changes in mortality from lung\\ncancer are certainly due to changes in smoking pat-\\nterns over the past few decades. The smaller increas-\\nes in mortality from melanoma and cancer of the\\nbrain, the prostate, and perhaps the breast (in older\\nwomen) can hardly be due to a decline in the effective-\\nness of treatment; they must reflect rising incidence.\\nThus, the observed trends largely reflect changing in-\\ncidence or earlier detection, rather than improved\\ntherapy.\\nDespite numerous past claims that success was just\\naround the corner, mortality due to cancer contin-\\nued to increase, until quite recently. The death rate\\nin 1994 was 2.7 percent higher than in 1982, the last\\nyear covered in the 1986 paper,\\n \\n1\\n \\n but it is likely that\\nthe recent downturn will be confirmed and substan-\\ntially extended as a result of improved prevention\\n \\nFigure 4.\\n \\n Mortality from Cancer of the Trachea, Bronchus, or\\nLung, 1970 through 1993, in the Total U.S. Population as a Per-\\ncentage of the Rate in 1970, According to Age and Sex.\\nThe rates have been age-adjusted to the U.S. resident popula-\\ntion of 1990.\\n400\\n1970 1990\\n200\\n300\\n100\\n1975 1980 1985\\nPercentage of 1970\\nMortality Rate\\nFemales /H1109155 yr\\nMales /H1109155 yr\\nFemales 0 – 54 yr\\nMales 0 – 54 yr\\n \\nFigure 5.\\n \\n Mortality from Cancer at Selected Sites, 1970 through\\n1993, in the Total U.S. Population.\\nThe rates have been age-adjusted to the U.S. resident popula-\\ntion of 1990.\\n0\\n10\\n1970 1990\\n20\\n30\\n1975 1980 1985\\nDeaths per 100,000\\nMelanoma\\nCervical cancer\\nCancer of uterus\\nCancer of brain\\nStomach cancer\\nLeukemia\\nLymphoma and\\nother cancers\\nColorectal cancer\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 18, 2025. For personal use only. \\n No other uses without permission. Copyright © 1997 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows; modified using iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': 'D:20010309153645', 'moddate': '2025-02-18T06:49:22-08:00', 'subject': 'N Engl J Med 1997.336:1569-1574', 'author': 'Bailar', 'title': 'Cancer Undefeated', 'source': 'PDF_docs/doc_2.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='CANCER UNDEFEATED\\n \\nVolume 336 Number 22\\n \\n/H11554\\n \\n1573\\n \\nand earlier detection and, especially, past reductions\\nin tobacco use.\\nIn 1986, we concluded that “some 35 years of in-\\ntense effort focused largely on improving treatment\\nmust be judged a qualified failure.”\\n \\n1\\n \\n Now, with 12\\nmore years of data and experience, we see little rea-\\nson to change that conclusion, though this assess-\\nment must be tempered by the recognition of some\\nareas of important progress. These include the much-\\nimproved outlook for children and young adults\\nwith cancer, which is entirely the result of improved\\ntreatment; better treatment for Hodgkin’s disease;\\nfar better palliation of many kinds of advanced can-\\ncer; a better understanding of cancer, which as a by-\\nproduct has improved the medical management of\\nnonmalignant immunologic, metabolic, and viral\\ndiseases, including the acquired immunodeficiency\\nsyndrome; and great improvements in imaging tech-\\nnology. Though these benefits must not be dis-\\ncounted, their effects on overall mortality due to\\ncancer have been largely disappointing.\\nThe argument that rising incidence has just bal-\\nanced rising case survival rates, so that mortality is\\nroughly constant, seems unlikely to be true but is ir-\\nrelevant anyway. However one analyzes and inter-\\nprets the present data, the salient fact remains that\\nage-adjusted rates of death due to cancer are now\\nbarely declining. Hopes for a substantial reduction\\nin mortality by the year 2000 were clearly mis-\\nplaced.\\n \\n17\\n \\n The effect of primary prevention (e.g., re-\\nductions in the prevalence of smoking) and second-\\nary prevention (e.g., the Papanicolaou smear) on\\nmortality due to cancer indicates a pressing need for\\nreevaluation of the dominant research strategies of\\nthe past 40 years, particularly the emphasis on im-\\nproving treatments, and a redirection of effort to-\\nward prevention.\\nUnfortunately, the means to prevent most cancers\\nhave not yet been elucidated, adequately tested, and\\nshown to be effective and feasible. For example, we\\nneed to know more about how to help the smoker\\nwho wants to quit, and much of the evidence that\\ndiet is related to one third or more of cancers\\n \\n18\\n \\n must\\nbe reduced to findings about specific dietary com-\\nponents. The needed research on prevention may\\ndemand as much in time, effort, and resources as has\\nalready been invested in studies of treatment. We em-\\nphatically do not propose that research on treatment\\nbe stopped; there should, however, be a substantial\\nrealignment of the balance between treatment and\\nprevention, and in an age of limited resources this\\nmay well mean curtailing efforts focused on therapy.\\nPrevention is much broader than the elimination\\nof carcinogens. For example, recent progress in un-\\nderstanding the roles of dietary modification, chemo-\\nprophylaxis (e.g., with retinoic acid and tamoxifen),\\nand genetic predispositions to cancer (in order to\\nreduce exposure to carcinogens and to increase sur-\\nveillance with the goal of earlier detection) holds in-\\ntriguing promise for substantial reductions in mor-\\ntality due to cancer, although much critical research\\nremains to be done. Also part of “prevention” re-\\nsearch is the investigation of risk factors for cancer\\nin order to determine which factors can be modified\\nand investigations in the behavioral sciences aimed\\nat improving the application of findings relevant to\\nprevention. The role of basic research is unclear,\\npartly because what is called “basic” is highly sub-\\njective and can be rapidly redefined in response to\\nthreatened budget cuts. However, we support the\\nexpansion of basic-science research that is not so\\nbasic as to have no clear, direct, and specific link to\\nprevention.\\nWill we at some future time do better in the war\\nagainst cancer? The present optimism about new\\ntherapeutic approaches rooted in molecular medi-\\ncine may turn out to be justified, but the arguments\\nare similar in tone and rhetoric to those of decades\\npast about chemotherapy, tumor virology, immu-\\nnology, and other approaches. In our view, prudence\\nrequires a skeptical view of the tacit assumption that\\nmarvelous new treatments for cancer are just waiting\\nto be discovered.\\nWe, like others, earnestly hope that such discover-\\nies can and will be made, but it is now evident that\\nthe worldwide cancer research effort should un-\\ndergo a substantial shift toward efforts to improve\\nprevention. Will this shift mean that prevention re-\\nsearch will ultimately succeed in the way that treat-\\nment research was expected to succeed? There is no\\nguarantee that it will. The ultimate results may be as\\ndisappointing as those to date from treatment ef-\\nforts, but it is time to find out.\\nThere are also questions of implementation. Pre-\\nvention is likely to be more difficult and costly than\\ntreatment, which can be rather narrowly focused on\\npersons in need during a limited time and can be\\nprovided without major changes in the ambient en-\\nvironment, workplace, diet, or consumer products.\\nTreatment, if it could be made to work, would ob-\\nviously be much simpler.\\nThe public seems to understand the need for the\\nshift in attitude and emphasis toward prevention. The\\nevidence includes the large and continuing reduction\\nin smoking, widespread individual efforts to change\\ndiet to prevent cancer, and the use of sunscreens to\\nreduce exposure to sunlight. The government has had\\nlittle role in these changes. However, to leave this\\nmatter entirely to the public is to risk faddism, on\\nthe one hand, and a turning away from orthodox ther-\\napy, on the other.\\nAside from overstatement of the decline in mor-\\ntality due to cancer in the United States in recent\\nyears, the recent joint press conference\\n \\n15\\n \\n held by the\\nDepartment of Health and Human Services and the\\nAmerican Cancer Society was notable for its public\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 18, 2025. For personal use only. \\n No other uses without permission. Copyright © 1997 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows; modified using iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': 'D:20010309153645', 'moddate': '2025-02-18T06:49:22-08:00', 'subject': 'N Engl J Med 1997.336:1569-1574', 'author': 'Bailar', 'title': 'Cancer Undefeated', 'source': 'PDF_docs/doc_2.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='1574\\n \\n/H11554\\n \\nMay 29, 1997\\n \\nThe New England Journal of Medicine\\n \\nrecognition of the importance of prevention in the\\neffort to control cancer. According to Secretary of\\nHealth and Human Services Donna Shalala,\\n \\nWe must continue to work for the day when our children\\nmust turn to the history books to learn about a disease\\ncalled cancer....I t  will take better research, better\\ntreatments, better detection, and most important, it will\\ntake better education....F r o m  tobacco to poor diet to\\nlack of reproductive screenings, we must give the Ameri-\\ncan people the information they need to prevent cancer\\nand make the best choices with their lives.\\n \\n15\\n \\nWe hope that this statement, as well as the recent in-\\ncrease in support of prevention activities in the Na-\\ntional Cancer Institute budget,\\n \\n19\\n \\n represents an early\\nstep in the commitment to prevention, rather than\\nlip service obscuring blind faith in treatment-based\\napproaches.\\nThe best of modern medicine has much to offer\\nto virtually every patient with cancer, for palliation\\nif not always for cure, and every patient should have\\naccess to the earliest possible diagnosis and the\\nbest possible treatment. The problem is the lack of\\nsubstantial improvement over what treatment could\\nalready accomplish some decades ago. A national\\ncommitment to the prevention of cancer, largely re-\\nplacing reliance on hopes for universal cures, is now\\nthe way to go.\\n \\nPresented in part as the Ramazzini Lecture, given by Dr. Bailar\\non October 26, 1996, in Carpi, Italy, as part of the annual Ra-\\nmazzini Days of the Collegium Ramazzini.\\n \\nWe are indebted to the National Center for Health Statistics for\\nsupplying most of the data used in this study; to the National Cancer\\nInstitute and the Bureau of the Census for the remainder; and to\\nDr. Samuel Broder for kindly suggesting the title.\\n \\nREFERENCES\\n \\n1.\\n \\nBailar JC III, Smith EM. Progress against cancer? N Engl J Med 1986;\\n314:1226-32.\\n \\n2.\\n \\nNational Cancer Act, P. L. No. 99-158 (1971).\\n \\n3.\\n \\nBailar JC. Rethinking the war on cancer. Issues Sci Technol 1987;\\n4:16-21.\\n \\n4.\\n \\nIdem.\\n \\n Cancer in Canada: recent trends in mortality. Chronic Dis Can \\n1992;13:Suppl:S2-S8.\\n \\n5.\\n \\nIdem.\\n \\n Deaths from all cancer: trends in sixteen countries. Ann N Y Acad \\nSci 1990;609:49-56.\\n \\n6.\\n \\nProgress against cancer? N Engl J Med 1986;315:963-8.\\n \\n7.\\n \\nMore on progress against cancer. N Engl J Med 1987;316:752-4.\\n \\n8.\\n \\nThe war on cancer: views from the front. Issues Sci Technol 1988;\\n4:14-6.\\n \\n9.\\n \\nExtramural Committee to Assess Measures of Progress Against Cancer. \\nMeasurement of progress against cancer. J Natl Cancer Inst 1990;82:\\n825-35.\\n \\n10.\\n \\nRies LAG, Kosary CL, Hankey BF, Harras A, Miller BA, Edwards BK, \\neds. SEER cancer statistics review, 1973-1993: tables and graphs. Bethes-\\nda, Md.: National Cancer Institute (in press). (For preliminary edition see \\n<http://www-seer.ims.nci.nih.gov>.)\\n \\n11.\\n \\nNational Center for Health Statistics. Vital statistics of the United \\nStates, 1970–1994. Vol. 2. Mortality. Part A. Washington, D.C.: Govern-\\nment Printing Office, 1974-1996 (1993 and 1994 in press).\\n \\n12.\\n \\nBureau of the Census. Preliminary estimates of the population of the \\nUnited States, by age, sex, and race: 1970-1981. Current population re-\\nports. Series P-25. No. 917. Washington, D.C.: Government Printing Of-\\nfice, 1982.\\n \\n13.\\n \\nIdem.\\n \\n U.S. population estimates, by age, sex, race, and Hispanic origin: \\n1980-1991. Current population reports. Series P-25. No. 1095. Washing-\\nton, D.C.: Government Printing Office, 1993.\\n \\n14.\\n \\nBreslow NE, Day NE. Statistical methods in cancer research. Vol. 2. \\nThe design and analysis of cohort studies. Lyon, France: International \\nAgency for Research on Cancer, 1987. (IARC scientific publications no. \\n82.)\\n \\n15.\\n \\nShalala DE. New cancer mortality rates. Washington, D.C.: Depart-\\nment of Health and Human Services, 1996 (press conference).\\n \\n16.\\n \\nCole P, Rodu B. Declining cancer mortality in the United States. Can-\\ncer 1996;78:2045-8.\\n \\n17.\\n \\nNational Cancer Institute. Cancer control: objectives for the nation, \\n1985–2000. NCI monographs. No. 2. Washington, D.C.: Government \\nPrinting Office, 1986. (NIH publication no. 86-2880.)\\n \\n18.\\n \\nDoll R, Peto R. The causes of cancer: quantitative estimates of avoid-\\nable risks of cancer in the United States today. J Natl Cancer Inst 1981;\\n66:1191-308.\\n \\n19.\\n \\nFinancial Management Branch. NCI fact book. Bethesda, Md.: Na-\\ntional Cancer Institute, 1993–1996.\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 18, 2025. For personal use only. \\n No other uses without permission. Copyright © 1997 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.9', 'creator': 'Adobe InDesign CS5 (7.0)', 'creationdate': '2016-11-07T23:10:36+09:00', 'moddate': '2016-11-07T23:10:37+09:00', 'trapped': '/False', 'source': 'PDF_docs/doc_4.pdf', 'total_pages': 4, 'page': 0, 'page_label': '351'}, page_content='Deep learning is a form of machine learning that enables \\ncomputers to learn from experience and understand the \\nworld in terms of a hierarchy of concepts. Because the com-\\nputer gathers knowledge from experience, there is no need \\nfor a human computer operator formally to specify all of \\nthe knowledge needed by the computer. The hierarchy of \\nconcepts allows the computer to learn complicated concepts \\nby building them out of simpler ones; a graph of these hier-\\narchies would be many layers deep. This book introduces a \\nbroad range of topics in relation to deep learning. \\n The text offers a mathematical and conceptual background \\ncovering relevant concepts in linear algebra, probability \\ntheory and information theory, numerical computation, \\nand machine learning. It describes deep learning techniques \\nwhich are used by practitioners in industry, including deep \\nfeedforward networks, regularization, optimization algo-\\nrithms, convolutional networks, sequence modeling, and \\npractical methodology; and it surveys such applications as \\nnatural language processing, speech recognition, computer \\nvision, online recommendation systems, bioinformatics, and \\nvideogames. Finally, the book offers research perspectives \\ncovering such theoretical topics as linear factor models, au-\\ntoencoders, representation learning, structured probabilistic \\nmodels, Monte Carlo methods, the partition function, ap-\\nproximate inference, and deep generative models. \\n Deep learning can be used by undergraduate or graduate \\nstudents who are planning careers in either industry or re-\\nsearch, and by software engineers who want to begin using \\ndeep learning in their products or platforms. A website offers \\nsupplementary material for both readers and instructors.\\n This book can be useful for a variety of readers, but the \\nauthor wrote it with two main target audiences in mind. \\nOne of these target audiences is university students (under-\\ngraduate or graduate) who study machine learning, includ-\\nDeep Learning\\nKwang Gi Kim, PhD\\nBiomedical Engineering Branch, Division of Precision Medicine and Cancer Informatics, National Cancer Center, Goyang, Korea\\nE-mail: kimkg@ncc.re.kr\\nHealthc Inform Res. 2016 October;22(4):351-354. \\nhttps://doi.org/10.4258/hir.2016.22.4.351\\npISSN 2093-3681  •  eISSN 2093-369X  \\nBook Review\\nThis is an Open Access article distributed under the terms of the Creative Com-\\nmons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-\\nnc/4.0/) which permits unrestricted non-commercial use, distribution, and reproduc-\\ntion in any medium, provided the original work is properly cited.\\nⓒ 2016 The Korean Society of Medical Informatics\\nAuthor: Ian Goodfellow, Y oshua Bengio, and Aaron  \\nCourville\\nY ear: 2016\\nName and location of publisher: The MIT Press,  \\nCambridge, MA, USA\\nNumber of pages: 800\\nLanguage: English\\nISBN: 978-0262035613\\nReviewed\\nJanuary\\nFebruary\\nMarch\\nApril \\nMay \\nJune \\nJuly\\nAugust \\nSeptember \\nOctober \\nNovember \\nDecember'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.9', 'creator': 'Adobe InDesign CS5 (7.0)', 'creationdate': '2016-11-07T23:10:36+09:00', 'moddate': '2016-11-07T23:10:37+09:00', 'trapped': '/False', 'source': 'PDF_docs/doc_4.pdf', 'total_pages': 4, 'page': 1, 'page_label': '352'}, page_content='352 www.e-hir.org\\nKwang Gi Kim\\nhttps://doi.org/10.4258/hir.2016.22.4.351\\ning those who are beginning their careers in deep learning \\nand artiﬁcial intelligence research. The other target audience \\nconsists of software engineers who may not have a back-\\nground in machine learning or statistics but who nonetheless \\nwant to acquire this knowledge rapidly and begin using deep \\nlearning in their fields. Deep learning has already proven \\nuseful in many software disciplines, including computer vi-\\nsion, speech and audio processing, natural language process-\\ning, robotics, bioinformatics and chemistry, video games, \\nsearch engines, online advertising and ﬁnance.\\n This book has been organized into three parts so as to \\nbest accommodate a variety of readers. In Part I, the author \\nintro duces basic mathematical tools and machine learning \\nconcepts. Part II describes the most established deep learn-\\ning algorithms that are essentially solved technologies. Part \\nIII describes more speculative ideas that are widely believed \\nto be important for future research in deep learning.\\n In this book, certain areas assume that all readers have \\na computer science background. The authors assume fa-\\nmiliarity with programming and a basic understanding of \\ncomputational performance issues, complexity theory, intro-\\nductory-level calculus and some of the terminology of graph \\ntheory.\\nChapter 1. Introduction\\nThis book offers a solution to more intuitive problems in \\nthese areas. These solutions allow computers to learn from \\nexperience and understand the world in terms of a hierarchy \\nof concepts, with each concept defined in terms of its rela-\\ntionship to simpler concepts. By gathering knowledge from \\nexperience, this approach avoids the need for human opera-\\ntors to specify formally all of the knowledge needed by the \\ncomputer. The hierarchy of concepts allows the computer to \\nlearn complicated concepts by building them out of simpler \\nones. If the authors draw a graph to show how these con-\\ncepts have been built on top of each other, the graph will be \\ndeep, with many layers. For this reason, the authors call this \\napproach “ AI Deep Learning. ”\\nChapter 2. Linear Algebra\\nLinear algebra is a branch of mathematics that is widely \\nused throughout science and engineering. However, because \\nlinear algebra is a form of continuous rather than discrete \\nmathematics, many computer scientists have little experience \\nwith it. This chapter will completely omit many important \\nlinear algebra topics that are not essential for understanding \\ndeep learning.\\nChapter 3. Probability and Information Theory\\nThis chapter describes probability and information theory. \\nProbability theory is a mathematical framework for repre-\\nsenting uncertain statements. It provides a means of quan-\\ntifying uncertainties and axioms to derive new uncertainty \\nstatements. In addition, probability theory is a fundamental \\ntool of many disciplines of science and engineering. The \\nauthors mention this chapter to ensure that readers whose \\nfields are primarily in software engineering with limited ex-\\nposure to probability theory can understand the material in \\nthis book.\\nChapter 4. Numerical Computation \\nThis chapter includes a brief overview of numerical optimi-\\nzation in general. Machine learning algorithms usually re-\\nquire a large amount of numerical computation. This typical-\\nly refers to algorithms that solve mathematical problems by \\nmethods that update estimates of the solution via an iterative \\nprocess rather than analytically deriving a formula and thus \\nproviding a symbolic expression for the correct solution.\\nChapter 5. Machine Learning Basics \\nThis chapter introduces the basic concepts of generalization, \\nunderfitting, overfitting, bias, variance and regularization. \\nDeep learning is a specific type of machine learning. In or-\\nder to understand deep learning well, one must have a solid \\nunderstanding of the basic principles of machine learning. \\nThis chapter provides a brief course in the most important \\ngeneral principles, which will be applied throughout the \\nrest of the book. Novice readers or those who want to gain \\na broad perspective are recommended to consider machine \\nlearning textbooks with more comprehensive coverage of the \\nfundamentals, such as those by Murphy [1] or Bishop [2].\\nChapter 6. Deep Feedforward Networks\\nDeep feedforward networks, also often called neural net-\\nworks or multilayer perceptrons (MLPs), are the quintes-\\nsential deep-learning models. Feedforward networks are of \\nextreme importance to machine learning practitioners. They \\nform the basis of many important commercial applications. \\nFor example, the convolutional networks used for object rec-\\nognition from photos are a specialized type of feedforward \\nnetwork. Feedforward networks are a conceptual stepping \\nstone on the path to recurrent networks, which power many \\nnatural-language applications.\\nChapter 7. Regularization for Deep Learning\\nIn this chapter, the authors describe regularization in more'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.9', 'creator': 'Adobe InDesign CS5 (7.0)', 'creationdate': '2016-11-07T23:10:36+09:00', 'moddate': '2016-11-07T23:10:37+09:00', 'trapped': '/False', 'source': 'PDF_docs/doc_4.pdf', 'total_pages': 4, 'page': 2, 'page_label': '353'}, page_content='353Vol. 22  •  No. 4  •  October 2016 www.e-hir.org\\nDeep Learning\\ndetail, focusing on regularization strategies for deep models \\nor models that may be used as building blocks to form deep \\nmodels. Some sections of this chapter deal with standard \\nconcepts in machine learning. If readers are already famil-\\niar with these concepts, they may want to skip the relevant \\nsections. However, most of this chapter is concerned with \\nextensions of these basic concepts to the particular case of \\nneural networks.\\nChapter 8. Optimization for Training Deep Models\\nThis chapter focuses on one particular case of optimization: \\nfinding the parameters θ  of a neural network that signifi-\\ncantly reduce a cost function J(θ), which typically includes a \\nperformance measure evaluated on the entire training set as \\nwell as additional regularization terms.\\nChapter 9. Convolutional Networks\\nConvolutional networks [3], also known as neural networks \\nor CNNs, are a specialized type of neural network for pro-\\ncessing data that has a known, grid-like topology. In this \\nchapter, the authors initially describe what convolution is. \\nNext, they explain the motivation behind the use of convolu-\\ntion in a neural network, after which they describe an opera-\\ntion, called pooling, employed by nearly all convolutional \\nnetworks.\\nChapter 10. Sequence Modeling: Recurrent and Recur-\\nsive Nets\\nRecurrent neural networks or RNNs are a family of neural \\nnetworks for the processing of sequential data [4]. \\n This chapter extends the idea of a computational graph \\nto include cycles. These cycles represent the influence of \\nthe present value of a variable on its own value at a future \\ntime step. Such computational graphs allow one to define \\nrecurrent neural networks. Also in this chapter, the authors \\ndescribe several different ways to construct, train, and use \\nrecurrent neural networks.\\nChapter 11. Practical Methodology\\nSuccessfully applying deep learning techniques requires \\nmore than merely knowing what algorithms exist and under-\\nstanding the principles by which they work. \\n Correct application of an algorithm depends on mastering \\nsome fairly simple methodology. Many of the recommenda-\\ntions in this chapter are adapted from Ng [5].\\nChapter 12. Applications\\nIn this chapter, the authors describe how to use deep learn-\\ning to solve applications in computer vision, speech recog -\\nnition, natural language processing, and other application \\nareas of commercial interest. The authors begin by discuss-\\ning the large-scale neural network implementations that are \\nrequired for most serious AI applications.\\nChapter 13. Linear Factor Models\\nIn this chapter, the authors describe some of the simplest \\nprobabilistic models with latent variables, i.e., linear fac-\\ntor models. These models are occasionally used as building \\nblocks of mixture models [6-8] or larger, deep probabilistic \\nmodels. Additionally, they show many of the basic approach-\\nes that are necessary to build generative models, which are \\nmore advanced deep models.\\nChapter 14. Autoencoders \\nAn autoencoder is a neural network that is trained to at-\\ntempt to copy its input to its output. Internally, it has a hid-\\nden layer ‘h’ that describes the code used to represent the \\ninput.\\nChapter 15. Representation Learning\\nThis chapter initially discusses what it means to learn repre-\\nsentations and how the notion of representation can be use-\\nful to design deep architectures. Secondly, it discusses how \\nlearning algorithms share statistical strength across different \\ntasks, including the use of information from unsupervised \\ntasks to perform supervised tasks. \\nChapter 16. Structured Probabilistic Models for Deep \\nLearning\\nDeep learning draws upon many modeling formalisms that \\nresearchers can use to guide their design efforts and describe \\ntheir algorithms. One of these formalisms is the idea of \\nstructured probabilistic models.\\nChapter 17. Monte Carlo Methods\\nRandomized algorithms fall into two rough categories: Las \\nVegas algorithms and Monte Carlo algorithms. Las Vegas \\nalgorithms always return precisely the correct answer (or \\nreport their failure). These algorithms consume a random \\namount of resources, usually in the form of memory or time. \\nIn contrast, Monte Carlo algorithms return answers with a \\nrandom amount of error.\\nChapter 18. Confronting the Partition Function\\nIn this chapter, the authors describe techniques used for \\ntraining and evaluating models that have intractable parti-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.9', 'creator': 'Adobe InDesign CS5 (7.0)', 'creationdate': '2016-11-07T23:10:36+09:00', 'moddate': '2016-11-07T23:10:37+09:00', 'trapped': '/False', 'source': 'PDF_docs/doc_4.pdf', 'total_pages': 4, 'page': 3, 'page_label': '354'}, page_content='354 www.e-hir.org\\nKwang Gi Kim\\nhttps://doi.org/10.4258/hir.2016.22.4.351\\ntion functions.\\nChapter 19. Approximate Inference\\nThis chapter introduces several of the techniques used to \\nconfront these intractable inference problems.\\nChapter 20. Deep Generative Models\\nThis describes how to use these techniques to train proba-\\nbilistic models that would otherwise be intractable, such as \\ndeep belief networks and deep Boltzmann machines.\\n This book provides the reader with a good overview of \\ndeep learning, and in the future this knowledge can serve as \\ngood material when researching content related to the study \\nof artificial intelligence. \\n In particular, in medical imaging data, there are a number \\nof images which require some preparation processes. These \\nimages can create a higher detection rate with artificial intel-\\nligence of tumors and diseases to help medical staff. At pres-\\nent, much effort is required to create the proper data values. \\nIn the future, it will be possible to generate useful images \\nautomatically, with more input image data then utilized. This \\nbook describes a wide range of different methods that make \\nuse of deep learning for object or landmark detection tasks \\nin 2D and 3D medical imaging; it also examines a varied se-\\nlection of techniques for semantic segmentation or detection \\nusing deep learning principles in medical imaging.\\nReferences\\n1. Murphy KP . Machine learning: a probabilistic perspec-\\ntive. Cambridge (MA): MIT Press; 2012.\\n2. Bishop CM. Pattern recognition and machine learning. \\nNew Y ork (NY): Springer; 2006. p. 98-108. \\n3. Le Cun Y , Jackel LD, Boser B, Denker JS, Graf HP , Guy-\\non I, et al. Handwritten digit recognition: applications \\nof neural network chips and automatic learning. IEEE \\nCommun Mag 1989;27(11):41-6.\\n4. Rumelhart DE, Hinton GE, Williams RJ. Learning \\nrepresentations by back-propagating errors. Nature \\n1986;323(6088):533-6.\\n5. Ng A. Advice for applying machine learning [Internet]. \\nStanford (CA): Stanford University; 2015 [cited at 2016 \\nOct 22]. Available from: http://cs229.stanford.edu/mate-\\nrials/ML-advice.pdf.\\n6. Hinton GE, Dayan P , Frey BJ, Neal RM. The \"wake-\\nsleep\" algorithm for unsupervised neural networks. Sci-\\nence 1995;268(5214):1158-61.\\n7. Ghahramani Z, Hinton GE. The EM algorithm for mix-\\ntures of factor analyzers. Toronto, Canada: University of \\nToronto; 1996.\\n8. Roweis ST, Saul LK, Hinton GE. Global coordination \\nof local linear models. Adv Neural Inf Process Syst \\n2002;2:889-96.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(all_docs)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='granite-embedding:30m', base_url=None, client_kwargs={}, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding():\n",
    "    embeddings = OllamaEmbeddings(model=\"granite-embedding:30m\")  # Korrekte Embeddings\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embedding() \n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(name=collection_name)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def persistent_clientChroma():\n",
    "    persistent_client = chromadb.PersistentClient(path=\"./chroma_langchain_db\")  # Verzeichnis für Speicherung\n",
    "    collection = persistent_client.get_or_create_collection(\"collection_name\")\n",
    "    return collection\n",
    "\n",
    "collection = persistent_clientChroma()\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_Chroma():\n",
    "    doc_texts = [doc.page_content for doc in texts]  # Extrahiere den Text\n",
    "    doc_ids = [f\"doc_{i}\" for i in range(len(doc_texts))]  # Einzigartige IDs\n",
    "\n",
    "    collection_db = collection.add(ids=doc_ids, documents=doc_texts)  # Speichern in ChromaDB\n",
    "    return collection_db\n",
    "collection_db = add_doc_to_Chroma()\n",
    "collection_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_store_from_clientChroma():\n",
    "    # Schritt 5: Chroma-VectorStore mit gespeicherten Daten initialisieren\n",
    "    vector_store_from_client = Chroma(\n",
    "        persist_directory=\"./chroma_langchain_db\",\n",
    "        collection_name=\"collection_name\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return vector_store_from_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x128bc2cf0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_from_client= vector_store_from_clientChroma()\n",
    "vector_store_from_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'OllamaEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x128bc2cf0> search_kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='doc_23', metadata={}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nwhere (52) holds by the mutual independence of the (wk,bk) and (53) holds since the random\\nvariables\\n(w′\\nk,b′\\nk) |(w′\\nk,b′\\nk) ∈supp ψ (55)\\nare uniformly distributed on supp ψ. Next, deﬁne the auxiliary functional\\nM(ψ) =\\n∫\\nR\\n∫\\nsupp ψ\\neivψ(w,b) d(w,b) dPV(v). (56)\\nWe have that\\nˆQwPoi(ψ) = E\\n\\uf8ee\\n\\uf8f0\\nNψ∏\\nk=1\\nM(ψ)\\n|supp ψ|\\n\\uf8f9\\n\\uf8fb\\n= E\\n[( M(ψ)\\n|supp ψ|\\n)Nψ\\n]\\n=\\n∞∑\\nn=0\\n( M(ψ)\\n|supp ψ|\\n)n(λ|supp ψ|)n\\nn! e−λ|supp ψ| (57)\\n= e−λ|supp ψ|\\n∞∑\\nn=0\\n(λM(ψ))n\\nn!\\n= e−λ|supp ψ|eλM(ψ) (58)\\n= exp(λ(M(ψ) −|supp ψ|) (59)\\n= exp\\n(\\nλ\\n∫\\nR\\n∫\\nSd−1×R\\n(\\neivψ(z) −1\\n)\\ndzdPV(v)\\n)\\n, (60)\\nwhere (57) holds since Nψ is a Poisson random variable with mean λ|supp ψ|, (58) holds by\\nthe Taylor series expansion of t↦→et, (59) holds since |supp ψ|=\\n∫\\nsupp ψ1 dz, and (60) holds\\nsince z↦→eivψ(z) −1 vanishes outside supp ψ. At this point, we remark that, since PV is a\\nL´ evy measure (Deﬁnition 5), it is well-known that the form of (60) is continuous, positive\\ndeﬁnite, and satisﬁes ˆQwPoi(0) = 1 (see, e.g., Gelfand and Vilenkin, 1964, Theorem 2, p. 275).\\nThis implies that wPoi is indeed a generalized stochastic process that takes values in D′\\nR.\\nTo prove the lemma, it remains to extend the domain of ˆQwPoi to SR. To that end, let\\nˆPwPoi(ψ) = exp\\n(\\nλ\\n∫\\nR\\n∫\\nSd−1×R\\n(\\neivψ(z) −1\\n)\\ndzdPV(v)\\n)\\n, ψ ∈SR. (61)\\nWe now invoke an adaption of Fageot et al. (2014, Theorem 3) which investigates impulsive\\nwhite noise deﬁned on Rd as a special case. Their theorem implies that, thanks to the\\nadmissibility conditions on PV (Deﬁnition 5), the probability measures QwPoi and PwPoi are\\ncompatible on B(S′\\nR) = Bc(S′\\nR) ⊂Bc(D′\\nR) in the sense that\\nQwPoi(B) = PwPoi(B), for all B ∈B(S′\\nR) (62)\\nand QwPoi(D′\\nR \\\\S′\\nR) = 0, which proves the lemma.\\n16'),\n",
       " Document(id='doc_2', metadata={}, page_content='Two Hundred Years of Cancer Research\\nn engl j med 366;23 nejm.org june 7, 2012 2209\\n(NSABP), which Fisher led, he clearly showed that \\nradical en bloc removal of tissue did nothing \\nmore than could be accomplished by removing the \\ntumor mass itself, if surgery was supplemented by \\nchemotherapy, radiation therapy, or both. Fisher \\nalso showed that less radical surgery plus chemo-\\ntherapy or radiation therapy accomplished the \\ngoal with much less morbidity. These studies15-25\\nrevolutionized the treatment of breast cancer. \\nSince then, most other surgical procedures have \\nbeen tailored to the availability of other treat-\\nments, and cancer surgery has become more \\neffective, with less morbidity. In the first half of \\nthe 20th century, however, surgery was the only \\noption, and a minority of patients could be cured \\nby surgical removal of their tumors alone.\\nChemotherapy/uni0020or\\nsystemic/uni0020therapy\\nRadiation/uni0020therapy\\nSurgery\\nDiscovery of radium,\\n1898\\nTransplantable\\nrodent tumors,\\n1912\\nHead and neck cancer\\ncured by fractionated\\nradiotherapy,\\n1928\\nFolic acid antagonists used in leukemia,\\n1948\\nFisher hypothesis,\\n1968\\nGamma-knife radiosurgery,\\n1968\\nMultileaf collimator,\\n1980\\nIntensity-modulated\\nradiation therapy,\\n1988\\nAdjuvant chemotherapy\\nfor breast cancer,\\n1974\\nCure for testicular cancer,\\n1976\\nTargeting of aromatase enzyme,\\n1977\\nProof of principle:\\ntargeted therapy\\nwith imatinib for CML,\\n1996\\nFirst effective cancer\\nimmunotherapy with interleukin-2,\\n1985\\nLinear accelerator at Stanford,\\n1961\\nMethotrexate used in choriocarcinoma,\\n1957\\nProof of principle:\\ndrug cures for Hodgkin’s disease\\nand childhood leukemia,\\n1967\\nDiscovery of estrogen receptor,\\n1961\\nFirst monoclonal\\nantibody approved,\\n1997\\nDevelopment of\\nkinase inhibitors,\\n2005\\nBreast-conserving\\nsurgery,\\n2002\\nProton beam at Berkeley,\\n1954\\nCobalt teletherapy,\\n1950\\nDiscovery that prostate cancer\\nis hormone-dependent,\\n1945\\nNitrogen mustard used\\nin lymphomas,\\n1943\\nDiscovery of\\nroentgen rays,\\n1895\\nHalsted hypothesis\\nleads to radical\\nmastectomy and\\nen bloc resection,\\n1894\\nOophorectomy for\\nadvanced breast cancer,\\n1906\\nBreast-cancer\\nmortality\\nbegins to fall,\\n1991\\nIncidence\\nper 100,000\\n(age-adjusted)\\nMortality\\nper 100,000\\n(age-adjusted)\\n1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\\n400\\n200\\n511\\n215\\n473\\n178\\n444\\n151\\n5/18/2012\\nAUTHOR/uni0020PLEASE/uni0020NOTE:\\nFigure has been redrawn and type has been reset\\nPlease check carefully\\nAuthor\\nFig #\\nTitle\\nDE\\nME\\nArtist\\nCOLOR/uni0020/uni0020FIGURE\\nDraft 5\\nTwo Hundred Years of\\nCancer Research\\n1\\nPrince\\nWilliams\\nDeVi_ra1204479\\nBaden\\nFigure 1. Timeline of Pivotal Events in Cancer Treatment.\\nCML denotes chronic myeloid leukemia.\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(id='doc_3', metadata={}, page_content='T h e n e w  e ng l a n d  j o u r na l  o f m e dic i n e\\nn engl j med 366;23 nejm.org june 7, 20122210\\nThe era of radiation treatment began in 1895, \\nwhen Roentgen reported on his discovery of \\nx-rays,26 and accelerated in 1898 with the discov-\\nery of radium by Pierre and Marie Curie. 27 In \\n1928, it was shown that head and neck cancers \\ncould be cured by fractionated radiation treat-\\nments, a milestone in the field.28 The modern era \\nof radiation therapy began in 1950 with the intro-\\nduction of cobalt teletherapy. Since then, aided \\nby advances in computing, the field has been \\ndriven by advances in technology that have al-\\nlowed the therapeutic radiologist to deliver beam \\nenergy precisely to the tumor and to spare the \\nnormal tissue in the path of the radiation beam. \\nIncidence\\nper 100,000\\n(age-adjusted)\\nMortality\\nper 100,000\\n(age-adjusted)\\n1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\\n400\\n200\\n511\\n215\\n473\\n178\\n444\\n151\\nViruses/uni0020and/uni0020cancer\\nChemoprevention\\nTobacco/uni0020and/uni0020cancer\\nHypothesis that tobacco\\nis linked to lung cancer,\\n1912\\nTamoxifen discovered,\\n1967\\nHepatitis B discovered,\\n1967\\nTobacco advertising on radio and\\ntelevision banned in U.S.,\\n1970\\nFirst vaccine against hepatitis B,\\n1974\\nHepatitis linked to hepatoma,\\n1974\\nLink discovered between\\nHPV and cervical cancer,\\n1976\\nVaccine prevents\\nhepatitis and hepatoma,\\n1981\\nHPV vaccine developed,\\n1985\\nTamoxifen\\nprevention trials,\\n1989\\nFinasteride reduces\\nprostate-cancer\\nincidence,\\n2003\\nAspirin prevents\\ncolon cancer,\\n2003\\nProof of principle:\\nchemoprevention\\nworks,\\n1990\\nBCG prevents bladder cancer,\\n1991\\nAntiestrogen drugs\\nprevent DCIS,\\n1995\\nTamoxifen reduces\\nbreast-cancer incidence,\\n1998\\nFDA approves HPV\\nvaccine to prevent\\ncervical cancer,\\n2000\\nLung-cancer incidence\\nand mortality begin to fall,\\n1990–1991\\nExperimental evidence\\nlinks lung cancer to smoking,\\n1950\\nHPV discovered,\\n1907\\nSurgeon General’s report\\non risks of smoking,\\n1964\\nWarning labels on\\ncigarette packages,\\n1965\\n5/21/2012\\nAUTHOR/uni0020PLEASE/uni0020NOTE:\\nFigure has been redrawn and type has been reset\\nPlease check carefully\\nAuthor\\nFig #\\nTitle\\nDE\\nME\\nArtist\\nCOLOR/uni0020/uni0020FIGURE\\nDraft 6\\nTwo Hundred Years of\\nCancer Research\\n2\\nPrince\\nWilliams\\nDeVi_ra1204479\\nBaden\\nFigure 2. Timeline of Pivotal Events in Cancer Prevention.\\nBCG denotes bacille Calmette–Guérin, DCIS ductal carcinoma in situ, FDA Food and Drug Administration, and HPV human papillomavirus.\\nThe New England Journal of Medicine is produced by NEJM Group, a division of the Massachusetts Medical Society.\\nDownloaded from nejm.org on February 11, 2025. For personal use only. \\n No other uses without permission. Copyright © 2012 Massachusetts Medical Society. All rights reserved.'),\n",
       " Document(id='doc_37', metadata={}, page_content='Parhi, Bohra, El Biari, Pourya, and Unser\\nBenoit B. Mandelbrot and John W. Van Ness. Fractional Brownian motions, fractional\\nnoises and applications. SIAM Review, 10(4):422–437, 1968.\\nAlexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin\\nGhahramani. Gaussian process behaviour in wide deep neural networks. In International\\nConference on Learning Representations, 2018.\\nRobert A. Minlos. Generalized random processes and their extension in measure. Trudy\\nMoskovskogo Matematicheskogo Obshchestva, 8:497–518, 1959.\\nRadford M. Neal. Bayesian Learning for Neural Networks . Lecture Notes in Statistics.\\nSpringer New York, 1996.\\nRoman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolaﬁa,\\nJeﬀrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks\\nwith many channels are Gaussian processes. In International Conference on Learning\\nRepresentations, 2019.\\nGreg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view\\nof bounded norm inﬁnite width ReLU nets: The multivariate case. In International\\nConference on Learning Representations, 2020.\\nRahul Parhi and Robert D. Nowak. Banach space representer theorems for neural networks\\nand ridge splines. Journal of Machine Learning Research , 22(43):1–40, 2021.\\nRahul Parhi and Robert D. Nowak. What kinds of functions do deep neural networks learn?\\nInsights from variational spline theory. SIAM Journal on Mathematics of Data Science , 4\\n(2):464–489, 2022.\\nRahul Parhi and Robert D. Nowak. Near-minimax optimal estimation with shallow ReLU\\nneural networks. IEEE Transactions on Information Theory , 69(2):1125–1140, 2023a.\\nRahul Parhi and Robert D. Nowak. Deep learning meets sparse regularization: A signal\\nprocessing perspective. IEEE Signal Processing Magazine , 40(6):63–74, 2023b.\\nRahul Parhi and Michael Unser. Distributional extension and invertibility of the k-plane\\ntransform and its dual. SIAM Journal on Mathematical Analysis , 56(4):4662–4686, 2024.\\nRahul Parhi and Michael Unser. Function-space optimality of neural architectures with\\nmultivariate nonlinearities. SIAM Journal on Mathematics of Data Science , 7(1):110–135,\\n2025.\\nAlexander G. Ramm and Alexander I. Katsevich. The Radon transform and local tomography.\\nCRC Press, Boca Raton, FL, 1996.\\nWalter Rudin. Functional analysis. International Series in Pure and Applied Mathematics.\\nMcGraw-Hill, Inc., New York, second edition, 1991.\\nKen-Iti Sato. L´ evy Processes and Inﬁnitely Divisible Distributions. Cambridge Studies in\\nAdvanced Mathematics. Cambridge University Press, 1999.\\n30')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vector_store_from_client.as_retriever(search_type='similarity',\n",
    "    k=2\n",
    ")\n",
    "print(results)\n",
    "question = ' to a Gaussian process'\n",
    "results.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings.embed_query(\"test\")))  # Gibt die Anzahl der Dimensionswerte aus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_docs[30].metadata)  # Prüfen, welche Metadaten die Dokumente haben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection.count())  # Sollte die Anzahl der gespeicherten Dokumente anzeigen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.embed_query(\"test\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
