Body Text:
 JournalofMachineLearningResearch26(2025)1-31 Submitted5/24;Published1/25
Random ReLU Neural Networks as Non-Gaussian Processes
Rahul Parhi rahul@ucsd.edu
Department of Electrical and Computer Engineering
University of California, San Diego
La Jolla, CA 92093, USA
Pakshal Bohra pakshalbohra@gmail.com
Ayoub El Biari ayoubelbiari@gmail.com
Mehrsa Pourya mehrsa.pourya@epfl.ch
Michael Unser michael.unser@epfl.ch
Biomedical Imaging Group
E´cole polytechnique f´ed´erale de Lausanne
CH-1015 Lausanne, Switzerland
Editor: Mohammad Emtiyaz Khan
Abstract
We consider a large class of shallow neural networks with randomly initialized parameters
and rectified linear unit activation functions. We prove that these random neural networks
are well-defined non-Gaussian processes. As a by-product, we demonstrate that these
networks are solutions to stochastic differential equations driven by impulsive white noise
(combinations of random Dirac measures). These processes are parameterized by the law of
the weights and biases as well as the density of activation thresholds in each bounded region
of the input domain. We prove that these processes are isotropic and wide-sense self-similar
with Hurst exponent 3/2. We also derive a remarkably simple closed-form expression for
their autocovariance function. Our results are fundamentally different from prior work in
that we consider a non-asymptotic viewpoint: The number of neurons in each bounded
region of the input domain (i.e., the width) is itself a random variable with a Poisson law
with mean proportional to the density parameter. Finally, we show that, under suitable
hypotheses, as the expected width tends to infinity, these processes can converge in law not
only to Gaussian processes, but also to non-Gaussian processes depending on the law of
the weights. Our asymptotic results provide a new take on several classical results (wide
networks converge to Gaussian processes) as well as some new ones (wide networks can
converge to non-Gaussian processes).
Keywords: Gaussian processes, non-Gaussian processes, random initialization, random
neural networks, stochastic processes.
1. Introduction
A shallow (single-hidden-layer) neural network is a function of the form
N
(cid:88)
x (cid:55)→ v σ(wTx−b ), x ∈ Rd, (1)
k k k
k=1
(cid:13)c2025RahulParhi,PakshalBohra,AyoubElBiari,MehrsaPourya,andMichaelUnser.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsareprovided
athttp://jmlr.org/papers/v26/24-0737.html.
Parhi, Bohra, El Biari, Pourya, and Unser
where σ : R → R is the activation function, N is the width of the network, and, for
k = 1,...,N, v ∈ R and w ∈ Rd \{0} are the weights and b ∈ R are the biases of the
k k k
network. It is well-known that, as N → ∞, several such networks with i.i.d. random weights
and biases are equivalent to a Gaussian process (Neal, 1996). This result was extended to
deepneuralnetworkswithi.i.d.randomparametersbyLeeetal.(2018). Thiscorrespondence
enables exact Bayesian inference for regression using wide neural networks (Williams, 1996;
Lee et al., 2018).
Motivated by the tight link between wide neural networks and stochastic processes, we
study properties of shallow rectified linear unit (ReLU) neural networks with randomly
initialized parameters, henceforth referred to as random (ReLU) neural networks. We study
Poisson-type random functions of the form
(cid:88) (cid:104) (cid:105)
s (x) = v ReLU(wTx−b )+cTx+c , x ∈ Rd, (2)
ReLU k k k k 0,k
k∈Z
where ReLU(t) := t = max{0,t}, the v are drawn i.i.d. with respect to the law P and
+ k V
the (w ,b ) are drawn such that
k k
1. the activation thresholds1 are mutually independent;
2. in expectation, the number of thresholds that intersect a finite volume in Rd is a
constant (proportional to the product of a parameter λ > 0 and a property related to
the geometry of the volume); and
3. for every finite volume in Rd, the thresholds are i.i.d. uniformly in the volume.
The randomness that generates the (w ,b ) motivates the denomination Poisson as
k k
it mimics the randomness in the jumps found in a unit interval of a compound Poisson
process (Daley and Vere-Jones, 2007). The parameter λ > 0 plays the role of the rate
parameter of a compound Poisson process and controls the density of activation thresholds
in each finite volume. The correction terms (x (cid:55)→ cT kx+c 0,k) k∈Z that appear in the sum are
affine functions that ensure that the sum in (2) converges almost surely. This is equivalent to
imposing boundary conditions on s . These boundary conditions are crucial in proving
ReLU
that, under suitable hypotheses on P , s is a well-defined stochastic process. This is
V ReLU
one of the primary technical contributions of this paper. Similar correction terms/boundary
conditions appear in the definition of fractional Brownian motion (Mandelbrot and Van Ness,
1968) and L´evy processes (Sato, 1999; Jacob and Schilling, 2001).
By restricting our attention to compact subsets Ω ⊂ Rd, say, to the unit ball Bd = {x ∈
1
Rd : (cid:107)x(cid:107) ≤ 1}, we have that (see Section 4.1) the process (2) is realized by a random
2
Poisson sum of the form
N
s ReLU(cid:12) (cid:12) Bd(x) = w 0Tx+b
0+(cid:88)λ
v kReLU(w kTx−b k), (3)
1
k=1
where the width N is a Poisson random variable with mean λS, where S is proportional to
λ
the surface area of Bd, and wTx+b is an affine function. Thus, the form in the right-hand
1 0 0
1. Theactivationthreshold oftheneuronx(cid:55)→ReLU(wTx−b)isthehyperplaneH ={x∈Rd: wTx=b}.
w,b
2
Random ReLU Neural Networks as Non-Gaussian Processes
side of (3) is a finite-width neural network with random parameters (including the width).
The affine function x (cid:55)→ wTx+b is a skip connection in neural network parlance. As
0 0
λ → ∞, we have that the expected value of the width satisfies E[N ] → ∞. Therefore, this
λ
limiting scenario corresponds to the asymptotic (i.e., infinite-width) regime.
1.1 Contributions
The purpose of this paper is to study the properties of random neural networks as in (2)
and (3) for the class of admissible laws P (in the sense of Definition 5) which, for example,
V
includes the Gaussian law. As these networks are completely specified by the law P and
V
the rate parameter λ > 0, we let
s (·) ∼ RP(λ;P ) (4)
ReLU V
denote that s is generated according to the randomness described above, where RP
ReLU
stands for ReLU process. The main contributions of this paper are outlined below.
Random ReLU Networks as Stochastic Processes In Section 4, we prove that
s is a well-defined stochastic process. In doing so, we derive the so-called characteristic
ReLU
functional2 oftheprocess,whichprovidesuswithacompletecharacterizationofitsstatistical
distribution. Further, we show that s is the unique continuous piecewise linear (CPwL)
ReLU
solution to the stochastic differential equation (SDE)
L
T s = w s.t. ∂ms(0) = 0,|m| ≤ 1, (5)
ReLU
L
where = denotes equality in law and T = KR∆ is the whitening operator for ReLU neu-
ReLU
rons. The driving term w of the SDE is an impulsive white noise process which is constructed
from combinations of random Dirac measures. The boundary conditions ∂ms(0) = 0,
|m| ≤ 1, are crucial in guaranteeing the existence of solutions to this SDE. In the form
of the whitening operator, K is the filtering operator of computed tomography, R is the
Radon transform, and ∆ is the Laplacian (see Section 3 for a precise definition of these
operators). The operator T was proposed by Ongie et al. (2020) to study the capacity
ReLU
of bounded-norm infinite-width ReLU networks.
Properties of Random ReLU Networks In Section 5, we derive the first- and second-
order statistics of s . Specifically, we present a remarkably simple closed-form expression
ReLU
for its autocovariance function. With the help of these statistics and the characteristic
functional, we show that s is a non-Gaussian process. We then show that s is
ReLU ReLU
isotropic and wide-sense self-similar with Hurst exponent H = 3/2.
Asymptotic Results In Section 6, we show that in the infinite-width regime (λ → ∞),
s converges in law to a Gaussian process when P is a Gaussian law with a variance
ReLU V
that is inversely proportional to λ. On the other hand, when P is a symmetric α-stable
V
(SαS) law with α ∈ (1,2) and scaling parameter proportional to λ−1/α, s converges in
ReLU
law to a non-Gaussian process.
2. Thecharacteristicfunctionalofastochasticprocessisanalogoustothecharacteristicfunctionofarandom
variable. See Section 2 for a detailed discussion.
3
Parhi, Bohra, El Biari, Pourya, and Unser
1.2 Related Work
There is a large body of work that investigates the connections between neural networks with
random initialization and stochastic processes. Early work in this direction is due to Neal
(1996) who proved that wide limits of shallow neural networks with bounded activation
functions are Gaussian processes when the (w ,b ) are drawn i.i.d. with respect to any law
k k
and the v are drawn i.i.d. with respect to a law that has zero mean and finite variance.
k
More recently, it has been argued by many authors, with varying degrees of mathematical
rigor, that deep neural networks with i.i.d. random initialization are Gaussian processes in
wide limits (Lee et al., 2018; Matthews et al., 2018; Garriga-Alonso et al., 2019; Novak et al.,
2019; Yang, 2019; Dyer and Gur-Ari, 2020; Hanin, 2023).
Another line of work that is closely related to our setting is that of Yaida (2020), who
studies the stochastic processes realized by finite-width random neural networks and shows
that such processes are non-Gaussian. The results of this paper are complementary to
that of Yaida (2020) in that our finite-width networks as in (3) also correspond to non-
Gaussian processes. However, our work is fundamentally different as we use the framework
of generalized stochastic processes (see Section 2). This allows us to derive the characteristic
functional of the random neural network, which provides a complete description of its
statistical distribution (i.e., the law of the process). The characteristic functional also allows
us to easily study the limiting processes as the expected width E[N ] → ∞, which Yaida
λ
(2020) does not investigate.
In particular, we derive a novel and remarkably simple closed-form expression of the
autocovariance function of the ReLU processes. Another important distinction of our
asymptotic results compared to prior work on wide networks is that, in the asymptotic
regime (λ → ∞), the neural networks as in (2) and (3) can converge not only to Gaussian
processes, but also to non-Gaussian processes, depending on the specific choice of P . This
V
type of result was alluded to by Neal (1996) in the case of SαS initialization, although
theoretical arguments were not carried out. Thus, this paper is the first, to the best of our
knowledge, to carry out a rigorous investigation of the convergence of wide networks to
non-Gaussian processes.
2. Generalized Stochastic Processes
The mathematical framework used in this paper is based on the theory of generalized
stochastic processes (Itˆo, 1954; Gelfand, 1955; Gelfand and Vilenkin, 1964; Itˆo, 1984) as
opposed to the more common “time-series” approach to studying stochastic processes. In
this section, we present the relevant background on generalized stochastic processes. We also
refer the reader to the book of Unser and Tafti (2014) for further background. While this
theory relies on some rather heavy concepts from functional analysis, it allows for elegant
arguments to investigate the properties of the stochastic processes realized by the random
neural networks in (2) and (3).
Throughoutthispaper,wefixacompleteprobabilityspace(Ω,F,P). Beforeweintroduce
thistheory,wefirstrecallsomeresultsfromclassicalprobabilitytheory. Areal-valuedrandom
vector X is a measurable function from the probability space (Ω,F,P) to (Rd,B(Rd)), where
4
Random ReLU Neural Networks as Non-Gaussian Processes
B(Rd) denotes the Borel σ-algebra on Rd. The law of X is the pushforward measure
P (A) := (X P)(A) :=
P(cid:0) X−1(A)(cid:1)
= P({ω ∈ Ω : X(ω) ∈ A}) = P(X ∈ A), (6)
X (cid:93)
for all A ∈ B(Rd). Consequently, the characteristic function of X is the (conjugate) Fourier
transform of P , given by
X
P(cid:98)X(ξ) = E[eiXTξ], ξ ∈ Rd, (7)
where i2 = −1.
Generalizedstochasticprocessesarerandomvariablesthattakevaluesinthe(continuous)
dual of a nuclear space. In the remainder of this section, let N denote a nuclear space and
N(cid:48) denote its dual. If u ∈ N(cid:48) and ϕ ∈ N, we let (cid:104)u,ϕ(cid:105) denote the the duality pairing
N(cid:48)×N
of u and ϕ (i.e., the evaluation of u at ϕ). A prototypical example of a nuclear space is the
Schwartz space S(Rd) of smooth and rapidly decreasing test functions. Its dual S(cid:48)(Rd) is
the space of tempered generalized functions.3 In order to discuss random variables that take
values in the dual of a nuclear space, we must equip that space with a σ-algebra.
Definition 1 The cylindricalσ-algebraonN(cid:48), denotedbyB (N(cid:48)), istheσ-algebragenerated
c
by cylinders of the form {u ∈ N(cid:48): ((cid:104)u,ϕ (cid:105) ,...,(cid:104)u,ϕ (cid:105) ) ∈ A}, where N ∈ N\{0},
1 N(cid:48)×N N N(cid:48)×N
ϕ ,...,ϕ ∈ N, and A ∈ B(RN).
1 N
We remark that when N is not only nuclear, but also Fr´echet, such as S(Rd), the
cylindrical σ-algebra B (N(cid:48)) coincides with the Borel σ-algebra B(N(cid:48)) (see Fernique, 1967;
c
Itˆo, 1984).
Definition 2 A generalized stochastic process is a measurable mapping
s : (Ω,F,P) → (N(cid:48),B (N(cid:48))). (8)
c
The law of s is then the probability measure P := s P which is defined on B (N(cid:48)). The
s (cid:93) c
characteristic functional4 of s is the (conjugate) Fourier transform of P , given by
s
P(cid:98)s(ϕ) = E[ei(cid:104)s,ϕ(cid:105) N(cid:48)×N], ϕ ∈ N. (9)
Observe that this definition recovers the classical characteristic function for random
vectors that take values in Rd. Indeed, Rd is a nuclear space whose dual is Rd. Furthermore,
for any (x,ξ) ∈ Rd ×Rd, we have that (cid:104)x,ξ(cid:105) = xTξ. The characteristic functional
Rd×Rd
of a generalized stochastic process contains all statistical information of the process in
the same way that the characteristic function of a classical random variable contains all
statistical information of that random variable. Analogous to the finite-dimensional case,
the Bochner–Minlos theorem (see Minlos (1959)) says that a functional P(cid:98) : N → C is the
characteristic functional of a generalized stochastic process if and only if P(cid:98) is continuous,
positive definite, and satisfies P(cid:98)(0) = 1.
The attractive feature of the framework of generalized stochastic processes is that it
coversnotonlyclassicalstochasticprocesses, butalsoprocessesthatdonotadmitapointwise
interpretation such as white noise processes. For example, a generalized Gaussian process is
defined as follows.
3. This space is often referred to as the space of tempered distributions. We adopt the nomenclature of
tempered generalized functions in this paper so as to not cause confusion with probability distributions.
4. The characteristic functional of a generalized stochastic process was introduced by Kolmogorov (1935).
5
Parhi, Bohra, El Biari, Pourya, and Unser
Definition 3 A generalized stochastic process s that takes values in N(cid:48) is said to be
Gaussian if its characteristic functional is of the form
(cid:18) (cid:19)
1
P(cid:98)s(ϕ) = exp iµ s(ϕ)− Σ s(ϕ,ϕ) , (10)
2
where ϕ ∈ N, µ : N → R is the mean functional of the process, given by
s
µ (ϕ) = E[(cid:104)s,ϕ(cid:105) ], (11)
s N(cid:48)×N
and Σ : N ×N → R is the covariance functional of the process, given by
s
Σ (ϕ ,ϕ ) = E[((cid:104)s,ϕ (cid:105) −µ (ϕ ))((cid:104)s,ϕ (cid:105) −µ (ϕ ))]. (12)
s 1 2 1 N(cid:48)×N s 1 2 N(cid:48)×N s 2
The above definition is backwards compatible with classical Gaussian processes that are
space-indexed, as shown by Duttweiler and Kailath (1973), yet it also includes Gaussian
white noise (Hida and Ikeda, 1967).
With this machinery in hand, the primary technical contributions of this paper are (i) to
prove that, for any λ > 0 and any admissible P (in the sense of Definition 5), the random
V
neural network s ∼ RP(λ;P ) is a generalized stochastic process that takes values in
ReLU V
S(cid:48)(Rd), and (ii) to provide an explicit form of its (non-Gaussian) characteristic functional
(Section 4). With the help of the latter, we then derive various properties of the stochastic
process in the non-asymptotic regime (Section 5) and also study its asymptotic (λ → ∞)
behavior (Section 6) for various P .
V
3. The Radon Transform and Related Operators
Our characterization of random ReLU neural networks as stochastic processes hinges on
the whitening operator that appears in the SDE (5). This operator is based on the Radon
transform. In this section we introduce the relevant background on the Radon transform
and related operators. We refer the reader to the books of Ramm and Katsevich (1996) and
Helgason (2011) for an in depth treatment of the Radon transform. The Radon transform
of ϕ ∈ L1(Rd) is given by
(cid:90)
R{ϕ}(u,t) = ϕ(x)dx, (u,t) ∈ Sd−1×R, (13)
uTx=t
where dx denotes the integration against the (d−1)-dimensional Lebesgue measure on
the hyperplane {x ∈ Rd : uTx = t} and Sd−1 = {x ∈ Rd : (cid:107)x(cid:107) = 1} denotes the unit
2
sphere in Rd. Observe that the Radon transform of ϕ is a even since (u,t) and (−u,−t)
parametrize the same hyperplane. The adjoint operator, or dual Radon transform, applied
to φ ∈ L∞(Sd−1×R) is given by
(cid:90)
R∗{φ}(x) = φ(u,uTx)du, x ∈ R, (14)
Sd−1
where du denotes integration against the surface measure of Sd−1.
Let S(Sd−1×R) denote the Schwartz space of smooth and rapidly decreasing functions
on Sd−1×R. The range of the Radon transform on S(Rd), defined by SR := R(cid:0) S(Rd)(cid:1) , is
6
Random ReLU Neural Networks as Non-Gaussian Processes
a closed subspace of S(Sd−1×R) (Helgason, 2011, p. 60). Therefore, since S(Sd−1×R) is
nuclear, SR is also nuclear. The next proposition summarizes the continuity and invertibility
of the Radon transform.
Proposition 4 (Ludwig 1966; Gelfand et al. 1966; Helgason 2011) TheoperatorR
continuously maps S(Rd) into S(Sd−1×R). Moreover,
1 1
R∗KR = (−∆)d− 21 R∗R = R∗R(−∆)d− 21 = Id (15)
2(2π)d−1 2(2π)d−1
on S(Rd). The underlying operators5 are the Laplacian ∆ = (cid:80)d ∂2 and the filtering
n=1 xn
operator K = 2(2π1 )d−1(−∂ t2)d− 21 . Furthermore, R : S(Rd) → SR is a homeomorphism with
inverse R−1 = R∗K : SR → S(Rd).
4. Random ReLU Neural Networks as Stochastic Processes
In this section, we will prove that, for any λ > 0 and admissible P , the random neural
V
network s ∼ RP(λ;P ) is a well-defined stochastic process and derive its characteristic
V
functional on S(Rd). The admissibility conditions in Definition 5 are rather mild and most
choices of P (e.g., Gaussian, SαS for 1 < α ≤ 2, uniform, etc.) satisfy these hypotheses.
V
Definition 5 We say that the probability measure P is admissible if
V
1. it is a L´evy measure, i.e., it satisfies P ({0}) = 0 and (cid:82) min{1,v2}dP (v) < ∞,
V R V
and
2. it has a first absolute moment, i.e., if V ∼ P , then E[|V|] < ∞.
V
Given a ReLU neuron x (cid:55)→ ReLU(wTx−b) with w ∈ Rd\{0} and b ∈ R, we observe
that, thanks to the homogeneity of the ReLU,
ReLU(wTx−b) = (cid:107)w(cid:107) 2ReLU(w (cid:101)Tx−(cid:101)b), (16)
where w
(cid:101)
= w/(cid:107)w(cid:107)
2
and (cid:101)b = b/(cid:107)w(cid:107) 2. Therefore, the space of functions representable by
shallow ReLU neural networks with input weights constrained to be unit norm is the same
as the space of functions representable by shallow ReLU neural networks without constraints
on the weights (Parhi and Nowak, 2023b; Shenouda et al., 2024). To that end, we focus on
neurons of the form ReLU(wTx−b) with (w,b) ∈ Sd−1×R.
An important property of the operator T = KR∆ is that it “whitens” ReLU
ReLU
neurons. This result was implicitly proven by Ongie et al. (2020, Example 1), explicitly
proven by Parhi and Nowak (2021, Lemma 17), and then further investigated by, e.g.,
Bartolucci et al. (2023, Lemma 5.6) and Unser (2023, Corollary 11). The whitening property
is summarized in the following proposition.
Proposition 6 For any ReLU neuron
r (x) = ReLU(wTx−b) (17)
(w,b)
5. Non-integer powers of (−∆) and (−∂2) are understood in the Fourier domain.
t
7
Parhi, Bohra, El Biari, Pourya, and Unser
with (w,b) ∈ Sd−1×R, we have that
T r = δe , (18)
ReLU (w,b) (w,b)
where δe = (δ +δ )/2 denotes the even symmetrization of the Dirac measure δ supported
z z −z z
at z ∈ Sd−1×R.
The equality in (18) is understood in Me(Sd−1×R), the subspace of even finite (Radon)
measures on Sd−1×R. The arguments of the proof are based on duality. Indeed, observe
that the adjoint of T takes the form T∗ = ∆R∗K (since ∆ and K are self-adjoint).
ReLU ReLU
Furthermore, from Proposition 4 combined with the fact that ∆ : S(Rd) → S(Rd) is
continuous, we see that T∗
ReLU
: SR → S(Rd) is continuous. Therefore, by duality, T
ReLU
:
S(cid:48)(Rd) → S(cid:48) is continuous. Since r ∈ S(cid:48)(Rd), we have that T r is indeed
R (w,b) ReLU (w,b)
well-defined. Finally, Me(Sd−1×R) is continuously embedded in S(cid:48) and so any finite
R
measure in the range of KR can be concretely identified to have even symmetries (see Unser,
2023; Parhi and Unser, 2024, for a detailed discussion). These symmetries are evidenced
by the fact that the Radon transform of a “classical” function is necessarily even from the
integral form in (13).
Proposition 6 motivates us to study Radon-domain impulsive white noises that are
realized by Poisson-type random measures of the form
(cid:88)
w = v δe , (19)
Poi k (w ,b )
k k
k∈Z
i.i.d.
where v ∼ P for some admissible P (in the sense of Definition 5) and the collection of
k V V
random variables ((w ,b )) is a (homogeneous) Poisson point process6 on Sd−1×R with
k k k∈Z
rate parameter λ > 0. This point process satisfies the following properties.
1. The (w ,b ) are mutually independent.
k k
2. For any measurable subset Π ⊂ Sd−1×R, if we define the random variable
N = |{(w ,b ) : (w ,b ) ∈ Π}|, (20)
Π k k k k
then
(λ|Π|)n
P(N = n) = e−λ|Π|, (21)
Π
n!
where |Π| denotes the d-dimensional Hausdorff measure of Π. That is to say, N is a
Π
Poisson random variable with mean λ|Π|.
3. For any measurable subset B ⊂ Sd−1×R,
|B∩Π|
P((w ,b ) ∈ B|(w ,b ) ∈ Π) = . (22)
k k k k
|Π|
That is to say, if a point lies in Π, then its location will be uniformly distributed on Π.
6. For a general treatment of point processes, we refer the reader to the book of Daley and Vere-Jones
(2007).
8
Random ReLU Neural Networks as Non-Gaussian Processes
Next, if we suppose that there exists a “suitable” right-inverse T† of T that
ReLU ReLU
satisfies T T† = Id on S(cid:48) , then, intuitively, we could “invert” the result of Propo-
ReLU ReLU R
sition 6 to find that T† {w } is precisely a random ReLU neural network generated
ReLU Poi
in (2). It turns out that such a family of right-inverses exist. These inverses were first
proposed by Parhi and Nowak (2021, Lemma 21) in order to prove representer theorems
for neural networks. Some further properties of these operator were identified by Parhi and
Nowak (2022) and Unser (2023). We summarize the properties from Parhi and Nowak (2021,
Lemma 21) and Unser (2023, Theorem 13) that are required for our investigation in the
next proposition.
Proposition 7 For any ε > 0, there exists an operator T†ε defined on S(cid:48) such that, for
ReLU R
any w ∈ S(cid:48) ,
R
T T†ε w = w, (23)
ReLU ReLU
(cid:104) (cid:105)
(∂mgε)∗T†ε {w} (0) = 0,|m| ≤ 1, (24)
d ReLU
where gε : Rd → R is the multivariate Gaussian probability density function with mean 0 and
d
covariance matrix diag(ε,...,ε). The restriction of T†ε to the subspace Me(Sd−1×R) ⊂
ReLU
S(cid:48) continuously maps Me(Sd−1×R) to S(cid:48)(Rd). This mapping is realized by the integral
R
operator
(cid:12) (cid:90)
T†ε (cid:12) {µ}(x) = kε(u,t)dµ(u,t) (25)
ReLU(cid:12)
Me(Sd−1×R) Sd−1×R
x
whose kernel is given by
(uTx−t) (cid:18) |·|(cid:19) (cid:16) sgn(cid:17)
kε(u,t) = ReLU(uTx−t)− − gε∗ (t)+(uTx) gε∗ (t)
x 2 1 2 1 2
= ReLU(uTx−t)+uεTx+tε, (26)
0 0
where sgn is the signum function. Furthermore, there exists a universal constant C > 0 such
that
|kε(u,t)| ≤ C(1+(cid:107)x(cid:107) ) for all (u,t) ∈ Sd−1×R. (27)
x 2
Remark 8 The purpose of introducing the ε-indexed right-inverse operators is for a mollifi-
cation argument. We will eventually consider the limit ε → 0 (see the proof of Theorem 9 in
Appendix A).
With this inverse operator, we observe that, if w is an impulsive Poisson noise with
Poi
rate λ > 0 and weights drawn i.i.d. according to P (as in (19)), then, for any ε > 0,
V
(cid:40) (cid:41)
T†ε {w } = T†ε (cid:88) v δe
ReLU Poi ReLU k (w k,b k)
k∈Z
= (cid:88) v T†ε (cid:110) δe (cid:111)
k ReLU (w k,b k)
k∈Z
(cid:88) (cid:104) (cid:105)
= v ReLU(wT(·)−b )+cεT(·)+cε , (28)
k k k k 0,k
k∈Z
9
Parhi, Bohra, El Biari, Pourya, and Unser
where the second line is justified due to the uniform bound in (27), and the third line follows
from (25). Therefore, T†ε {w } is a random neural network as in (2) that satisfies the
ReLU Poi
boundary conditions in (24). We write
sε ∼ RPε(λ;P ) (29)
ReLU V
to denote that sε is such a random neural network. Furthermore, we let
ReLU
s ∼ RP(λ;P ), (30)
ReLU V
as introduced in Section 1, correspond to a random ReLU neural network that satisfies the
the limiting boundary conditions as ε → 0. That is to say, ∂ms (0) = 0, |m| ≤ 1, with
ReLU
the convention that the value of a piecewise constant function at a jump is the middle value.
Inthenexttheorem,weprovethattheserandomneuralnetworksarewell-definedstochas-
tic process that take values in S(cid:48)(Rd) and provide a complete statistical characterization
through their characteristic functional.
Theorem 9 For any ε > 0, λ > 0, and admissible P (in the sense of Definition 5), the
V
random neural network sε ∼ RPε(λ;P ) is a measurable mapping
ReLU V
sε : (Ω,F,P) → (S(cid:48)(Rd),B(S(cid:48)(Rd)) (31)
ReLU
with characteristic functional given by
(cid:18) (cid:90) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)sε (ϕ) = exp λ eivT† Rε e∗ LU{ϕ}(u,t)−1 dudtdP V(v) , ϕ ∈ S(Rd), (32)
ReLU R R Sd−1
where du denotes integration against the surface measure on Sd−1 and
(cid:90)
T†ε∗ : ϕ (cid:55)→ kε(·)ϕ(x)dx (33)
ReLU x
Rd
is the adjoint7 of T†ε . Furthermore, sε is the unique CPwL solution to the SDE
ReLU ReLU
L
T s = w s.t. [(∂mgε)∗s](0) = 0,|m| ≤ 1, (34)
ReLU Poi d
among all tempered weak solutions,8 where w is an impulsive Poisson noise with rate λ
Poi
and weights drawn i.i.d. according to P (as in (19)). All other tempered weak solutions to
V
the SDE take the form sε +h, where h is a harmonic polynomial of degree ≥ 2.9
ReLU
Finally, in the limiting scenario (ε → 0), we have that s ∼ RP(λ;P ) is a
ReLU V
measurable mapping (Ω,F,P) → (S(cid:48)(Rd),B(S(cid:48)(Rd)) with characteristic functional given by
(cid:18) (cid:90) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)sReLU(ϕ) = exp λ eivT† R∗ eLU{ϕ}(u,t)−1 dudtdP V(v) , ϕ ∈ S(Rd), (35)
R R Sd−1
7. Observe that T†ε∗ is well-defined on S(Rd) thanks to (27).
ReLU
8. A tempered weak solution to the SDE is any random tempered generalized function s(cid:63) ∈S(cid:48)(Rd) that
satisfies(34). Suchasolutionisreferredtoas“tempered”asitliesinS(cid:48)(Rd)and“weak”sincetheaction
of T on s(cid:63) is understood by duality.
ReLU
9. A harmonic polynomial h is a polynomial defined on Rd such that ∆h=0 on all of Rd.
10
Random ReLU Neural Networks as Non-Gaussian Processes
where T†∗ is the limiting operator as ε → 0 whose kernel is k := lim kε (pointwise
ReLU x ε→0 x
limit). This random neural network is the unique CPwL solution to the SDE
L
T s = w s.t. ∂ms(0) = 0,|m| ≤ 1. (36)
ReLU Poi
While the proof of the theorem is rather technical, the main ingredients can be divided
into two steps. The first is to prove that w is a well-defined stochastic process that
Poi
takes values in S(cid:48) . The second is to invoke the computation in (28) which linearly and
R
continuously transforms w into a random ReLU neural network. This transformation
Poi
allows us to derive the characteristic functional of s in terms of the characteristic
ReLU
functional of w . The proof appears in Appendix A.
Poi
4.1 Restrictions to Compact Domains
Recall from (3) that, for any λ > 0 and admissible P (in the sense of Definition 5), the
V
restriction of the random neural network s ∼ RP(λ;P ) to a compact domain, say, the
ReLU V
unit ball Bd is a random Poisson sum of the form
1
N
s ReLU(cid:12) (cid:12) Bd(x) = w 0Tx+b
0+(cid:88)λ
v kReLU(w kTx−b k), (37)
1
k=1
where the width N is a Poisson random variable. The reader can quickly check that the
λ
activation thresholds that intersect Bd correspond to Poisson points that lie in Sd−1×[−1,1].
1
Thus, the number of neurons N is a Poisson random variable with mean λ|Sd−1×[−1,1]|,
λ
which is λ multiplied by twice the surface area of the (d−1)-sphere. For general compact
domains Ω ⊂ Rd, following Parhi and Nowak (2023a, Section IV), we define
Z := {(w,b) ∈ Sd−1×R : {x : wTx = b}∩Ω (cid:54)= ∅}. (38)
Ω
(cid:12)
Then, the restriction s ReLU(cid:12) Ω is a random neural network whose width N λ,Ω is a Poisson
random variable with mean λ|Z |. As λ → ∞, we see that E[N ] → ∞. Therefore, the
Ω λ,Ω
asymptotic setting (λ → ∞) corresponds to the infinite-width regime.
5. Properties of Random ReLU Neural Networks
The characteristic functional (35) allows us to derive the first- and second-order statistics
of s as well as infer some of its other properties such as isotropy and wide-sense
ReLU
self-similarity. We summarize these properties in Theorem 10.
Theorem 10 For λ > 0 and admissible P (in the sense of Definition 5), let s ∼
V ReLU
RP(λ;P ). Then, the following statements hold.
V
1. The mean of s is given by
ReLU
(cid:90) (cid:90)
E[s (x)] = λE[V] k (u,t)dudt, (39)
ReLU x
R Sd−1
where k is defined in Theorem 9.
x
11
Parhi, Bohra, El Biari, Pourya, and Unser
2. If P has a finite second moment, then the autocovariance of s is given by
V ReLU
C (x,y) = E[(s (x)−E[s (x)])(s (y)−E[s (y)])]
sReLU ReLU ReLU ReLU ReLU
(cid:16) (cid:17)
= AλE[V2] (cid:107)x−y(cid:107)3−(cid:107)x(cid:107)3−(cid:107)y(cid:107)3+3xTy((cid:107)x(cid:107) +(cid:107)y(cid:107) ) , (40)
2 2 2 2 2
Γ(−3/2)
where A = and Γ(·) is Euler’s gamma function.
2d+3πd/2Γ((d+3)/2)
3. The process s is isotropic, i.e., it has the same probability law as its rotated version
ReLU
s (UT·), where U is any (d×d) rotation matrix.
ReLU
4. If P has zero mean and a finite second moment, then s is wide-sense self-similar
V ReLU
with Hurst exponent H = 3/2, i.e., it has the same second-order moments as its scaled
and renormalized version aHs (·/a) with a > 0.
ReLU
5. The process s is non-Gaussian.
ReLU
The proof of Theorem 10 can be found in Appendix B. We mention that the expression
of the autocovariance in (40) is remarkably simple. This is in contrast to prior works that
either (i) do not provide a closed-form expression (Lee et al., 2018; Yaida, 2020; Hanin,
2023), or (ii) provide a closed-form expression, but do not consider the ReLU activation
function (Williams, 1996). Furthermore, other than the work of Yaida (2020), these prior
works only consider the infinite-width regime.
6. Asymptotic Results
In the literature, there has been a lot of work on studying the wide limits of random neural
networks. Here, we present an asymptotic result for random ReLU neural networks with
i.i.d. weights drawn from an SαS law. The proof appears in Appendix C.
Theorem 11 For n ∈ N, let sn ∼ RP(λ = n;P ) with P being a symmetric α-
ReLU V V
stable law with scale parameter bn(−1/α),10 where α ∈ (1,2] and b ∈ R +, that is, P(cid:98)V(ξ) =
(cid:16) (cid:17)
|bξ|α
exp − . Then, we have
n
L
sn −−−→ s∞ , (41)
ReLU ReLU
n→∞
where s∞ is a well-defined generalized stochastic process that takes values in S(cid:48)(Rd) and
ReLU
has the characteristic functional
(cid:16) (cid:17)
P(cid:98)s∞ ReLU(ϕ) = exp −|b|α(cid:107)T† R∗ eLU{ϕ}(cid:107)α
Lα
, ϕ ∈ S(Rd). (42)
When α = 2, the SαS law is the Gaussian law. In this case, we can deduce that s∞
ReLU
is indeed a Gaussian process (see Appendix C). On the other hand, for α ∈ (1,2), we can
readily see that s∞ is non-Gaussian. Therefore, we have rigorously shown that wide limits
ReLU
of random neural networks are not necessarily Gaussian processes.
10. Similar to Neal (1996); Lee et al. (2018), the scale parameter inversely depends on the expected width of
the network.
12
Random ReLU Neural Networks as Non-Gaussian Processes
(a) λ = 1 (b) λ = 10 (c) λ = 100 (d) λ = 1000
Figure 1: P is Gaussian.
V
(a) λ = 1 (b) λ = 10 (c) λ = 100 (d) λ = 1000
Figure 2: P is symmetric (α = 1.25)-stable.
V
We illustrate these observations numerically in Figures 1 and 2, where we generated
random neural networks with P being Gaussian (α = 2) and non-Gaussian (α = 1.25),
V
respectively. There, we plot a top-down view of realizations of random neural networks for
λ ∈ {1,10,100,1000} where we color the linear regions with the magnitude of the gradient
of the function. Figure 1(d) looks like a two-dimensional Gaussian process, while Figure 2(d)
remains to look CPwL (non-Gaussian). Discussion on how we generated the random neural
networks numerically along with some additional figures appear in Appendix D.
7. Conclusion
We have investigated the statistical properties of random ReLU neural networks. We proved
that these networks are well-defined non-Gaussian processes in the non-asymptotic regime.
We showed that these processes are isotropic and wide-sense self-similar with Hurst exponent
3/2. Remarkably, the autocovariances of these processes have simple closed-form expressions.
Finally, we showed that, under suitable hypotheses, as the expected width tends to infinity,
these processes can converge in law not only to Gaussian processes, but also to non-Gaussian
processes depending on the law of the weights. These asymptotic results recover the classical
observation that wide networks converge to Gaussian processes as well as prove that wide
networks can converge to non-Gaussian processes. Although the presented investigation
only considered shallow random ReLU neural networks, an important direction of future
work would be to generalize our exact characterizations to deeper networks. To that end,
13
Parhi, Bohra, El Biari, Pourya, and Unser
the techniques developed by Zavatone-Veth and Pehlevan (2021) could provide a starting
point for that investigation.
Acknowledgments
The authors would like to thank the anonymous reviewers and the action editor for their
careful reading of the manuscript. This work was supported in part by the Swiss National
Science Foundation under Grant 200020 219356 / 1 and in part by the European Research
Council (ERC Project FunLearn) under Grant 101020573.
Appendix A. Proof of Theorem 9
As preparation before the proof of Theorem 9, we collect and prove some intermediary
results. To begin, we shall first prove that w is a well-defined stochastic process taking
Poi
values in S(cid:48) . Recall that w is an impulsive white noise that is realized by a Poisson-type
R Poi
random measure of the form
(cid:88)
w = v δe , (43)
Poi k (w ,b )
k k
k∈Z
i.i.d.
where v ∼ P for some admissible P (in the sense of Definition 5) and the collection of
k V V
random variables ((w ,b )) is a (homogeneous) Poisson point process on Sd−1×R with
k k k∈Z
rate parameter λ > 0. This point process satisfies the following properties.
1. The (w ,b ) are mutually independent.
k k
2. For any measurable subset Π ⊂ Sd−1×R, if we define the random variable
N = |{(w ,b ) : (w ,b ) ∈ Π}|, (44)
Π k k k k
then
(λ|Π|)n
P(N = n) = e−λ|Π|, (45)
Π
n!
where |Π| denotes the d-dimensional Hausdorff measure of Π. That is to say, N is a
Π
Poisson random variable with mean λ|Π|.
3. For any measurable subset B ⊂ Sd−1×R,
|B∩Π|
P((w ,b ) ∈ B|(w ,b ) ∈ Π) = . (46)
k k k k
|Π|
That is to say, if a point lies in Π, then its location will be uniformly distributed on Π.
Lemma 12 The random measure w can be viewed as a measurable mapping
Poi
w : (Ω,F,P) → (S(cid:48) ,B(S(cid:48) )) (47)
Poi R R
with characteristic functional given by
(cid:18) (cid:90) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)wPoi(ψ) = exp λ eivψ(u,t)−1 dudtdP V(v) , (48)
R R Sd−1
where du denotes integration against the surface measure on Sd−1.
14
Random ReLU Neural Networks as Non-Gaussian Processes
Proof Let D(Rd) ⊂ S(Rd) denote the space of infinitely differentiable and compactly
supported functions on Rd. Let DR := R(cid:0) D(Rd)(cid:1) denote the range of the Radon transform
on D(Rd). We now summarize the properties of DR that are relevant for our problem (cf.,
Ludwig, 1966). First, DR is a closed subspace of D(Sd−1×R), the nuclear space of infinitely
differentiable and compactly supported functions on Sd−1×R and is therefore nuclear.
Furthermore, DR is dense in SR, which implies that S R(cid:48) is continuously embedded in D(cid:48) R.
In particular, DR is the subspace of compactly supported functions in SR.
Next, we shall prove that w can be viewed as a measurable mapping
Poi
w : (Ω,F,P) → (D(cid:48) ,B (D(cid:48) )) (49)
Poi R c R
by computing its characteristic functional Q(cid:98)wPoi on DR.11 Let ψ ∈ DR and let
N = |{(w ,b ) : (w ,b ) ∈ suppψ}|. (50)
ψ k k k k
We have, by definition, that
N
ψ
(cid:88)
(cid:104)w ,ψ(cid:105) = v(cid:48)ψ(w(cid:48),b(cid:48)), (51)
Poi D(cid:48) R×DR k k k
k=1
where we use an appropriate relabeling of {v ,w ,b : (w ,b ) ∈ suppψ}. Therefore,
k k k k k
Q(cid:98)wPoi(ψ) = E[ei(cid:104)wPoi,ψ(cid:105) D(cid:48) R×DR]
= E[ei(cid:80)N k=ψ 1v k(cid:48)ψ(w k(cid:48),b(cid:48) k)]
  (cid:12) 
N ψ (cid:12)
= EE(cid:89) eiv k(cid:48)ψ(w k(cid:48),b(cid:48) k)(cid:12) (cid:12)N ψ
(cid:12)
k=1 (cid:12)
 
N
=
E(cid:89)ψ E(cid:104)
eiv k(cid:48)ψ(w k(cid:48),b(cid:48)
k)(cid:12)
(cid:12) (cid:12)N
ψ(cid:105)
 (52)
k=1
 
N
=
E(cid:89)ψ E(cid:104)
eiv k(cid:48)ψ(w k(cid:48),b(cid:48)
k)(cid:105)

k=1
 
N
=
E(cid:89)ψ E(cid:104) E(cid:104)
eiv k(cid:48)ψ(w k(cid:48),b(cid:48)
k)(cid:12)
(cid:12) (cid:12)v
k(cid:105)(cid:105)

k=1
 
=
E(cid:89)N ψ E(cid:20) 1 (cid:90)
eiv
k(cid:48)ψ(w,b)d(w,b)(cid:21)
 (53)
|suppψ|
suppψ
k=1
 
(cid:89)N ψ 1 (cid:90) (cid:90)
= E eivψ(w,b)d(w,b)dP V(v), (54)
|suppψ|
R suppψ
k=1
11. Note that DR is not Fr´echet so we use the cylindrical σ-algebra as opposed to the Borel σ-algebra.
15
Parhi, Bohra, El Biari, Pourya, and Unser
where (52) holds by the mutual independence of the (w ,b ) and (53) holds since the random
k k
variables
(w(cid:48),b(cid:48))|(w(cid:48),b(cid:48)) ∈ suppψ (55)
k k k k
are uniformly distributed on suppψ. Next, define the auxiliary functional
(cid:90) (cid:90)
M(ψ) = eivψ(w,b)d(w,b)dP (v). (56)
V
R suppψ
We have that
 
N
(cid:89)ψ M(ψ)
Q(cid:98)wPoi(ψ) = E |suppψ|
k=1
(cid:34) (cid:35)
(cid:18)
M(ψ)
(cid:19)N
ψ
= E
|suppψ|
(cid:88)∞ (cid:18)
M(ψ)
(cid:19)n(λ|suppψ|)n
= e−λ|suppψ| (57)
|suppψ| n!
n=0
(cid:88)∞ (λM(ψ))n
= e−λ|suppψ|
n!
n=0
= e−λ|suppψ|eλM(ψ) (58)
= exp(λ(M(ψ)−|suppψ|) (59)
(cid:18) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
= exp λ eivψ(z)−1 dzdP (v) , (60)
V
R Sd−1×R
where (57) holds since N is a Poisson random variable with mean λ|suppψ|, (58) holds by
ψ
the Taylor series expansion of t (cid:55)→ et, (59) holds since |suppψ| = (cid:82) 1dz, and (60) holds
suppψ
since z (cid:55)→ eivψ(z)−1 vanishes outside suppψ. At this point, we remark that, since P is a
V
L´evy measure (Definition 5), it is well-known that the form of (60) is continuous, positive
definite, and satisfies Q(cid:98)wPoi(0) = 1 (see, e.g., Gelfand and Vilenkin, 1964, Theorem 2, p. 275).
This implies that w is indeed a generalized stochastic process that takes values in D(cid:48) .
Poi R
To prove the lemma, it remains to extend the domain of Q(cid:98)wPoi to SR. To that end, let
(cid:18) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)wPoi(ψ) = exp λ eivψ(z)−1 dzdP V(v) , ψ ∈ SR. (61)
R Sd−1×R
We now invoke an adaption of Fageot et al. (2014, Theorem 3) which investigates impulsive
white noise defined on Rd as a special case. Their theorem implies that, thanks to the
admissibility conditions on P (Definition 5), the probability measures Q and P are
V wPoi wPoi
compatible on B(S(cid:48) ) = B (S(cid:48) ) ⊂ B (D(cid:48) ) in the sense that
R c R c R
Q (B) = P (B), for all B ∈ B(S(cid:48) ) (62)
wPoi wPoi R
and Q (D(cid:48) \S(cid:48) ) = 0, which proves the lemma.
wPoi R R
16
Random ReLU Neural Networks as Non-Gaussian Processes
Let S (Rd) := ∆(cid:0) S(Rd)(cid:1) denote the range of the Laplacian operator on S(Rd). This is a
∆
closed subspace of S(Rd). Observe that its dual S(cid:48) (Rd) can be identified with the quotient
∆
space S(cid:48)(Rd)/N , where
∆
N = {f ∈ S(cid:48)(Rd) : ∆f = 0 ⇔ (cid:104)f,φ(cid:105) = 0 for all φ ∈ S (Rd)}. (63)
∆ S(cid:48)(Rd)×S(Rd) ∆
is the null space of the Laplacian operator. It is well-known that N is infinite-dimensional
∆
and that its members are necessarily polynomials, the so-called harmonic polynomials.
Therefore, the members of S(cid:48) (Rd) are actually equivalence classes of the form
∆
[f] = {f +h : h ∈ N } ∈ S(cid:48) (Rd), (64)
∆ ∆
where f ∈ S(cid:48)(Rd). With this notation, we now prove Theorem 9.
Proof [Proof of Theorem 9] Recall that T = KR∆ and so T∗ = ∆R∗K. Observe
ReLU ReLU
that, by Proposition 4,
T∗
ReLU
: SR → S ∆(Rd) (65)
is a continuous bijection, where we equip the closed subspaces SR ⊂ S(Sd−1×R) and
S (Rd) ⊂ S(Rd) with the subspace topology from their respective parent Fr´echet spaces.
∆
By the open mapping theorem for Fr´echet spaces (see, e.g., Rudin, 1991, Theorem 2.11),
there exists a continuous inverse operator
T∗ R−
eLU
: S ∆(Rd) → SR (66)
with the properties that that T∗ ReLUT∗ R−
eLU
= Id on S ∆(Rd) and T∗ R− eLUT∗
ReLU
= Id on SR.
Therefore, by duality, we have the continuous bijections
T
ReLU
: S ∆(cid:48) (Rd) → SR
T− : S(cid:48) → S(cid:48) (Rd), (67)
ReLU R ∆
where we recall that S(cid:48) (Rd) ∼ = S(cid:48)(Rd)/N .
∆ ∆
Next, we note that the operator
(cid:90)
T†ε∗ : ϕ (cid:55)→ kε(·)ϕ(x)dx (68)
ReLU x
Rd
specified in (33) continuously maps S ∆(Rd) → SR (cf., Parhi and Unser, 2025, Equa-
tion (A.3)). Observe that, by Proposition 7, its extension by duality T†ε : S(cid:48) → S(cid:48) (Rd)
ReLU R ∆
coincides with T− . In particular, T†ε imposes the boundary conditions from (24) on
ReLU ReLU
the affine component of the harmonic polynomials in the equivalence classes in S(cid:48) (Rd). Said
∆
differently, the range space T†ε (cid:0) S(cid:48) (cid:1) is the closed subspace of S(cid:48) (Rd) whose equivalence
ReLU R ∆
class members [s] ∈ S(cid:48) (Rd) additionally satisfy
∆
[(∂mgε)∗s ](0) = 0,|m| ≤ 1 (69)
d 0
for all s ∈ [s]. Therefore, we can rewrite the SDE (34) as
0
s L = T†ε w , (70)
ReLU Poi
17
Parhi, Bohra, El Biari, Pourya, and Unser
where the equality is understood in S(cid:48) (Rd), i.e.,
∆
(cid:104)s,φ(cid:105) L = (cid:104)T†ε w ,φ(cid:105) = (cid:104)w ,T†ε∗ φ(cid:105) , (71)
S ∆(cid:48) (Rd)×S∆(Rd) ReLU Poi S ∆(cid:48) (Rd)×S∆(Rd) Poi ReLU S R(cid:48) ×SR
for all φ ∈ S (Rd). The above equality implies that the characteristic functional of any
∆
solution s to (70) (and, subsequently, the original SDE (34)) takes the form
P(cid:98)s(φ) = P(cid:98)
T† Rε
eLUwPoi(φ) = P(cid:98)wPoi(T† Rε e∗ LUφ). (72)
This characteristic functional is well-defined for any φ ∈ S ∆(Rd) since T† Rε e∗ LUφ ∈ SR, which
ensures that the right-hand side is well-defined by Lemma 12.
Since sε := T†ε w via the computation in (28), we see that sε is one member
ReLU ReLU Poi ReLU
in an equivalence class in S(cid:48)(Rd)/N . In particular, this implies that s ∈ S(cid:48)(Rd) and
∆ ReLU
that the equivalence class [sε ] = {sε +h: h ∈ N } is a well-defined stochastic process
ReLU ReLU ∆
that takes values in S(cid:48) (Rd) ∼ = S(cid:48)(Rd)/N whose characteristic functional on S is given
∆ ∆ ∆
by (72). Equivalently stated, the full set of tempered weak solutions of the SDE (34) has
members that necessarily take the form sε +h, where h ∈ N is a harmonic polynomial
ReLU ∆
of degree ≥ 2 (since boundary conditions of the SDE, imposed by T†ε , force the affine
ReLU
component of all solutions to be the same). Consequently, from these boundary conditions,
we readily see that the only CPwL solution to the SDE is sε .
ReLU
To complete the proof we need to derive the form of the characteristic functional of
sε on the larger space S(Rd) ⊃ S (Rd). For any ϕ ∈ S(Rd), we have that
ReLU ∆
(cid:104)sε ,ϕ(cid:105) L = (cid:104)T†ε w ,ϕ(cid:105) (73)
ReLU S(cid:48)(Rd)×S(Rd) ReLU Poi S(cid:48)(Rd)×S(Rd)
From the expression of the kernel (u,t) (cid:55)→ kε(u,t) in (26) we see that (i) it is continuous
x
in the variables (u,t) ∈ Sd−1×R and (ii) it decays faster than any polynomial in the
t-variable. Therefore, for every ϕ ∈ S(Rd), the function T†ε∗ {ϕ} is a continuous function
ReLU
in (u,t) ∈ Sd−1×R that decays faster than any polynomial in the t-variable. In particular,
this ensures that, for any 1 ≤ p ≤ ∞, the map
T†ε∗ : S(Rd) → Lp(Sd−1×R) (74)
ReLU
is continuous.
The right-hand side of (73) is, by definition, the integration of T†ε∗ {ϕ} against the
ReLU
locally finite Radon measure w , i.e., for any ϕ ∈ S(Rd) we have that
Poi
(cid:32) (cid:33)
(cid:90)
(cid:104)sε ,ϕ(cid:105) L = T†ε∗ {ϕ}d (cid:88) v δe
ReLU S(cid:48)(Rd)×S(Rd)
Sd−1×R
ReLU
k∈Z
k (w k,b k)
(cid:90)
= (cid:88) v T†ε∗ {ϕ}dδe
k∈Z
k
Sd−1×R
ReLU (w k,b k)
= (cid:88) v T†ε∗ {ϕ}(w ,b ), (75)
k ReLU k k
k∈Z
18
Random ReLU Neural Networks as Non-Gaussian Processes
where interchanging of the integral and sum in the second line is well-defined due to the
regularity of T†ε∗ {ϕ} and the third line uses the fact that the range of T†ε∗ on S(Rd) is
ReLU ReLU
a space of even functions. This proves that
P(cid:98)sε (ϕ)
ReLU
(cid:18) (cid:90) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
= P(cid:98)wPoi(T† Rε e∗ LUϕ) = exp λ eivT† Rε e∗ LU{ϕ}(u,t)−1 dudtdP V(v) , (76)
R R Sd−1
for all ϕ ∈ S(Rd), where the last equality comes from Lemma 12. We shall now ver-
ify that P(cid:98)sε is a valid characteristic functional on S(Rd). This then implies that
ReLU
sε : (Ω,F,P) → (S(cid:48)(Rd),B(S(cid:48)(Rd)) is a measurable mapping and therefore a well-
ReLU
defined stochastic process.
Observe that the second admissibility condition on P (Item 2 in Definition 5) states
V
that P has a finite absolute moment. This is a sufficient condition to ensure that this
V
characteristic functional (76) is well-defined for every ϕ ∈ S(Rd). Indeed, we have that
(cid:90) (cid:16) (cid:17)
Ψ(ξ) := λ eivξ −1 dP (v) ≤ λ|ξ|E[|V|], (77)
V
R
where V ∼ P (cf., Unser et al., 2014, p. 1952). Therefore,
V
(cid:18) (cid:90) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)sε (ϕ) = exp λ eivT† Rε e∗ LU{ϕ}(u,t)−1 dudtdP V(v)
ReLU R R Sd−1
(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)
= exp Ψ T†ε∗ {ϕ} dz
ReLU
Sd−1×R
(cid:16) (cid:17)
≤ exp C(cid:107)T†ε∗ {ϕ}(cid:107)
ReLU L1
< ∞, (78)
for any ϕ ∈ S(Rd), where C = λE[|V|] < ∞, where in the last line we used (74) with
p = 1. Since T†ε∗ : S(Rd) → L1(Sd−1×R) linearly and continuously, Proposition 3.1 of
ReLU
Fageot and Unser (2019) then guarantees that P(cid:98)sε is continuous, positive definite, and
ReLU
satisfies P(cid:98)sε (0) = 1. Therefore, the Bochner–Minlos theorem ensures that P(cid:98)sε is the
ReLU ReLU
characteristic functional of the well-defined stochastic process sε .
ReLU
In the limiting scenario of ε → 0, we see that the random neural network s ∼
ReLU
RP(λ;P ) is a measurable mapping (Ω,F,P) → (S(cid:48)(Rd),B(S(cid:48)(Rd)) whose characteristic
V
functional is
(cid:18) (cid:90) (cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)sReLU(ϕ) = exp λ eivT† R∗ eLU{ϕ}(u,t)−1 dudtdP V(v) , ϕ ∈ S(Rd), (79)
R R Sd−1
where we observe that this limiting characteristic functional remains to be valid in the sense
of the Bochner–Minlos theorem since the property that, for any 1 ≤ p ≤ ∞, the map
T†∗ : S(Rd) → Lp(Sd−1×R) (80)
ReLU
19
Parhi, Bohra, El Biari, Pourya, and Unser
is continuous, remains to be true since k = lim kε (pointwise limt) is compactly sup-
x ε→0 x
ported. Consequently, s is the unique CPwL solution to SDE (36).12
ReLU
Appendix B. Proof of Theorem 10
Proof
1. Thanks to the moment generating properties of the characteristic functional (Gelfand
and Vilenkin, 1964), the mean functional of s can be obtained as
ReLU
d (cid:12)
µ sReLU(ϕ) = E[(cid:104)s ReLU,ϕ(cid:105) S(cid:48)(Rd)×S(Rd)] = (−i) dξP(cid:98)sReLU(ξϕ)(cid:12)
(cid:12)
ξ=0, (81)
where ϕ ∈ S(Rd). First, observe that the characteristic functional of s can be
ReLU
written as
(cid:18)(cid:90)
(cid:16) (cid:17)
(cid:19)
P(cid:98)sReLU(ϕ) = exp Ψ T† R∗ eLU{ϕ}(z) dz (82)
Sd−1×R
with Ψ defined as in (77). Here, note that we have
(cid:90)
Ψ(cid:48)(x) = iλ veivxdP (v). (83)
V
R
Let us denote h(z) = T†∗ {ϕ}(z). By applying the chain rule, we can write
ReLU
(cid:18)(cid:90) (cid:19) (cid:90)
d
dξP(cid:98)sReLU(ξϕ) = exp Ψ(ξh(z))dz · Ψ(cid:48)(ξh(z))h(z)dz. (84)
R×Sd−1 R×Sd−1
On setting ξ = 0, we get
d (cid:12) (cid:90)
dξP(cid:98)sReLU(ξϕ)(cid:12)
(cid:12)
ξ=0
= Ψ(cid:48)(0) R×Sd−1h(z)dz (85)
as Ψ(0) = 0. Therefore, the mean functional is
d (cid:12)
µ sReLU(ϕ) = (−i) dξP(cid:98)sReLU(ξϕ)(cid:12)
(cid:12)
ξ=0
(cid:90)
= λE[V] T†∗ {ϕ}(z)dz
ReLU
Sd−1×R
(cid:90) (cid:90) (cid:90)
= λE[V] k (u,t)ϕ(x)dudtdx. (86)
x
Rd R Sd−1
Next, we establish a link between the mean functional of s and the quantity
ReLU
E[s (x)]. Since s has a pointwise interpretation, we have
ReLU ReLU
(cid:90)
(cid:104)s ,ϕ(cid:105) = s (x)ϕ(x)dx. (87)
ReLU S(cid:48)(Rd)×S(Rd) ReLU
Rd
12. The mollifier argument is necessary in order to make sense of the boundary conditions (36) for elements
of S(cid:48)(Rd) that are not regular enough for the derivatives to exist.
20
Random ReLU Neural Networks as Non-Gaussian Processes
Consequently, the mean functional can also be computed as
(cid:20)(cid:90) (cid:21)
µ (ϕ) = E[(cid:104)s ,ϕ(cid:105) ] = E s (x)ϕ(x)dx
sReLU ReLU S(cid:48)(Rd)×S(Rd) ReLU
Rd
(cid:90)
= E[s (x)]ϕ(x)dx, (88)
ReLU
Rd
where exchanging the expectation and the integral is justified by the Fubini–Tonelli
theorem since the integrand in (86) is absolutely integrable by (80) with p = 1. On
comparing (88) with (86), we see that
(cid:90) (cid:90)
E[s (x)] = λE[V] k (u,t)dudt. (89)
ReLU x
R Sd−1
2. The covariance functional of s is given by
ReLU
Σ (ϕ ,ϕ )
sReLU 1 2
(cid:104)(cid:16) (cid:17)(cid:16) (cid:17)(cid:105)
= E (cid:104)s ,ϕ (cid:105) −µ (ϕ ) (cid:104)s ,ϕ (cid:105) −µ (ϕ )
ReLU 1 S(cid:48)(Rd)×S(Rd) sReLU 1 ReLU 2 S(cid:48)(Rd)×S(Rd) sReLU 2
= R (ϕ ,ϕ )−µ (ϕ )µ (ϕ ), (90)
sReLU 1 2 sReLU 1 sReLU 2
where ϕ ,ϕ ∈ S(Rd) and
1 2
(cid:104) (cid:105)
R (ϕ ,ϕ ) = E (cid:104)s ,ϕ (cid:105) (cid:104)s ,ϕ (cid:105) (91)
sReLU 1 2 ReLU 1 S(cid:48)(Rd)×S(Rd) ReLU 2 S(cid:48)(Rd)×S(Rd)
is the correlation functional of s . This quantity can be computed from its
ReLU
characteristic functional (cf., Gelfand and Vilenkin, 1964) as
d2 (cid:12)
R sReLU(ϕ 1,ϕ 2) = −
dξ 1dξ
2P(cid:98)sReLU(ξ 1ϕ 1+ξ 2ϕ 2)(cid:12)
(cid:12)
ξ1=0,ξ2=0. (92)
Let us first define the quantity f(ξ ,ξ ) as
1 2
(cid:90) (cid:16) (cid:17)
f(ξ ,ξ ) = Ψ T†∗ {ξ ϕ +ξ ϕ }(z) dz
1 2 ReLU 1 1 2 2
Sd−1×R
(cid:90) (cid:16) (cid:17)
= Ψ ξ T†∗ {ϕ }(z)+ξ T†∗ {ϕ }(z) dz. (93)
1 ReLU 1 2 ReLU 2
Sd−1×R
Further,letusdenoteh (z) = T†∗ {ϕ }(z)andh (z) = T†∗ {ϕ }(z). Byapplying
1 ReLU 1 2 ReLU 2
the chain rule twice, we write
d2
dξ dξ
P(cid:98)sReLU(ξ 1ϕ 1+ξ 2ϕ 2)
1 2
(cid:18) d2 d d (cid:19)
= exp(f(ξ ,ξ )) f(ξ ,ξ )+ f(ξ ,ξ ) f(ξ ,ξ ) ,
1 2 1 2 1 2 1 2
dξ dξ dξ dξ
1 2 1 2
(94)
21
Parhi, Bohra, El Biari, Pourya, and Unser
where
(cid:90)
d
f(ξ ,ξ ) = Ψ(cid:48)(ξ h (z)+ξ h (z))h (z)dz (95)
1 2 1 1 2 2 k
dξ
k Sd−1×R
and
d2 (cid:90)
f(ξ ,ξ ) = Ψ(cid:48)(cid:48)(ξ h (z)+ξ h (z))h (z)h (z)dz. (96)
1 2 1 1 2 2 1 2
dξ dξ
1 2 Sd−1×R
On setting ξ = 0 and ξ = 0, we get
1 2
d2 (cid:12) (cid:90)
dξ 1dξ
2P(cid:98)sReLU(ξ 1ϕ 1+ξ 2ϕ 2)(cid:12)
(cid:12) ξ1=0,ξ2=0
= Ψ(cid:48)(cid:48)(0) Sd−1×Rh 1(z)h 2(z)dz
(cid:18) (cid:90) (cid:19)(cid:18) (cid:90) (cid:19)
+ Ψ(cid:48)(0) h (z)dz Ψ(cid:48)(0) h (z)dz .
1 2
Sd−1×R Sd−1×R
(97)
Note that we have
(cid:90)
Ψ(cid:48)(cid:48)(x) = −λ v2eivxdP (v). (98)
V
R
Thus, the correlation functional is of the form
d2 (cid:12)
R sReLU(ϕ 1,ϕ 2) = −
dξ 1dξ
2P(cid:98)sReLU(ξ 1ϕ 1+ξ 2ϕ 2)(cid:12)
(cid:12) ξ1=0,ξ2=0
(cid:18)(cid:90) (cid:19)
= λE[V2] T†∗ {ϕ }(z)T†∗ {ϕ }(z)dz
ReLU 1 ReLU 2
Sd−1×R
+µ (ϕ )µ (ϕ ). (99)
sReLU 1 sReLU 2
Consequently, the covariance functional is given by
Σ (ϕ ,ϕ )
sReLU 1 2
(cid:90)
= λE[V2] T†∗ {ϕ }(z)T†∗ {ϕ }(z)dz
ReLU 1 ReLU 2
Sd−1×R
(cid:90) (cid:90) (cid:90) (cid:90)
= λE[V2] k (u,t)k (u,t)ϕ (x)ϕ (y)dudtdxdy. (100)
x y 1 2
Rd Rd R Sd−1
Next, we derive the connection between the covariance functional of s and the
ReLU
autocovariance E[(s (x)−E[s (x)])(s (y)−E[s (y)])]. Since s
ReLU ReLU ReLU ReLU ReLU
has a pointwise interpretation, the covariance functional can also be computed as
Σ (ϕ ,ϕ )
sReLU 1 2
(cid:104)(cid:16) (cid:17)(cid:16) (cid:17)(cid:105)
= E (cid:104)s ,ϕ (cid:105) −µ (ϕ ) (cid:104)s ,ϕ (cid:105) −µ (ϕ )
ReLU 1 S(cid:48)(Rd)×S(Rd) sReLU 1 ReLU 2 S(cid:48)(Rd)×S(Rd) sReLU 2
(cid:20)(cid:18)(cid:90) (cid:19)(cid:18)(cid:90) (cid:19)(cid:21)
= E (s (x)−E[s (x)])ϕ (x)dx (s (y)−E[s (y)])ϕ (y)dy
ReLU ReLU 1 ReLU ReLU 2
Rd Rd
(cid:90) (cid:90)
= E[(s (x)−E[s (x)])(s (y)−E[s (y)])]ϕ (x)ϕ (y)dxdy,
ReLU ReLU ReLU ReLU 1 2
Rd Rd
(101)
22
Random ReLU Neural Networks as Non-Gaussian Processes
where exchanging the expectation and the integral is justified by the Fubini–Tonelli
theorem since the integrand in (100) is absolutely integrable from (80) with p = 2. If
we compare (101) with (100), we see that
C (x,y) = E[(s (x)−E[s (x)])(s (y)−E[s (y)])]
sReLU ReLU ReLU ReLU ReLU
(cid:90) (cid:90)
= λE[V2] k (u,t)k (u,t)dudt. (102)
x y
R Sd−1
To simplify the double integral in (102), we first observe that, by definition,
(cid:90) (cid:90)
(x,y) (cid:55)→ k (u,t)k (u,t)dudt, (x,y) ∈ Rd×Rd, (103)
x y
R Sd−1
is the (Schwartz) kernel of the operator T† T†∗ . Next, we note that the right-
ReLU ReLU
inverse operator can be equivalently specified as the composition of operators T† =
ReLU
(Id−P)∆−1R∗ (cf. Unser, 2023, Equation (57)), where ∆−1 is the Riesz potential of
order 2, i.e., it is the Fourier multiplier
((cid:98)−∆)−γ 2f(ω) = (cid:107)ω(cid:107)− 2γf(cid:98)(ω), ω ∈ Rd, (104)
with γ = 2, and P is the projection onto the space of affine functions adapted to the
boundary conditions of the SDE (36). Concretely,
d
(cid:88)
P{f} = (cid:104)φ ,f(cid:105)p , (105)
n n
n=0
where p (x) = 1 and p (x) = x , n = 1,...,d is a basis for the space of affine function
0 n n
on Rd and φ = δ (Dirac distribution) and φ = −δ(cid:48) := −∂ δ, n = 1,...,d, is the
0 n n xn
linear functional that evaluates the partial derivative in the nth component at 0, i.e.,
(cid:104)φ ,f(cid:105) = ∂ f(0), n = 1,...,d. Consequently, the adjoint projector is given by
n xn
d
(cid:88)
P∗{f} = (cid:104)p ,f(cid:105)φ . (106)
n n
n=0
With this notation, we have that
T† T†∗ = (Id−P)∆−1R∗R∆−1(Id−P∗)
ReLU ReLU
= (Id−P)∆−1(−∆)−d− 21 ∆−1(Id−P∗)
= (Id−P)(−∆)−d+ 23 (Id−P∗), (107)
wherethesecondlinefollowsfromProposition4. The(Schwartz)kerneloftheoperator
(generalized impulse response) can be identified with (x,y) (cid:55)→ T† T†∗ {δ(· −
ReLU ReLU
y)}(x). We have that
d d
(cid:88) (cid:88)
(Id−P∗){δ(·−y)} = δ(·−y)− (cid:104)p ,δ(·−y)(cid:105)φ = δ(·−y)−δ+ y δ(cid:48), (108)
k n 1 n
k=0 n=1
23
Parhi, Bohra, El Biari, Pourya, and Unser
whereweusedthepropertythattheshiftedDiracdistributionisthesamplingfunctional.
Next,
(cid:32) d (cid:33)
(−∆)−d+ 23 (Id−P∗){δ(· −y)}(x) = A (cid:107)x−y(cid:107)3 2−(cid:107)x(cid:107)3 2+(cid:88) y 1(3x n(cid:107)x(cid:107) 2)
n=1
(cid:16) (cid:17)
= A (cid:107)x−y(cid:107)3−(cid:107)x(cid:107)3+3xTy(cid:107)x(cid:107) , (109)
2 2 2
where A = Γ(−3/2) and we used the fact that x (cid:55)→ A(cid:107)x(cid:107)3 is the radially
2d+3πd/2Γ((d+3)/2) 2
d+3
symmetric Green’s function of (−∆) 2 (Gelfand and Shilov, 1964). Finally,
(Id−P)(−∆)−d+ 23 (Id−P∗){δ(· −y)}(x)
(cid:32) d (cid:33)
(cid:16) (cid:17) (cid:88)
= A (cid:107)x−y(cid:107)3−(cid:107)x(cid:107)3+3xTy(cid:107)x(cid:107) −A (cid:107)y(cid:107)3− 3y (cid:107)y(cid:107) x
2 2 2 2 n 2 n
n=1
(cid:16) (cid:17)
= A (cid:107)x−y(cid:107)3−(cid:107)x(cid:107)3−(cid:107)y(cid:107)3+3xTy((cid:107)x(cid:107) +(cid:107)y(cid:107) ) . (110)
2 2 2 2 2
Putting everything together, we find that the autocovariance takes the form
(cid:16) (cid:17)
C (x,y) = λAE[V2] (cid:107)x−y(cid:107)3−(cid:107)x(cid:107)3−(cid:107)y(cid:107)3+3xTy((cid:107)x(cid:107) +(cid:107)y(cid:107) ) . (111)
sReLU 2 2 2 2 2
3. In order to show that s is isotropic, we will show that its characteristic functional
ReLU
satisfies
P(cid:98)sReLU(ϕ) = P(cid:98)sReLU(ϕ(U·)) (112)
for any ϕ ∈ S(Rd) and any (d×d) rotation matrix U. First, we note that the kernel
of T† can be written as
ReLU
(uTx−t) |t| sgn(t)
k (u,t) = ReLU(uTx−t)− − +(uTx)
x
2 2 2
= ReLU(uTx−t)+(uTx)h (t)+h (t), (113)
1 2
sgn(t)−1 t−|t|
where h (t) = and h (t) = . Let U be a (d×d) rotation matrix. Then,
1 2 2 2
we have
(cid:90)
T†∗ {ϕ(U·)}(u,t) = k (u,t)ϕ(Ux)dx
ReLU x
Rd
(cid:90)
= k (u,t)ϕ(x)dx (114)
UTx (cid:101) (cid:101)
Rd (cid:101)
(cid:90)
= k (Uu,t)ϕ(x)dx (115)
x (cid:101) (cid:101)
(cid:101)
Rd
= T†∗ {ϕ}(Uu,t). (116)
ReLU
The transition from (114) to (115) is possible because
k (u,t) = ReLU(uTUTx−t)+(uTUTx)h (t)+h (t)
UTx (cid:101) (cid:101) 1 2
(cid:101)
24
Random ReLU Neural Networks as Non-Gaussian Processes
= ReLU((Uu)Tx−t)+((Uu)Tx)h (t)+h (t)
(cid:101) (cid:101) 1 2
= k (Uu,t).
x
(cid:101)
From Theorem 9, the characteristic functional of s is given by
ReLU
(cid:18)(cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)sReLU(ϕ) = exp Ψ T† R∗ eLU{ϕ}(u,t) dudt (117)
R Sd−1
with Ψ defined as in (77). Thus, based on (116) and (117), we can write
(cid:18)(cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)sReLU(ϕ(U·)) = exp Ψ T† R∗ eLU{ϕ(U·)}(u,t) dudt
R Sd−1
(cid:18)(cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
= exp Ψ T†∗ {ϕ}(Uu,t) dudt
ReLU
R Sd−1
(cid:18)(cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
= exp Ψ T†∗ {ϕ}(u,t) dudt
ReLU (cid:101) (cid:101)
R Sd−1
= P(cid:98)sReLU(ϕ). (118)
4. In order to show that s (when P has zero mean and a finite second moment) is
ReLU V
wide-sense self-similar with Hurst exponent H = 3/2, we will show that for a > 0,
a2HE[s (x/a)s (y/a)] = E[s (x)s (y)]. (119)
ReLU ReLU ReLU ReLU
Since P has zero mean, based on (39), we have that E[s (x)] = 0. Thus, using
V ReLU
(40), we immediately see that
E[s (x/a)s (y/a)] = a−3E[s (x)s (y)]. (120)
ReLU ReLU ReLU ReLU
5. From the mean and covariance functionals in (86) and (100), respectively, and the
form of the characteristic functional (35), we deduce from Definition 3 that s is
ReLU
non-Gaussian, even when P has a finite second moment
V
Appendix C. Asymptotic Results
To prove Theorem 11, we rely on a generalized version of the L´evy continuity theorem from
Bierm´e et al. (2018, Theorem 2.3), which we state below.
Theorem 13 (Generalized L´evy continuity theorem) Let (s n) n∈N be a sequence of
generalized stochastic processes that take values in S(cid:48)(Rd) with characteristic functionals
(P(cid:98)sn) n∈N. If P(cid:98)sn converges pointwise to a functional Q(cid:98) : S(Rd) → C that is continuous at
0, then there exists a generalized stochastic process s such that its characteristic functional
L
satisfies P(cid:98)s = Q(cid:98) and s
n
−−−→ s.
n→∞
25
Parhi, Bohra, El Biari, Pourya, and Unser
Proof [Proof of Theorem 11] By Theorem 13, we need to show that
(cid:16) (cid:17)
1. for every ϕ ∈ S(Rd), the sequence P(cid:98)sn (ϕ) converges to
ReLU n∈N
(cid:16) (cid:17)
P(cid:98)s∞ ReLU(ϕ) := exp −|b|α(cid:107)T† R∗ eLU{ϕ}(cid:107)α
Lα
(121)
and
2. the functional P(cid:98)s∞ is continuous on S(Rd).
ReLU
We first show that, for every ϕ ∈ S(Rd),
lim P(cid:98)sn (ϕ) = P(cid:98)s∞ (ϕ). (122)
n→∞ ReLU ReLU
Our derivation is inspired from the proof of Lemma 2 of Fageot et al. (2020). Since sn
ReLU
is a bona fide generalized stochastic process that takes values in S(cid:48)(Rd), the functional
P(cid:98)sn (ϕ) is well-defined for ϕ ∈ S(Rd). On the other hand, we observe that P(cid:98)s∞ (ϕ)
ReLU ReLU
is also well-defined for ϕ ∈ S(Rd) due to (80). Next, we prove the convergence. The
characteristic functional of sn is
ReLU
(cid:18)(cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)sn ReLU(ϕ) = exp
R
Sd−1Ψ
n
T† R∗ eLU{ϕ}(u,t) dudt , (123)
where
Ψ n(ξ) :=
n(cid:16) e−|bξ n|α −1(cid:17)
. (124)
For a fixed z ∈ Sd−1×R, we have that
Ψ n(φ(z)) = n(cid:16) e−|bφ( nz)|α −1(cid:17) −−−→ −|bφ(z)|α, (125)
n→∞
where φ = T†∗ {ϕ}. Thus, we need to show that
ReLU
(cid:90) (cid:90)
Ψ (φ(z))dz −−−→ −|bφ(z)|αdz. (126)
n
Sd−1×R n→∞ Sd−1×R
From p. 1058 in Fageot et al. (2020), we have that
√
|Ψ (φ(z))| ≤ 2|bφ(z)|α. (127)
n
The function z (cid:55)→ |bφ(z)|α is in L1(Sd−1×R) due to (80). Thus, we can apply the Lebesgue
dominated convergence theorem to show that (126), and consequently (122), holds. Finally,
the continuity of P(cid:98)s∞ ReLU(ϕ) on S(Rd) follows from the fact that the operator T† R∗
eLU
continu-
ously maps S(Rd) to Lp(Sd−1×R) for p ∈ [1,2] (cf., Equation (80)).
26
Random ReLU Neural Networks as Non-Gaussian Processes
Gaussianity of s∞ When α = 2, the characteristic functional of s∞ can be written
ReLU ReLU
as
(cid:18)(cid:90) (cid:90) (cid:16) (cid:17) (cid:19)
P(cid:98)s∞ ReLU(ϕ) = exp
R
Sd−1Ψ
∞
T† R∗ eLU{ϕ}(u,t) dudt , (128)
where ϕ ∈ S(Rd) and Ψ (ξ) = −|bξ|2 for ξ ∈ R. Using the moment generating properties
∞
of the characteristic functional (as in Appendix B), we get that the mean functional is
µ s∞ (ϕ) = 0, ϕ ∈ S(Rd), (129)
ReLU
as Ψ(cid:48) (0) = 0, and the covariance functional is
∞
(cid:90)
Σ s∞ ReLU(ϕ 1,ϕ 2) = 2|b|2 Sd−1×RT† R∗ eLU{ϕ 1}(z)T† R∗ eLU{ϕ 2}(z)dz, ϕ 1,ϕ 2 ∈ S(Rd), (130)
asΨ(cid:48)(cid:48) (0) = −2|b|2. Thus, from(128)–(130)andDefinition3, weseethats∞ isaGaussian
∞ ReLU
process when α = 2.
Appendix D. Discussion of the Numerical Examples
We generated realizations of the random neural networks by taking advantage of the property
that Poisson points are uniformly distributed in each finite volume (cf., Equation (22))
combined with the fact that the width of a random neural network observed on a compact
domain is a Poisson random variable with mean proportional to the rate parameter λ
multiplied by a property related to the geometry of the domain (cf., Section 4.1). In
particular, the random neural network realizations in Figures 1 and 2 were plotted on the
compact domain Ω = [−1,+1]d and were generated according to the following procedure.
1. Generate a Poisson random variable N with mean λ|Z |, where Z was defined in
λ,Ω Ω Ω
(38).
2. Generate N points i.i.d. uniformly on the finite volume Z ⊂ Sd−1×R, which we
λ,Ω Ω
N
denote by {(w ,b )} λ,Ω.
k k k=1
3. Generate N i.i.d. random variables according to the law P , which we denote by
λ,Ω V
N
{v } λ,Ω.
k k=1
4. Construct the random neural network
N
(cid:88)λ,Ω (cid:104) (cid:105)
snumeric(x) = v ReLU(wTx−b )+cTx+c (131)
ReLU k k k k 0,k
k=1
according to the computation in (28) with ε → 0.
The resulting random neural network snumeric is, up to an affine function, a realization
ReLU
of RP(λ;P ). Finally, in order to highlight the linear regions of the generated networks,
V
we color the top-down plots in Figures 1 and 2 according to the magnitude of the gradient
of snumeric. As the color map choice is arbitrary, the resulting plots are thus realizations
ReLU
of RP(λ;P ) (since the magnitude of the gradient of an affine function is a constant, and
V
therefore simply shifts the color map). We include some additional plots of the random
neural networks in Figures 3 and 4. These figures are surface plots of the random neural
networks in Figures 1 and 2, respectively.
27
Parhi, Bohra, El Biari, Pourya, and Unser
(a) λ = 1 (b) λ = 10 (c) λ = 100 (d) λ = 1000
Figure 3: P is Gaussian.
V
(a) λ = 1 (b) λ = 10 (c) λ = 100 (d) λ = 1000
Figure 4: P is symmetric (α = 1.25)-stable.
V
